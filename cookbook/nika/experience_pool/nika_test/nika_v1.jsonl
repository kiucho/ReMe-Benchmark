{"workspace_id": "nika_v1", "memory_id": "ccbe0239042147a8beff905c93752ab0", "memory_type": "task", "when_to_use": "When an automated/aggregate diagnostic tool fails (e.g., JSON parsing errors, timeouts) but you still need a coherent network health assessment.", "content": "Treat tool failure as a data-quality signal, then immediately pivot to manual per-node inspection with a small, repeatable command bundle: (1) `ip -j addr` + `ip route` to validate addressing and L3 adjacency assumptions, (2) `ss -lntup`/`ss -lntup | head` + `pgrep -af` to confirm critical services are actually listening, and (3) a local loopback check for application health (e.g., `curl -I http://127.0.0.1:80/`). This preserves forward progress even when global reachability tooling is broken, and produces evidence-backed statements about host/service health vs. fabric health.", "score": 0.4657749124975679, "time_created": "2026-01-16 21:19:01", "time_modified": "2026-01-16 21:19:01", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When an automated/aggregate diagnostic tool fails (e.g., JSON parsing errors, timeouts) but you still need a coherent network health assessment.", "experience": "Treat tool failure as a data-quality signal, then immediately pivot to manual per-node inspection with a small, repeatable command bundle: (1) `ip -j addr` + `ip route` to validate addressing and L3 adjacency assumptions, (2) `ss -lntup`/`ss -lntup | head` + `pgrep -af` to confirm critical services are actually listening, and (3) a local loopback check for application health (e.g., `curl -I http://127.0.0.1:80/`). This preserves forward progress even when global reachability tooling is broken, and produces evidence-backed statements about host/service health vs. fabric health.", "tags": ["fallback", "tooling-failure", "per-node-triage", "ip-addr", "ip-route", "ss", "curl", "service-validation"], "confidence": 0.84, "step_type": "decision", "tools_used": ["exec"], "freq": 2, "utility": 2}}
{"workspace_id": "nika_v1", "memory_id": "c82853ab62a948afb266906c1eb118e4", "memory_type": "task", "when_to_use": "When an initial automated reachability/health tool fails or returns malformed output, and you still need a reliable network-health snapshot.", "content": "Fall back to direct per-node inspection with simple, high-signal commands: collect interface/IP state (e.g., `ip addr`), routing table (`ip route`), and routing daemon state (`vtysh show ip rip/status/routes`). This bypasses tooling/parsing failures and quickly establishes whether the underlay is up, whether addressing matches the design (/31 infra links, /24 LANs), and whether the control plane is exchanging routes. In the trajectory, this pivot replaced a failing `get_reachability` call and produced enough evidence to distinguish underlay health from prefix-advertisement issues.", "score": 0.4675524233645217, "time_created": "2026-01-16 21:30:53", "time_modified": "2026-01-16 21:30:53", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When an initial automated reachability/health tool fails or returns malformed output, and you still need a reliable network-health snapshot.", "experience": "Fall back to direct per-node inspection with simple, high-signal commands: collect interface/IP state (e.g., `ip addr`), routing table (`ip route`), and routing daemon state (`vtysh show ip rip/status/routes`). This bypasses tooling/parsing failures and quickly establishes whether the underlay is up, whether addressing matches the design (/31 infra links, /24 LANs), and whether the control plane is exchanging routes. In the trajectory, this pivot replaced a failing `get_reachability` call and produced enough evidence to distinguish underlay health from prefix-advertisement issues.", "tags": ["fallback", "tooling-failure", "baseline", "ip-addr", "ip-route", "frr", "rip"], "confidence": 0.83, "step_type": "decision", "tools_used": ["exec (shell on nodes)", "vtysh"], "freq": 1, "utility": 1}}
{"workspace_id": "nika_v1", "memory_id": "07a26cf451c3481a9598b1709921232e", "memory_type": "task", "when_to_use": "When fabric reachability tools fail or return parsing errors, and you still need to assess a Clos/EBGP control-plane health quickly.", "content": "Fallback from high-level reachability tooling to direct per-node CLI inspection. First, validate endpoint/container liveness (tool errors like \"container is not running\" immediately explain missing reachability). Then, on routers, check for presence of FRR management CLI (vtysh) and FRR daemons (zebra/bgpd/watchfrr) via process listing. This pattern works because it distinguishes tooling issues from actual network faults and rapidly confirms whether the routing control plane exists at all, before spending time on neighbor-level BGP debugging.", "score": 0.47801270128224493, "time_created": "2026-01-16 21:15:58", "time_modified": "2026-01-16 21:15:58", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When fabric reachability tools fail or return parsing errors, and you still need to assess a Clos/EBGP control-plane health quickly.", "experience": "Fallback from high-level reachability tooling to direct per-node CLI inspection. First, validate endpoint/container liveness (tool errors like \"container is not running\" immediately explain missing reachability). Then, on routers, check for presence of FRR management CLI (vtysh) and FRR daemons (zebra/bgpd/watchfrr) via process listing. This pattern works because it distinguishes tooling issues from actual network faults and rapidly confirms whether the routing control plane exists at all, before spending time on neighbor-level BGP debugging.", "tags": ["troubleshooting", "clos", "ebgp", "frr", "tool-fallback", "control-plane", "container-health"], "confidence": 0.86, "step_type": "decision", "tools_used": ["get_reachability", "ping_pair", "run_cmd"], "freq": 5, "utility": 4}}
{"workspace_id": "nika_v1", "memory_id": "e9d0598a6d8841a89480187a2b339730", "memory_type": "task", "when_to_use": "SDN/L2 fabrics where some host-to-host pings succeed but failures are intermittent or show suspiciously low RTTs, suggesting endpoint/ARP issues rather than a full fabric outage.", "content": "Start with lightweight end-to-end reachability sampling (multi-pair pings) and immediately sanity-check the returned destination IPs and RTTs. Treat (a) extremely low RTT to a remote host and/or (b) asymmetric/intermittent loss as triggers to verify host IP assignments (`ip addr`, `ifconfig`, `ip route`) across all endpoints. In this run, correlating ping targets with the host inventory exposed a duplicate IP (10.0.0.1 on two hosts), which cleanly explained ARP ambiguity and the observed 50% loss pattern. This pattern works because it quickly distinguishes dataplane path failure from address/ARP instability and localizes the fault to endpoints with minimal tooling.", "score": 0.87, "time_created": "2026-01-16 22:09:30", "time_modified": "2026-01-16 22:09:30", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "SDN/L2 fabrics where some host-to-host pings succeed but failures are intermittent or show suspiciously low RTTs, suggesting endpoint/ARP issues rather than a full fabric outage.", "experience": "Start with lightweight end-to-end reachability sampling (multi-pair pings) and immediately sanity-check the returned destination IPs and RTTs. Treat (a) extremely low RTT to a remote host and/or (b) asymmetric/intermittent loss as triggers to verify host IP assignments (`ip addr`, `ifconfig`, `ip route`) across all endpoints. In this run, correlating ping targets with the host inventory exposed a duplicate IP (10.0.0.1 on two hosts), which cleanly explained ARP ambiguity and the observed 50% loss pattern. This pattern works because it quickly distinguishes dataplane path failure from address/ARP instability and localizes the fault to endpoints with minimal tooling.", "tags": ["sdn", "ovs", "spine-leaf", "icmp", "arp", "ip-conflict", "endpoint-validation", "rtx-rtt-anomaly"], "confidence": 0.9, "step_type": "decision", "tools_used": ["ping_mesh", "host_netinfo"]}}
{"workspace_id": "nika_v1", "memory_id": "d59e021e20d7435f96253750c5a76084", "memory_type": "task", "when_to_use": "Tasks requiring a final structured 'submission' (classification) after narrative diagnosis, especially when multiple candidate fault types exist.", "content": "After producing the narrative diagnosis, map findings to the closest predefined root-cause label by matching concrete evidence (e.g., duplicated IPs + ARP-like intermittent loss) to taxonomy entries, then submit a minimal JSON containing: anomaly flag, implicated devices, and root-cause name. Here, selecting `host_ip_conflict` and listing both hosts sharing 10.0.0.1 ensured the submission aligned with the platform’s expected categories and preserved the key localization. This works because it converts qualitative troubleshooting into an unambiguous, machine-checkable outcome.", "score": 0.86, "time_created": "2026-01-16 22:09:30", "time_modified": "2026-01-16 22:09:30", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "Tasks requiring a final structured 'submission' (classification) after narrative diagnosis, especially when multiple candidate fault types exist.", "experience": "After producing the narrative diagnosis, map findings to the closest predefined root-cause label by matching concrete evidence (e.g., duplicated IPs + ARP-like intermittent loss) to taxonomy entries, then submit a minimal JSON containing: anomaly flag, implicated devices, and root-cause name. Here, selecting `host_ip_conflict` and listing both hosts sharing 10.0.0.1 ensured the submission aligned with the platform’s expected categories and preserved the key localization. This works because it converts qualitative troubleshooting into an unambiguous, machine-checkable outcome.", "tags": ["taxonomy-mapping", "structured-output", "root-cause-labeling", "submission"], "confidence": 0.82, "step_type": "action", "tools_used": ["list_root_causes", "submit_root_cause"]}}
{"workspace_id": "nika_v1", "memory_id": "37b57911e1764be9b750f97cec68cfb4", "memory_type": "task", "when_to_use": "Multi-tier routed fabric where end-to-end service reachability fails and you must quickly localize whether the problem is host config, underlay links, or routing/control-plane.", "content": "Use a bottom-up, boundary-localization sequence: (1) collect host IP/default-route state on client and servers; (2) run targeted pings that distinguish 'host unreachable' (often local L2/next-hop issues) vs 'net unreachable' (missing route on a router) and note the ICMP source address; (3) validate first-hop reachability from each host to its gateway to rule out local link issues; (4) validate underlay adjacency pings between router tiers (/31 neighbors) to confirm physical/L3 underlay health. This pattern worked because the ICMP error sources and first-hop checks separated two independent faults: client gateway mismatch (client-sourced host unreachable) and missing routing between service subnets (leaf-sourced net unreachable).", "score": 0.86, "time_created": "2026-01-16 22:31:06", "time_modified": "2026-01-16 22:31:06", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "Multi-tier routed fabric where end-to-end service reachability fails and you must quickly localize whether the problem is host config, underlay links, or routing/control-plane.", "experience": "Use a bottom-up, boundary-localization sequence: (1) collect host IP/default-route state on client and servers; (2) run targeted pings that distinguish 'host unreachable' (often local L2/next-hop issues) vs 'net unreachable' (missing route on a router) and note the ICMP source address; (3) validate first-hop reachability from each host to its gateway to rule out local link issues; (4) validate underlay adjacency pings between router tiers (/31 neighbors) to confirm physical/L3 underlay health. This pattern worked because the ICMP error sources and first-hop checks separated two independent faults: client gateway mismatch (client-sourced host unreachable) and missing routing between service subnets (leaf-sourced net unreachable).", "tags": ["troubleshooting", "fault-localization", "icmp", "default-gateway", "underlay", "clos"], "confidence": 0.82, "step_type": "reasoning", "tools_used": ["host_ifconfig/ip_addr/ip_route", "ping"]}}
{"workspace_id": "nika_v1", "memory_id": "113b3ee8e32646409998519877ad3be6", "memory_type": "task", "when_to_use": "When end-to-end connectivity is reported broken in a routed enterprise topology and you need to quickly separate (a) endpoint/service failure from (b) routing/control-plane failure.", "content": "Use a bottom-up triage sequence: (1) verify at least one user host is actually running and has correct IP/GW/DNS; (2) verify server-side services are locally healthy (DNS :53 listening, web :80 responding via localhost curl) to avoid misattributing the outage to applications; (3) run a minimal reachability test from the user host to its default gateway (should succeed) and then to a remote subnet (e.g., server farm IP). If the remote ping fails with 'Destination Net Unreachable' sourced from the gateway, treat it as strong evidence of missing routes on the gateway (routing plane issue) rather than a link/ACL issue. This pattern worked because it created a clear fault boundary: local L2/L3 to the gateway was good, services were up, but inter-subnet routing was absent.", "score": 0.5417486999364173, "time_created": "2026-01-16 21:27:44", "time_modified": "2026-01-16 21:27:44", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When end-to-end connectivity is reported broken in a routed enterprise topology and you need to quickly separate (a) endpoint/service failure from (b) routing/control-plane failure.", "experience": "Use a bottom-up triage sequence: (1) verify at least one user host is actually running and has correct IP/GW/DNS; (2) verify server-side services are locally healthy (DNS :53 listening, web :80 responding via localhost curl) to avoid misattributing the outage to applications; (3) run a minimal reachability test from the user host to its default gateway (should succeed) and then to a remote subnet (e.g., server farm IP). If the remote ping fails with 'Destination Net Unreachable' sourced from the gateway, treat it as strong evidence of missing routes on the gateway (routing plane issue) rather than a link/ACL issue. This pattern worked because it created a clear fault boundary: local L2/L3 to the gateway was good, services were up, but inter-subnet routing was absent.", "tags": ["triage", "fault-isolation", "ping", "default-gateway", "service-validation", "dns", "http"], "confidence": 0.86, "step_type": "decision", "tools_used": ["exec"], "freq": 9, "utility": 5}}
{"workspace_id": "nika_v1", "memory_id": "b79a9944d8384688aa412e774cdd764b", "memory_type": "task", "when_to_use": "When end-to-end connectivity is reported as partially failing (some nodes reachable, others timing out) and you need to quickly localize whether the issue is endpoint-local vs core routing.", "content": "Start with a minimal reachability matrix from multiple vantage points (e.g., internal hosts and external servers) to detect asymmetry. If external->internal works for one host (host_2) but internal host_1 cannot reach anything (timeouts), treat it as a strong signal of a localized host_1 LAN/gateway issue rather than RIP/core failure. This comparative probing narrows the fault domain before deeper device inspection.", "score": 0.82, "time_created": "2026-01-16 22:46:03", "time_modified": "2026-01-16 22:46:03", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When end-to-end connectivity is reported as partially failing (some nodes reachable, others timing out) and you need to quickly localize whether the issue is endpoint-local vs core routing.", "experience": "Start with a minimal reachability matrix from multiple vantage points (e.g., internal hosts and external servers) to detect asymmetry. If external->internal works for one host (host_2) but internal host_1 cannot reach anything (timeouts), treat it as a strong signal of a localized host_1 LAN/gateway issue rather than RIP/core failure. This comparative probing narrows the fault domain before deeper device inspection.", "tags": ["triage", "reachability-matrix", "fault-localization", "asymmetric-connectivity", "icmp"], "confidence": 0.78, "step_type": "decision", "tools_used": ["connectivity_check", "exec"]}}
{"workspace_id": "nika_v1", "memory_id": "fa5f2794aeb740ddb9787c430d164574", "memory_type": "task", "when_to_use": "When hosts in a Clos/EBGP fabric show loss of connectivity (host-to-host or host-to-any) and you need to quickly determine whether the failure is at the edge (L2/L3) or in the routed fabric (BGP/underlay).", "content": "Start troubleshooting from the endpoints: (1) collect each host’s IP/prefix and default gateway, then (2) ping the default gateway and, if it fails, run ARP/arping to distinguish L2 adjacency failure from routed-path failure. In this trajectory, pc_0_0 could not ping/ARP its gateway while pc_0_1 could, immediately localizing the fault to pc_0_0’s edge rather than the entire fabric. This works because first-hop gateway reachability is a prerequisite for any BGP/underlay issues to matter; ARP failure strongly indicates on-link addressing/VLAN/L2 problems or wrong gateway IP.", "score": 0.86, "time_created": "2026-01-16 22:48:40", "time_modified": "2026-01-16 22:48:40", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When hosts in a Clos/EBGP fabric show loss of connectivity (host-to-host or host-to-any) and you need to quickly determine whether the failure is at the edge (L2/L3) or in the routed fabric (BGP/underlay).", "experience": "Start troubleshooting from the endpoints: (1) collect each host’s IP/prefix and default gateway, then (2) ping the default gateway and, if it fails, run ARP/arping to distinguish L2 adjacency failure from routed-path failure. In this trajectory, pc_0_0 could not ping/ARP its gateway while pc_0_1 could, immediately localizing the fault to pc_0_0’s edge rather than the entire fabric. This works because first-hop gateway reachability is a prerequisite for any BGP/underlay issues to matter; ARP failure strongly indicates on-link addressing/VLAN/L2 problems or wrong gateway IP.", "tags": ["clos", "datacenter", "edge-troubleshooting", "default-gateway", "arp", "l2-vs-l3", "fault-localization"], "confidence": 0.86, "step_type": "action", "tools_used": ["host interface/route inspection", "ping", "arping"]}}
{"workspace_id": "nika_v1", "memory_id": "50d7027631eb4b098fd54f45b0fc07c5", "memory_type": "task", "when_to_use": "When an end host reports inability to reach any remote subnet (e.g., DNS/HTTP targets) and ping failures show 'Destination Host Unreachable' originating from the source host.", "content": "Start troubleshooting at the edge by validating the host’s L3 basics before inspecting the fabric: collect `ip addr` and `ip route` to confirm the host IP/prefix and default gateway, then check L2 adjacency to the gateway using `ip neigh` (look for FAILED/INCOMPLETE) and interface counters (TX increases with near-zero RX suggests no replies). This quickly distinguishes a first-hop/L2 problem from underlay/BGP issues and prevents wasted time on core routing when the host cannot even ARP its gateway.", "score": 0.9, "time_created": "2026-01-16 22:52:16", "time_modified": "2026-01-16 22:52:16", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When an end host reports inability to reach any remote subnet (e.g., DNS/HTTP targets) and ping failures show 'Destination Host Unreachable' originating from the source host.", "experience": "Start troubleshooting at the edge by validating the host’s L3 basics before inspecting the fabric: collect `ip addr` and `ip route` to confirm the host IP/prefix and default gateway, then check L2 adjacency to the gateway using `ip neigh` (look for FAILED/INCOMPLETE) and interface counters (TX increases with near-zero RX suggests no replies). This quickly distinguishes a first-hop/L2 problem from underlay/BGP issues and prevents wasted time on core routing when the host cannot even ARP its gateway.", "tags": ["first-hop", "arp", "ip-neigh", "default-gateway", "edge-triage", "l2-l3-boundary"], "confidence": 0.9, "step_type": "decision", "tools_used": ["ip_addr", "ip_route", "ip_neigh", "ip_link", "ping"]}}
{"workspace_id": "nika_v1", "memory_id": "65e2b8e333c44906b51c35b70eb9261f", "memory_type": "task", "when_to_use": "When a single endpoint (host) cannot reach multiple destinations and errors include local \"Destination Host Unreachable\" or timeouts, while other hosts appear healthy.", "content": "Start by contrasting reachability from multiple sources to the same destination(s) to quickly determine whether the issue is localized or systemic. In this case, checking host_2->vpn_server_1 success while host_1 failed immediately narrowed scope to host_1 or its access segment. Then run a minimal triage sequence from the failing host: (1) ping a local peer and an off-subnet target, (2) inspect interface IP/netmask and default route, (3) check ARP/neighbor state for the default gateway. The combination of a FAILED neighbor entry for the configured gateway and an IP/subnet that doesn't match the connected router LAN is strong evidence of host-side L3 misconfiguration rather than routing/VPN issues.", "score": 0.86, "time_created": "2026-01-16 23:10:10", "time_modified": "2026-01-16 23:10:10", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When a single endpoint (host) cannot reach multiple destinations and errors include local \"Destination Host Unreachable\" or timeouts, while other hosts appear healthy.", "experience": "Start by contrasting reachability from multiple sources to the same destination(s) to quickly determine whether the issue is localized or systemic. In this case, checking host_2->vpn_server_1 success while host_1 failed immediately narrowed scope to host_1 or its access segment. Then run a minimal triage sequence from the failing host: (1) ping a local peer and an off-subnet target, (2) inspect interface IP/netmask and default route, (3) check ARP/neighbor state for the default gateway. The combination of a FAILED neighbor entry for the configured gateway and an IP/subnet that doesn't match the connected router LAN is strong evidence of host-side L3 misconfiguration rather than routing/VPN issues.", "tags": ["scope-isolation", "endpoint-triage", "arp", "default-gateway", "misconfiguration", "icmp-symptoms"], "confidence": 0.86, "step_type": "decision", "tools_used": ["ping", "ip_addr", "ip_route", "ip_neigh", "ifconfig"]}}
{"workspace_id": "nika_v1", "memory_id": "7b0ab6d3aa53476488ea4e79083fdf6c", "memory_type": "task", "when_to_use": "When a topology description specifies a single L2/L3 host subnet (e.g., 10.0.0.0/24) but reachability tests show partial connectivity or asymmetric failures involving one host.", "content": "Start with end-to-end symptom triage using a small ping matrix across hosts to quickly separate 'fabric-wide' failures from 'single-endpoint' failures. Then immediately validate host-side L3 fundamentals (ip addr/ifconfig + ip route) on the failing endpoint(s) and at least one working endpoint. In this run, pings showed hosts 10.0.0.2/3/4 could reach each other while any path involving host_1_1 failed; host_1_1 was confirmed misaddressed (10.2.1.8/24) with a default route via 10.2.1.1, explaining both 'Destination Host Unreachable' and 'Network is unreachable' from 10.0.0.0/24 peers. This pattern works because incorrect IP/subnet/gateway is a common single-host root cause and can be proven quickly with host routing tables.", "score": 0.86, "time_created": "2026-01-16 23:16:26", "time_modified": "2026-01-16 23:16:26", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When a topology description specifies a single L2/L3 host subnet (e.g., 10.0.0.0/24) but reachability tests show partial connectivity or asymmetric failures involving one host.", "experience": "Start with end-to-end symptom triage using a small ping matrix across hosts to quickly separate 'fabric-wide' failures from 'single-endpoint' failures. Then immediately validate host-side L3 fundamentals (ip addr/ifconfig + ip route) on the failing endpoint(s) and at least one working endpoint. In this run, pings showed hosts 10.0.0.2/3/4 could reach each other while any path involving host_1_1 failed; host_1_1 was confirmed misaddressed (10.2.1.8/24) with a default route via 10.2.1.1, explaining both 'Destination Host Unreachable' and 'Network is unreachable' from 10.0.0.0/24 peers. This pattern works because incorrect IP/subnet/gateway is a common single-host root cause and can be proven quickly with host routing tables.", "tags": ["sdn", "spine-leaf", "partial-connectivity", "ping-matrix", "host-l3-validation", "incorrect-ip"], "confidence": 0.86, "step_type": "decision", "tools_used": ["ping", "ifconfig", "ip_addr", "ip_route"]}}
{"workspace_id": "nika_v1", "memory_id": "ba20a43ef5fd48c2ba31c916864ec6e8", "memory_type": "task", "when_to_use": "When troubleshooting multi-tier enterprise networks where some segments may be healthy but end-to-end connectivity is failing (e.g., users cannot reach server farm).", "content": "Start by establishing a baseline within a single segment to reduce scope: run targeted pings among server-farm nodes (DNS ↔ web servers, web ↔ web). If intra-segment traffic is clean (0% loss, low RTT), treat L2 and host addressing in that segment as healthy and shift focus to inter-segment routing. This worked because it quickly separated 'local segment health' from 'routing domain health', preventing wasted time debugging DNS/web services when the underlying issue was reachability between subnets.", "score": 0.82, "time_created": "2026-01-16 23:30:35", "time_modified": "2026-01-16 23:30:35", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When troubleshooting multi-tier enterprise networks where some segments may be healthy but end-to-end connectivity is failing (e.g., users cannot reach server farm).", "experience": "Start by establishing a baseline within a single segment to reduce scope: run targeted pings among server-farm nodes (DNS ↔ web servers, web ↔ web). If intra-segment traffic is clean (0% loss, low RTT), treat L2 and host addressing in that segment as healthy and shift focus to inter-segment routing. This worked because it quickly separated 'local segment health' from 'routing domain health', preventing wasted time debugging DNS/web services when the underlying issue was reachability between subnets.", "tags": ["baseline", "blast-radius", "segmentation", "ping", "server-farm", "scope-reduction"], "confidence": 0.82, "step_type": "decision", "tools_used": ["connectivity_test"]}}
{"workspace_id": "nika_v1", "memory_id": "65d1c9fdd4c447798dc9c8dda7563234", "memory_type": "task", "when_to_use": "End-to-end connectivity fails between hosts in a routed Clos/EBGP fabric and the ping error is immediate (e.g., \"Network is unreachable\") or tooling cannot determine a host IP.", "content": "Start troubleshooting at the endpoints before inspecting BGP. Run host-side interface and route inspection (e.g., ip addr / ifconfig, ip route). If a host has no IPv4 address on its NIC and/or an empty routing table, the failure is local and will present as an immediate \"Network is unreachable\" (no packets sent). This quickly localizes the fault to host configuration and avoids unnecessary fabric-level investigation.", "score": 0.9, "time_created": "2026-01-16 23:35:58", "time_modified": "2026-01-16 23:35:58", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "End-to-end connectivity fails between hosts in a routed Clos/EBGP fabric and the ping error is immediate (e.g., \"Network is unreachable\") or tooling cannot determine a host IP.", "experience": "Start troubleshooting at the endpoints before inspecting BGP. Run host-side interface and route inspection (e.g., ip addr / ifconfig, ip route). If a host has no IPv4 address on its NIC and/or an empty routing table, the failure is local and will present as an immediate \"Network is unreachable\" (no packets sent). This quickly localizes the fault to host configuration and avoids unnecessary fabric-level investigation.", "tags": ["endpoint-first", "ping-signature", "host-missing-ip", "l3-baseline", "fault-localization"], "confidence": 0.9, "step_type": "decision", "tools_used": ["ping", "get_host_config"]}}
{"workspace_id": "nika_v1", "memory_id": "98d5ca65a54b4e7688dae607accc4ee1", "memory_type": "task", "when_to_use": "You must produce a clear diagnosis report and a structured submission (faulty devices + root cause category) from mixed tool outputs.", "content": "Translate raw symptoms into a layered report: (1) failure signature (what error and what it implies), (2) endpoint L3 state (IP + default route), (3) underlay/router interface sanity, (4) anomaly summary with fault domain and most likely root cause. Then map the conclusion to a standardized root-cause label (e.g., host_missing_ip) and list the minimal faulty device set. This structure is reusable and supports automated submission workflows.", "score": 0.84, "time_created": "2026-01-16 23:35:58", "time_modified": "2026-01-16 23:35:58", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "You must produce a clear diagnosis report and a structured submission (faulty devices + root cause category) from mixed tool outputs.", "experience": "Translate raw symptoms into a layered report: (1) failure signature (what error and what it implies), (2) endpoint L3 state (IP + default route), (3) underlay/router interface sanity, (4) anomaly summary with fault domain and most likely root cause. Then map the conclusion to a standardized root-cause label (e.g., host_missing_ip) and list the minimal faulty device set. This structure is reusable and supports automated submission workflows.", "tags": ["report-structure", "evidence-based", "root-cause-mapping", "standardized-labels", "submission"], "confidence": 0.85, "step_type": "reasoning", "tools_used": ["ping", "get_host_config", "get_router_config", "submit"]}}
{"workspace_id": "nika_v1", "memory_id": "5a5aae2e22bb4767a7d6a0de79fcf6d5", "memory_type": "task", "when_to_use": "End-to-end service access fails (DNS/HTTP) from an external client in a routed Clos/leaf-spine fabric and the symptom includes immediate errors like \"network unreachable\" or \"no route to host\".", "content": "Start troubleshooting at the client and validate L3 viability before diving into the fabric: (1) inspect client interface addressing (does eth0 actually have the expected 192.168.<pod>.x/24?), (2) attempt DNS resolution and HTTP access to capture the exact failure mode, and (3) interpret \"network unreachable\" as a likely local routing/IP configuration issue rather than a remote service problem. In the successful sequence, the client output showed eth0 UP but no IPv4 address, and both dig/curl failed immediately with \"network unreachable\", correctly localizing the first hard failure to the client host configuration.", "score": 0.86, "time_created": "2026-01-16 23:39:02", "time_modified": "2026-01-16 23:39:02", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "End-to-end service access fails (DNS/HTTP) from an external client in a routed Clos/leaf-spine fabric and the symptom includes immediate errors like \"network unreachable\" or \"no route to host\".", "experience": "Start troubleshooting at the client and validate L3 viability before diving into the fabric: (1) inspect client interface addressing (does eth0 actually have the expected 192.168.<pod>.x/24?), (2) attempt DNS resolution and HTTP access to capture the exact failure mode, and (3) interpret \"network unreachable\" as a likely local routing/IP configuration issue rather than a remote service problem. In the successful sequence, the client output showed eth0 UP but no IPv4 address, and both dig/curl failed immediately with \"network unreachable\", correctly localizing the first hard failure to the client host configuration.", "tags": ["clos", "leaf-spine", "client-first", "l3-basics", "network-unreachable", "dns", "http"], "confidence": 0.86, "step_type": "decision", "tools_used": ["host_state_snapshot"]}}
{"workspace_id": "nika_v1", "memory_id": "14f6ad2103d747be8b2ce9dd85c0d8c3", "memory_type": "task", "when_to_use": "When user endpoints appear unreachable or upstream tests return null/unknown, and you suspect an edge/DHCP issue in a routed enterprise topology.", "content": "Start by validating endpoint IP configuration before testing the wider network. Pull `ip addr`/`ifconfig` and `ip route` from the affected hosts. If hosts have no IPv4 address and an empty routing table, interpret errors like “Network is unreachable” as a local stack/config symptom (not an upstream routing/ACL failure). Then pivot to checking the local default gateway interface on the first-hop (e.g., dist router SVI/bridge IP) to confirm the subnet is present and correctly addressed. This quickly localizes the primary fault domain to DHCP acquisition/relay vs. L3 routing.", "score": 0.86, "time_created": "2026-01-16 23:43:22", "time_modified": "2026-01-16 23:43:22", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When user endpoints appear unreachable or upstream tests return null/unknown, and you suspect an edge/DHCP issue in a routed enterprise topology.", "experience": "Start by validating endpoint IP configuration before testing the wider network. Pull `ip addr`/`ifconfig` and `ip route` from the affected hosts. If hosts have no IPv4 address and an empty routing table, interpret errors like “Network is unreachable” as a local stack/config symptom (not an upstream routing/ACL failure). Then pivot to checking the local default gateway interface on the first-hop (e.g., dist router SVI/bridge IP) to confirm the subnet is present and correctly addressed. This quickly localizes the primary fault domain to DHCP acquisition/relay vs. L3 routing.", "tags": ["dhcp", "host_missing_ip", "edge_troubleshooting", "first_hop_gateway", "fault_localization"], "confidence": 0.86, "step_type": "decision", "tools_used": ["reachability_matrix", "get_host_network_state"]}}
{"workspace_id": "nika_v1", "memory_id": "2b9ad376a8844e948638e38e7453f053", "memory_type": "task", "when_to_use": "When you need to produce a final structured incident submission after generating an evidence-based diagnosis report.", "content": "Map observed symptoms to standardized root-cause categories by selecting the minimal set that explains the outage. Use host evidence (no IPv4/route) to select `host_missing_ip`, and router control-plane evidence (no vtysh/daemons, only connected routes) to select `frr_service_down`. Include both the directly impacted endpoints and the key infrastructure nodes implicated by the evidence in `faulty_devices`. This yields a consistent, machine-actionable submission aligned with the narrative report.", "score": 0.84, "time_created": "2026-01-16 23:43:22", "time_modified": "2026-01-16 23:43:22", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When you need to produce a final structured incident submission after generating an evidence-based diagnosis report.", "experience": "Map observed symptoms to standardized root-cause categories by selecting the minimal set that explains the outage. Use host evidence (no IPv4/route) to select `host_missing_ip`, and router control-plane evidence (no vtysh/daemons, only connected routes) to select `frr_service_down`. Include both the directly impacted endpoints and the key infrastructure nodes implicated by the evidence in `faulty_devices`. This yields a consistent, machine-actionable submission aligned with the narrative report.", "tags": ["submission", "root_cause_mapping", "taxonomy", "incident_reporting"], "confidence": 0.78, "step_type": "action", "tools_used": ["submit_root_cause"]}}
{"workspace_id": "nika_v1", "memory_id": "99e9fc17cbcd4ee498561a0b725bdd92", "memory_type": "task", "when_to_use": "When diagnosing multi-segment routed labs where symptoms could be caused by either routing/control-plane issues or endpoint misconfiguration (e.g., “can’t reach service”, “VPN-only access”, intermittent connectivity).", "content": "Start with a broad, low-cost end-to-end reachability matrix (ICMP between representative internal hosts and external servers/VPN endpoints) to quickly separate L3 reachability from application/VPN problems. Immediately follow with per-node interface/route snapshots (ifconfig/ip addr/ip route) on the endpoints and key routers to catch obvious misconfigs (duplicate IPs, wrong default gateway, missing routes) and to validate that the routing protocol is distributing expected prefixes. This pattern worked because it (1) established that underlay routing was largely functional (pings succeeded, RIP routes present), while (2) revealing a decisive endpoint-layer fault (host_1 and host_2 both 10.0.0.2/24) that could otherwise invalidate higher-layer tests.", "score": 0.531602316952519, "time_created": "2026-01-16 22:04:03", "time_modified": "2026-01-16 22:04:03", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When diagnosing multi-segment routed labs where symptoms could be caused by either routing/control-plane issues or endpoint misconfiguration (e.g., “can’t reach service”, “VPN-only access”, intermittent connectivity).", "experience": "Start with a broad, low-cost end-to-end reachability matrix (ICMP between representative internal hosts and external servers/VPN endpoints) to quickly separate L3 reachability from application/VPN problems. Immediately follow with per-node interface/route snapshots (ifconfig/ip addr/ip route) on the endpoints and key routers to catch obvious misconfigs (duplicate IPs, wrong default gateway, missing routes) and to validate that the routing protocol is distributing expected prefixes. This pattern worked because it (1) established that underlay routing was largely functional (pings succeeded, RIP routes present), while (2) revealing a decisive endpoint-layer fault (host_1 and host_2 both 10.0.0.2/24) that could otherwise invalidate higher-layer tests.", "tags": ["triage", "reachability", "rip", "ip-conflict", "layer3-vs-layer7", "baseline-snapshot"], "confidence": 0.84, "step_type": "decision", "tools_used": ["reachability_test", "get_ifconfig", "get_ip_route"], "freq": 6, "utility": 4}}
{"workspace_id": "nika_v1", "memory_id": "8d493129e9c34e00bb24afc77598ff34", "memory_type": "task", "when_to_use": "SDN/L2 fabric issues are suspected and you need to quickly determine whether the problem is endpoint misconfiguration vs. switching/controller failure.", "content": "Start with a lightweight end-to-end reachability sweep (host-to-host pings) to find which pairs fail, then immediately validate host L3 fundamentals (ip addr/ifconfig + ip route) on the failing endpoints. In this run, pings among hosts on 10.0.0.0/24 succeeded while any pair involving host_1 was unknown/failing; inspecting host_1 showed it was in 10.2.1.0/24 with a mismatched default gateway. This pattern works because subnet mismatch is a high-probability, high-impact cause that explains asymmetric/partitioned connectivity without requiring deeper fabric inspection.", "score": 0.5950388658028937, "time_created": "2026-01-16 23:12:45", "time_modified": "2026-01-16 23:12:45", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "SDN/L2 fabric issues are suspected and you need to quickly determine whether the problem is endpoint misconfiguration vs. switching/controller failure.", "experience": "Start with a lightweight end-to-end reachability sweep (host-to-host pings) to find which pairs fail, then immediately validate host L3 fundamentals (ip addr/ifconfig + ip route) on the failing endpoints. In this run, pings among hosts on 10.0.0.0/24 succeeded while any pair involving host_1 was unknown/failing; inspecting host_1 showed it was in 10.2.1.0/24 with a mismatched default gateway. This pattern works because subnet mismatch is a high-probability, high-impact cause that explains asymmetric/partitioned connectivity without requiring deeper fabric inspection.", "tags": ["sdn", "triage", "ping-matrix", "host-l3-validation", "subnet-mismatch", "fault-localization"], "confidence": 0.86, "step_type": "action", "tools_used": ["ping_sweep", "host_ifconfig", "host_ip_addr", "host_ip_route"], "freq": 1, "utility": 1}}
{"workspace_id": "nika_v1", "memory_id": "f1d52b610ff84f279a922659a95d8eaa", "memory_type": "task", "when_to_use": "When end-to-end reachability fails in a routed enterprise topology and you need to quickly localize whether the issue is L2, default-gateway, or missing routing (e.g., OSPF/BGP) between access and server segments.", "content": "Use a layered isolation sequence: (1) verify same-subnet/L2 health in a known-good domain (e.g., server farm host-to-host pings) to establish a baseline; (2) test from an access host to a remote subnet and read ICMP error sources carefully—\"Destination Net Unreachable\" sourced from the default gateway strongly indicates the gateway lacks a route upstream (control-plane/routing failure), while \"Destination Host Unreachable\" sourced from the host suggests local L2/ARP/gateway adjacency problems; (3) validate host IP/gateway settings via ip addr/ip route to confirm the host is in the expected subnet and pointing to the correct gateway. This pattern works because it separates data-plane L2 faults from L3 routing-table faults using deterministic ICMP semantics and minimal commands.", "score": 0.5462085705384625, "time_created": "2026-01-16 23:06:48", "time_modified": "2026-01-16 23:06:48", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When end-to-end reachability fails in a routed enterprise topology and you need to quickly localize whether the issue is L2, default-gateway, or missing routing (e.g., OSPF/BGP) between access and server segments.", "experience": "Use a layered isolation sequence: (1) verify same-subnet/L2 health in a known-good domain (e.g., server farm host-to-host pings) to establish a baseline; (2) test from an access host to a remote subnet and read ICMP error sources carefully—\"Destination Net Unreachable\" sourced from the default gateway strongly indicates the gateway lacks a route upstream (control-plane/routing failure), while \"Destination Host Unreachable\" sourced from the host suggests local L2/ARP/gateway adjacency problems; (3) validate host IP/gateway settings via ip addr/ip route to confirm the host is in the expected subnet and pointing to the correct gateway. This pattern works because it separates data-plane L2 faults from L3 routing-table faults using deterministic ICMP semantics and minimal commands.", "tags": ["troubleshooting", "layered-isolation", "icmp-semantics", "default-gateway", "ospf", "routing-table"], "confidence": 0.84, "step_type": "decision", "tools_used": ["ping", "ifconfig", "ip addr", "ip route"], "freq": 4, "utility": 3}}
{"workspace_id": "nika_v1", "memory_id": "aa0490b628134e1495b6232e886419ec", "memory_type": "task", "when_to_use": "When an end host cannot reach services across a routed Clos/EBGP fabric and you need to quickly determine whether the failure is routing/control-plane vs. service/application.", "content": "Use a dependency-first triage sequence from the client: (1) verify local L3 adjacency by pinging the default gateway; (2) ping a remote service IP (e.g., DNS server) and interpret ICMP errors. If the gateway returns \"Destination Net Unreachable\", treat it as a strong indicator of missing routes on the first-hop router (control-plane/RIB issue) rather than link-level loss. Then attempt name resolution (getent/dig) and HTTP (curl) only after establishing whether L3 reachability exists, to avoid conflating DNS/app failures with routing failures.", "score": 0.86, "time_created": "2026-01-17 00:09:08", "time_modified": "2026-01-17 00:09:08", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When an end host cannot reach services across a routed Clos/EBGP fabric and you need to quickly determine whether the failure is routing/control-plane vs. service/application.", "experience": "Use a dependency-first triage sequence from the client: (1) verify local L3 adjacency by pinging the default gateway; (2) ping a remote service IP (e.g., DNS server) and interpret ICMP errors. If the gateway returns \"Destination Net Unreachable\", treat it as a strong indicator of missing routes on the first-hop router (control-plane/RIB issue) rather than link-level loss. Then attempt name resolution (getent/dig) and HTTP (curl) only after establishing whether L3 reachability exists, to avoid conflating DNS/app failures with routing failures.", "tags": ["clos", "ebgp", "triage", "client-first", "icmp-unreachable", "dependency-chain"], "confidence": 0.82, "step_type": "decision", "tools_used": ["ping", "getent", "dig", "curl"]}}
{"workspace_id": "nika_v1", "memory_id": "7745a5805e2e474e9c4d2ebea0b1eba9", "memory_type": "task", "when_to_use": "When some paths fail while others succeed in a routed multi-zone network (e.g., internal LANs + external LANs + VPN overlay) and you need to localize whether the issue is endpoint-specific vs routing-plane-wide.", "content": "Start with a small reachability matrix across representative endpoints (internal host↔internal host, internal host↔external service, external service↔internal host). Use results to segment the problem domain: if most pairs are OK but one source shows 'Network is unreachable', treat it as a local routing/config issue on that node rather than RIP/underlay failure. This worked because it quickly proved the underlay (routers + external LAN) was healthy using host_2 and servers, and isolated failures to host_1 before spending effort on router debugging.", "score": 0.5341785971252216, "time_created": "2026-01-16 23:52:43", "time_modified": "2026-01-16 23:52:43", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When some paths fail while others succeed in a routed multi-zone network (e.g., internal LANs + external LANs + VPN overlay) and you need to localize whether the issue is endpoint-specific vs routing-plane-wide.", "experience": "Start with a small reachability matrix across representative endpoints (internal host↔internal host, internal host↔external service, external service↔internal host). Use results to segment the problem domain: if most pairs are OK but one source shows 'Network is unreachable', treat it as a local routing/config issue on that node rather than RIP/underlay failure. This worked because it quickly proved the underlay (routers + external LAN) was healthy using host_2 and servers, and isolated failures to host_1 before spending effort on router debugging.", "tags": ["triage", "reachability-matrix", "localization", "icmp", "underlay-vs-endpoint"], "confidence": 0.84, "step_type": "decision", "tools_used": ["connectivity_test"], "freq": 1}}
{"workspace_id": "nika_v1", "memory_id": "182cde764e0e466c99d4a1ee5ab8d722", "memory_type": "task", "when_to_use": "When end-to-end connectivity is reported broken in a routed Clos/EBGP fabric and you need to quickly determine whether the issue is in the network core or at an endpoint.", "content": "Start troubleshooting at the edge by validating the client host’s local network state before diving into routing. Pull `ip -br addr`/`ifconfig` and `ip route` from the client, then attempt a ping to the directly connected first-hop router. Interpreting an immediate `Network is unreachable` as a local host routing/interface problem (not ACL/remote path loss) quickly localizes the fault. This worked because the client had no `eth0`, no IP, and no routes, which explains all downstream failures regardless of fabric health.", "score": 0.86, "time_created": "2026-01-17 00:29:03", "time_modified": "2026-01-17 00:29:03", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When end-to-end connectivity is reported broken in a routed Clos/EBGP fabric and you need to quickly determine whether the issue is in the network core or at an endpoint.", "experience": "Start troubleshooting at the edge by validating the client host’s local network state before diving into routing. Pull `ip -br addr`/`ifconfig` and `ip route` from the client, then attempt a ping to the directly connected first-hop router. Interpreting an immediate `Network is unreachable` as a local host routing/interface problem (not ACL/remote path loss) quickly localizes the fault. This worked because the client had no `eth0`, no IP, and no routes, which explains all downstream failures regardless of fabric health.", "tags": ["fault-localization", "edge-first", "clos", "endpoint", "ip-route", "network-unreachable"], "confidence": 0.9, "step_type": "decision", "tools_used": ["ip_addr", "ip_route", "ping", "ifconfig"]}}
{"workspace_id": "nika_v1", "memory_id": "5a61c3575578462f84a26132aa043d9c", "memory_type": "task", "when_to_use": "When end-to-end connectivity is reported broken in a routed enterprise topology and you need to quickly localize whether the fault is in endpoints, services, or the routing/control-plane.", "content": "Start with a minimal cross-section connectivity matrix: test server-farm internal reachability (DNS<->web, web<->web) and then test from edge hosts to (a) their default gateway and (b) a known server-farm IP. Interpret ICMP error sources: if the gateway returns \"Destination Net Unreachable\", the gateway lacks a route upstream; if the host itself reports \"Destination Host Unreachable\" while pinging its gateway, suspect L2/ARP or wrong gateway. This pattern rapidly separates 'server/service healthy' from 'routing path broken' and narrows the blast radius before diving into routing daemons.", "score": 0.5550058320159652, "time_created": "2026-01-16 22:42:23", "time_modified": "2026-01-16 22:42:23", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When end-to-end connectivity is reported broken in a routed enterprise topology and you need to quickly localize whether the fault is in endpoints, services, or the routing/control-plane.", "experience": "Start with a minimal cross-section connectivity matrix: test server-farm internal reachability (DNS<->web, web<->web) and then test from edge hosts to (a) their default gateway and (b) a known server-farm IP. Interpret ICMP error sources: if the gateway returns \"Destination Net Unreachable\", the gateway lacks a route upstream; if the host itself reports \"Destination Host Unreachable\" while pinging its gateway, suspect L2/ARP or wrong gateway. This pattern rapidly separates 'server/service healthy' from 'routing path broken' and narrows the blast radius before diving into routing daemons.", "tags": ["triage", "fault-localization", "icmp", "connectivity-matrix", "edge-vs-core", "routing"], "confidence": 0.86, "step_type": "decision", "tools_used": ["ping_matrix", "host_ping"], "freq": 12, "utility": 8}}
{"workspace_id": "nika_v1", "memory_id": "1a33f7e6272b4fb187476fbc3ff44a1d", "memory_type": "task", "when_to_use": "SDN/L2 access network troubleshooting when some host-to-host pings succeed but failures appear clustered around a specific endpoint (e.g., ping results show 'unknown' or only one host is unreachable).", "content": "Start with a quick reachability sweep to identify whether the issue is systemic (fabric/controller) or localized (single host/link). Use the pattern of successful paths (e.g., host_2->host_4 and host_3->host_4 OK) to infer that the core forwarding plane is likely functioning. Then focus investigation on the outlier endpoint(s) implicated by the sweep. This works because working multi-hop host-to-host traffic in a star topology strongly suggests the central switch + at least some edge programming is healthy, narrowing the fault domain early.", "score": 0.82, "time_created": "2026-01-17 00:56:56", "time_modified": "2026-01-17 00:56:56", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "SDN/L2 access network troubleshooting when some host-to-host pings succeed but failures appear clustered around a specific endpoint (e.g., ping results show 'unknown' or only one host is unreachable).", "experience": "Start with a quick reachability sweep to identify whether the issue is systemic (fabric/controller) or localized (single host/link). Use the pattern of successful paths (e.g., host_2->host_4 and host_3->host_4 OK) to infer that the core forwarding plane is likely functioning. Then focus investigation on the outlier endpoint(s) implicated by the sweep. This works because working multi-hop host-to-host traffic in a star topology strongly suggests the central switch + at least some edge programming is healthy, narrowing the fault domain early.", "tags": ["sdn", "fault-domain-isolation", "ping-sweep", "star-topology", "data-plane"], "confidence": 0.84, "step_type": "decision", "tools_used": ["reachability_test"]}}
{"workspace_id": "nika_v1", "memory_id": "7bc5694a0f4b4c3e978bde5dba34feb4", "memory_type": "task", "when_to_use": "When end-to-end connectivity fails in a routed Clos/EBGP fabric and you need to quickly determine whether the issue is at the host edge vs. the routing fabric.", "content": "Start with symmetric endpoint probes (ping both directions). Treat different failure strings as localization signals: (a) \"ping: connect: Network is unreachable\" typically indicates the source host lacks a usable egress (interface down and/or missing routes) before any packet leaves; (b) ICMP \"Destination Net Unreachable\" sourced by the default gateway indicates the host-to-gateway path works and the gateway is rejecting/doesn't have a route. This two-sided probing rapidly narrows the fault domain before spending time on control-plane checks.", "score": 0.82, "time_created": "2026-01-17 01:02:31", "time_modified": "2026-01-17 01:02:31", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When end-to-end connectivity fails in a routed Clos/EBGP fabric and you need to quickly determine whether the issue is at the host edge vs. the routing fabric.", "experience": "Start with symmetric endpoint probes (ping both directions). Treat different failure strings as localization signals: (a) \"ping: connect: Network is unreachable\" typically indicates the source host lacks a usable egress (interface down and/or missing routes) before any packet leaves; (b) ICMP \"Destination Net Unreachable\" sourced by the default gateway indicates the host-to-gateway path works and the gateway is rejecting/doesn't have a route. This two-sided probing rapidly narrows the fault domain before spending time on control-plane checks.", "tags": ["triage", "fault-localization", "ping", "icmp-unreachable", "host-vs-fabric", "clos"], "confidence": 0.86, "step_type": "decision", "tools_used": ["ping"]}}
{"workspace_id": "nika_v1", "memory_id": "b99b949731a043c585930a1db4dbb9a4", "memory_type": "task", "when_to_use": "End-to-end service access fails (HTTP/DNS) and initial probes show immediate local errors (e.g., 'Network is unreachable') rather than timeouts.", "content": "Start with simple reachability tests from the client to both the service IPs and the first-hop gateway. If errors are immediate/local, pivot to validating the client host's L1/L2 and basic IP stack: check interface admin/oper state (UP/DOWN), IP address presence, and whether a routing table/default route exists. This quickly distinguishes 'client-side access/link failure' from fabric/BGP issues and prevents wasting time on core routing when the packet never leaves the host.", "score": 0.86, "time_created": "2026-01-17 01:07:26", "time_modified": "2026-01-17 01:07:26", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "End-to-end service access fails (HTTP/DNS) and initial probes show immediate local errors (e.g., 'Network is unreachable') rather than timeouts.", "experience": "Start with simple reachability tests from the client to both the service IPs and the first-hop gateway. If errors are immediate/local, pivot to validating the client host's L1/L2 and basic IP stack: check interface admin/oper state (UP/DOWN), IP address presence, and whether a routing table/default route exists. This quickly distinguishes 'client-side access/link failure' from fabric/BGP issues and prevents wasting time on core routing when the packet never leaves the host.", "tags": ["triage", "client-first", "l1-l3-basics", "network-is-unreachable", "default-route", "gateway-reachability"], "confidence": 0.86, "step_type": "decision", "tools_used": ["ping", "ip_addr", "ip_route"]}}
{"workspace_id": "nika_v1", "memory_id": "2d151dc1b6c34244b9b37870c15ded47", "memory_type": "task", "when_to_use": "When end hosts report total loss of connectivity in a routed enterprise network that relies on DHCP + dynamic routing (e.g., OSPF/FRR).", "content": "Start troubleshooting at the edge by validating host L3 viability before investigating routing/services. Pull `ip addr` and `ip route` from each host; if IPv4 is missing and the route table is empty, treat this as a DHCP/on-link failure rather than a core routing issue. Confirm by attempting a simple ping (gateway/DHCP server); a \"Network is unreachable\" error is a strong indicator the host has no usable L3 config, not merely upstream packet loss. This quickly narrows the fault domain to DHCP/edge L2 rather than spending time on OSPF or server apps.", "score": 0.5571940570595764, "time_created": "2026-01-17 00:02:35", "time_modified": "2026-01-17 00:02:35", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When end hosts report total loss of connectivity in a routed enterprise network that relies on DHCP + dynamic routing (e.g., OSPF/FRR).", "experience": "Start troubleshooting at the edge by validating host L3 viability before investigating routing/services. Pull `ip addr` and `ip route` from each host; if IPv4 is missing and the route table is empty, treat this as a DHCP/on-link failure rather than a core routing issue. Confirm by attempting a simple ping (gateway/DHCP server); a \"Network is unreachable\" error is a strong indicator the host has no usable L3 config, not merely upstream packet loss. This quickly narrows the fault domain to DHCP/edge L2 rather than spending time on OSPF or server apps.", "tags": ["dhcp", "host-missing-ip", "edge-first", "layer3-baseline", "fault-localization"], "confidence": 0.86, "step_type": "decision", "tools_used": ["host_ifconfig", "host_ip_route", "ping"], "freq": 3, "utility": 2}}
{"workspace_id": "nika_v1", "memory_id": "3af7d71fdcae4d688f8a8960c81214ca", "memory_type": "task", "when_to_use": "When end-to-end app reachability is reported broken in a hierarchical routed network (core/distribution/access) and you need to quickly determine whether the issue is in the server farm, the core, or the user edge.", "content": "Start by running a small matrix of ICMP tests from multiple vantage points (at least one server-farm node and one or more user hosts) to separate 'service segment health' from 'access/edge routing' faults. In this trajectory, confirming 0% loss and low RTT within 10.200.0.0/24 (DNS<->web) immediately ruled out server/service failure and focused the investigation on user-side routing. This works because it establishes a known-good baseline segment before spending time on routing internals.", "score": 0.82, "time_created": "2026-01-17 01:14:12", "time_modified": "2026-01-17 01:14:12", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When end-to-end app reachability is reported broken in a hierarchical routed network (core/distribution/access) and you need to quickly determine whether the issue is in the server farm, the core, or the user edge.", "experience": "Start by running a small matrix of ICMP tests from multiple vantage points (at least one server-farm node and one or more user hosts) to separate 'service segment health' from 'access/edge routing' faults. In this trajectory, confirming 0% loss and low RTT within 10.200.0.0/24 (DNS<->web) immediately ruled out server/service failure and focused the investigation on user-side routing. This works because it establishes a known-good baseline segment before spending time on routing internals.", "tags": ["triage", "vantage-points", "ping-matrix", "fault-domain-isolation", "server-farm"], "confidence": 0.82, "step_type": "decision", "tools_used": ["connectivity_check"]}}
{"workspace_id": "nika_v1", "memory_id": "221a325a85b74f7e8877524dd0e702fc", "memory_type": "task", "when_to_use": "When diagnosing a routed multi-segment network where some destinations are reachable but a specific host-to-host path fails (partial connectivity).", "content": "Start with a quick reachability matrix (multi-source pings) to separate 'global routing failure' from 'prefix- or endpoint-specific failure'. Then run targeted pings that mirror the failing path (e.g., host_2 -> host_1) and compare against known-good paths (e.g., host_2 -> external servers). This pattern quickly localizes the fault domain: here, host_2 could reach 20.0.0.0/24 but not 10.0.0.2, indicating the underlay and RIP domain were mostly healthy and the issue was specific to the 10.0.0.0/24 endpoint/path.", "score": 0.5367656598463032, "time_created": "2026-01-16 23:33:39", "time_modified": "2026-01-16 23:33:39", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When diagnosing a routed multi-segment network where some destinations are reachable but a specific host-to-host path fails (partial connectivity).", "experience": "Start with a quick reachability matrix (multi-source pings) to separate 'global routing failure' from 'prefix- or endpoint-specific failure'. Then run targeted pings that mirror the failing path (e.g., host_2 -> host_1) and compare against known-good paths (e.g., host_2 -> external servers). This pattern quickly localizes the fault domain: here, host_2 could reach 20.0.0.0/24 but not 10.0.0.2, indicating the underlay and RIP domain were mostly healthy and the issue was specific to the 10.0.0.0/24 endpoint/path.", "tags": ["triage", "reachability-matrix", "fault-localization", "icmp", "partial-connectivity"], "confidence": 0.82, "step_type": "action", "tools_used": ["reachability_matrix", "ping"], "freq": 4, "utility": 2}}
{"workspace_id": "nika_v1", "memory_id": "d94919639d5d4e05ae7909032745f7e8", "memory_type": "task", "when_to_use": "When diagnosing SDN/L2 access-network issues where some hosts communicate but one or more hosts show reachability failures.", "content": "Start with a small reachability matrix (ping sweep) across multiple host pairs to classify the failure domain. If most host-to-host paths succeed but failures cluster around a single host, treat it as an endpoint/edge-attachment issue rather than a controller/fabric-wide issue. Validate by running targeted pings in both directions (isolated host -> healthy host and healthy host -> isolated host) to distinguish 'cannot send' vs 'cannot receive'. This pattern quickly narrows scope and prevents unnecessary controller/flow debugging when the fabric is evidently forwarding for other hosts.", "score": 0.86, "time_created": "2026-01-17 01:20:10", "time_modified": "2026-01-17 01:20:10", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When diagnosing SDN/L2 access-network issues where some hosts communicate but one or more hosts show reachability failures.", "experience": "Start with a small reachability matrix (ping sweep) across multiple host pairs to classify the failure domain. If most host-to-host paths succeed but failures cluster around a single host, treat it as an endpoint/edge-attachment issue rather than a controller/fabric-wide issue. Validate by running targeted pings in both directions (isolated host -> healthy host and healthy host -> isolated host) to distinguish 'cannot send' vs 'cannot receive'. This pattern quickly narrows scope and prevents unnecessary controller/flow debugging when the fabric is evidently forwarding for other hosts.", "tags": ["sdn", "fault-isolation", "ping-matrix", "dataplane", "scoping"], "confidence": 0.84, "step_type": "decision", "tools_used": ["reachability_test", "ping"]}}
{"workspace_id": "nika_v1", "memory_id": "3039a5d417714ef69c80979b8a5a28ef", "memory_type": "task", "when_to_use": "When host-to-host reachability is partially failing in an SDN fabric and you need to quickly determine whether the issue is endpoint-specific vs fabric/controller-wide.", "content": "Start with a small matrix of targeted pings that covers (a) same-leaf host pair and (b) cross-leaf host pair(s). Compare outcomes to spot patterns (e.g., only flows involving one host fail). Then validate from both directions (A->B and B->A) to distinguish 'destination unreachable' vs local send-path failures. This pattern worked because it rapidly localized the fault domain: successful pings between other hosts demonstrated the spine/leaf forwarding path was generally functional, while consistent failures only when host_1_1 participated indicated a single-endpoint outage.", "score": 0.84, "time_created": "2026-01-17 01:23:09", "time_modified": "2026-01-17 01:23:09", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When host-to-host reachability is partially failing in an SDN fabric and you need to quickly determine whether the issue is endpoint-specific vs fabric/controller-wide.", "experience": "Start with a small matrix of targeted pings that covers (a) same-leaf host pair and (b) cross-leaf host pair(s). Compare outcomes to spot patterns (e.g., only flows involving one host fail). Then validate from both directions (A->B and B->A) to distinguish 'destination unreachable' vs local send-path failures. This pattern worked because it rapidly localized the fault domain: successful pings between other hosts demonstrated the spine/leaf forwarding path was generally functional, while consistent failures only when host_1_1 participated indicated a single-endpoint outage.", "tags": ["sdn", "spine-leaf", "fault-localization", "ping-matrix", "endpoint-vs-fabric"], "confidence": 0.83, "step_type": "decision", "tools_used": ["ping_all", "ping"]}}
{"workspace_id": "nika_v1", "memory_id": "a72c775eb0274fdeb80712184dc94f41", "memory_type": "task", "when_to_use": "When end-to-end host connectivity in a Clos/EVPN/EBGP fabric is reported broken and you need to quickly localize whether the fault is at the host edge, underlay links, or routing control-plane.", "content": "Use a bottom-up triage sequence: (1) attempt host-to-host ping both directions and record the exact ICMP error text/source IP; (2) immediately inspect each host's IP/mask/default route and validate L2/L3 adjacency by pinging the on-link leaf SVI/interface IP; (3) if host adjacency is OK but inter-subnet fails, pivot to router underlay sanity by collecting `ip -br addr` and `ip route` on leaves/spines/super-spine to confirm interfaces are UP and /31 connected routes exist. This pattern works because it establishes a fault boundary early: ICMP 'Host Unreachable' sourced from the host typically indicates local next-hop/gateway issues, while 'Net Unreachable' sourced from a leaf indicates missing routes beyond the leaf. The quick gateway ping confirms whether the host can reach its intended first hop independent of BGP.", "score": 0.5903667323559089, "time_created": "2026-01-16 22:27:45", "time_modified": "2026-01-16 22:27:45", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When end-to-end host connectivity in a Clos/EVPN/EBGP fabric is reported broken and you need to quickly localize whether the fault is at the host edge, underlay links, or routing control-plane.", "experience": "Use a bottom-up triage sequence: (1) attempt host-to-host ping both directions and record the exact ICMP error text/source IP; (2) immediately inspect each host's IP/mask/default route and validate L2/L3 adjacency by pinging the on-link leaf SVI/interface IP; (3) if host adjacency is OK but inter-subnet fails, pivot to router underlay sanity by collecting `ip -br addr` and `ip route` on leaves/spines/super-spine to confirm interfaces are UP and /31 connected routes exist. This pattern works because it establishes a fault boundary early: ICMP 'Host Unreachable' sourced from the host typically indicates local next-hop/gateway issues, while 'Net Unreachable' sourced from a leaf indicates missing routes beyond the leaf. The quick gateway ping confirms whether the host can reach its intended first hop independent of BGP.", "tags": ["clos", "fat-tree", "triage", "icmp-errors", "default-gateway", "fault-boundary", "underlay"], "confidence": 0.86, "step_type": "decision", "tools_used": ["ping", "ip -br addr", "ip route"], "freq": 14, "utility": 8}}
{"workspace_id": "nika_v1", "memory_id": "44e890a4e0c343b88750fcb2a58d585f", "memory_type": "task", "when_to_use": "When diagnosing SDN/OpenFlow lab topologies where end-to-end connectivity issues could stem from either endpoint availability, controller connectivity, or missing dataplane programming.", "content": "Start by validating endpoint liveness and basic L3 config per host (interface up, IP/mask, routes). Treat tool/exec errors (e.g., container not running) as first-class evidence of an endpoint outage and explicitly scope expected failures to that host. This quickly separates \"network\" problems from \"host down\" problems and prevents over-investigating the fabric.", "score": 0.6086995044361265, "time_created": "2026-01-16 21:33:43", "time_modified": "2026-01-16 21:33:43", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When diagnosing SDN/OpenFlow lab topologies where end-to-end connectivity issues could stem from either endpoint availability, controller connectivity, or missing dataplane programming.", "experience": "Start by validating endpoint liveness and basic L3 config per host (interface up, IP/mask, routes). Treat tool/exec errors (e.g., container not running) as first-class evidence of an endpoint outage and explicitly scope expected failures to that host. This quickly separates \"network\" problems from \"host down\" problems and prevents over-investigating the fabric.", "tags": ["sdn", "openflow", "triage", "endpoint-health", "container-down", "scope-failures"], "confidence": 0.86, "step_type": "decision", "tools_used": ["host_ifconfig", "host_ip_addr", "host_ip_route"], "freq": 10, "utility": 9}}
{"workspace_id": "nika_v1", "memory_id": "632ec56e4cfe432999dfe0d3554bd691", "memory_type": "task", "when_to_use": "When end-to-end service access fails in a routed Clos and you need to quickly determine whether the fault is in the fabric (BGP/underlay) or at the edge (client/service access).", "content": "Start with a broad, minimal test matrix (client↔DNS, client↔web, service↔service, service↔client) to see which direction/segment is failing, then immediately collect basic L3/L2 state (ip addr/route) on all endpoints and routers. Next, validate the fabric independently by pinging along the expected path from core→spine→leaf→service host. This isolates whether the fabric underlay/access is healthy before spending time on BGP. In this trajectory, internal router/host pings succeeded while client-originated traffic failed, cleanly localizing the issue to the client↔super-spine access edge rather than the EBGP Clos.", "score": 0.78, "time_created": "2026-01-17 03:13:53", "time_modified": "2026-01-17 03:13:53", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When end-to-end service access fails in a routed Clos and you need to quickly determine whether the fault is in the fabric (BGP/underlay) or at the edge (client/service access).", "experience": "Start with a broad, minimal test matrix (client↔DNS, client↔web, service↔service, service↔client) to see which direction/segment is failing, then immediately collect basic L3/L2 state (ip addr/route) on all endpoints and routers. Next, validate the fabric independently by pinging along the expected path from core→spine→leaf→service host. This isolates whether the fabric underlay/access is healthy before spending time on BGP. In this trajectory, internal router/host pings succeeded while client-originated traffic failed, cleanly localizing the issue to the client↔super-spine access edge rather than the EBGP Clos.", "tags": ["clos", "fault-localization", "edge-vs-fabric", "ping-matrix", "ip-route", "underlay-validation"], "confidence": 0.82, "step_type": "decision", "tools_used": ["connectivity_check", "show_interfaces"]}}
{"workspace_id": "nika_v1", "memory_id": "567a5df024c5484e8439b9118408b899", "memory_type": "task", "when_to_use": "When a host shows broad reachability failures (can’t reach gateway/peers/external), but other parts of the network appear healthy.", "content": "Start with a quick multi-endpoint reachability matrix (pings from multiple sources to multiple destinations) to separate a localized edge failure from a core routing failure. In this run, comparing host_1 vs host_2 reachability immediately showed the external zone and routing fabric were largely healthy while host_1 was uniquely failing, which narrowed the fault domain early and prevented unnecessary router/VPN deep-dives.", "score": 0.86, "time_created": "2026-01-17 05:15:27", "time_modified": "2026-01-17 05:15:27", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When a host shows broad reachability failures (can’t reach gateway/peers/external), but other parts of the network appear healthy.", "experience": "Start with a quick multi-endpoint reachability matrix (pings from multiple sources to multiple destinations) to separate a localized edge failure from a core routing failure. In this run, comparing host_1 vs host_2 reachability immediately showed the external zone and routing fabric were largely healthy while host_1 was uniquely failing, which narrowed the fault domain early and prevented unnecessary router/VPN deep-dives.", "tags": ["triage", "reachability-matrix", "fault-domain-isolation", "ping", "baseline-compare"], "confidence": 0.82, "step_type": "decision", "tools_used": ["connectivity_check", "ping"]}}
{"workspace_id": "nika_v1", "memory_id": "a813269b2737424c8d652f7fe223bd55", "memory_type": "task", "when_to_use": "When troubleshooting SDN/L2 fabrics where some host-to-host paths work but others fail, and you need to quickly determine whether the issue is fabric-wide vs localized to a host/edge domain.", "content": "Start with a small reachability matrix (multi-source pings) to identify working vs failing pairs, then immediately pivot to targeted pings for the failing cluster to confirm persistence. Use the pattern of successes (e.g., host_2↔host_4 and host_3↔host_4 OK) to rule out controller/fabric-wide outage and localize the fault domain around the common endpoint (host_1) and its adjacent edge switch (switch_1). This works because clustering failures around a single endpoint is a strong indicator of edge/host-specific issues rather than core forwarding collapse.", "score": 0.84, "time_created": "2026-01-17 05:18:55", "time_modified": "2026-01-17 05:18:55", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When troubleshooting SDN/L2 fabrics where some host-to-host paths work but others fail, and you need to quickly determine whether the issue is fabric-wide vs localized to a host/edge domain.", "experience": "Start with a small reachability matrix (multi-source pings) to identify working vs failing pairs, then immediately pivot to targeted pings for the failing cluster to confirm persistence. Use the pattern of successes (e.g., host_2↔host_4 and host_3↔host_4 OK) to rule out controller/fabric-wide outage and localize the fault domain around the common endpoint (host_1) and its adjacent edge switch (switch_1). This works because clustering failures around a single endpoint is a strong indicator of edge/host-specific issues rather than core forwarding collapse.", "tags": ["sdn", "fault-localization", "reachability-matrix", "ping", "cluster-analysis", "star-topology"], "confidence": 0.83, "step_type": "decision", "tools_used": ["reachability_sweep", "run_ping"]}}
{"workspace_id": "nika_v1", "memory_id": "4a0b6903959c4e89b2221694ad7376d9", "memory_type": "task", "when_to_use": "When initial connectivity results are mixed (some host pairs succeed while others are unknown/failed) and you need to quickly determine whether the issue is fabric-wide or isolated to a single endpoint.", "content": "Start with a broad reachability sweep (ping matrix) to identify which pairs are healthy vs anomalous. Use the pattern of successes to infer fabric health (e.g., inter-leaf pings succeeding suggests spine/leaf dataplane is generally working). Then pivot to the specific failing endpoint(s) for deeper inspection rather than immediately debugging switches/controller. This narrows scope early and prevents unnecessary SDN control-plane investigation when the issue is localized.", "score": 0.86, "time_created": "2026-01-17 05:24:19", "time_modified": "2026-01-17 05:24:19", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When initial connectivity results are mixed (some host pairs succeed while others are unknown/failed) and you need to quickly determine whether the issue is fabric-wide or isolated to a single endpoint.", "experience": "Start with a broad reachability sweep (ping matrix) to identify which pairs are healthy vs anomalous. Use the pattern of successes to infer fabric health (e.g., inter-leaf pings succeeding suggests spine/leaf dataplane is generally working). Then pivot to the specific failing endpoint(s) for deeper inspection rather than immediately debugging switches/controller. This narrows scope early and prevents unnecessary SDN control-plane investigation when the issue is localized.", "tags": ["triage", "ping-matrix", "fault-localization", "sdn", "spine-leaf"], "confidence": 0.78, "step_type": "decision", "tools_used": ["ping_matrix"]}}
{"workspace_id": "nika_v1", "memory_id": "38c694465e84458bba3c1f99b7196c15", "memory_type": "task", "when_to_use": "When you must produce a structured diagnostic report and a final categorical submission (faulty devices + root cause label) from observed symptoms.", "content": "Translate observations into a report that (1) states overall fabric health, (2) lists anomalies with concrete evidence, (3) localizes the fault to specific device(s), and (4) provides evidence-based hypotheses without prescribing fixes. Then map the best-supported hypothesis to the closest standardized root-cause label from an allowed taxonomy (e.g., ARP works but ICMP fails -> choose an ICMP-specific block label such as 'icmp_acl_block'). This ensures the final submission is consistent, auditable, and aligned with the platform’s expected categories.", "score": 0.86, "time_created": "2026-01-17 05:24:19", "time_modified": "2026-01-17 05:24:19", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When you must produce a structured diagnostic report and a final categorical submission (faulty devices + root cause label) from observed symptoms.", "experience": "Translate observations into a report that (1) states overall fabric health, (2) lists anomalies with concrete evidence, (3) localizes the fault to specific device(s), and (4) provides evidence-based hypotheses without prescribing fixes. Then map the best-supported hypothesis to the closest standardized root-cause label from an allowed taxonomy (e.g., ARP works but ICMP fails -> choose an ICMP-specific block label such as 'icmp_acl_block'). This ensures the final submission is consistent, auditable, and aligned with the platform’s expected categories.", "tags": ["reporting", "evidence-based", "taxonomy-mapping", "submission", "root-cause-labeling"], "confidence": 0.74, "step_type": "action", "tools_used": ["root_cause_options", "submit"]}}
{"workspace_id": "nika_v1", "memory_id": "53cedaea98784dab97e3a7315a0d2641", "memory_type": "task", "when_to_use": "When east-west host connectivity fails in a Clos/leaf-spine fabric and you need to quickly distinguish control-plane vs access/host issues", "content": "Start with bidirectional host-to-host pings and interpret the failure mode. A first-hop ICMP error like \"Destination Net Unreachable\" sourced from the default gateway strongly localizes the problem to the gateway/router RIB/FIB (missing route/next-hop), whereas a silent timeout suggests drops/blackholes/ACLs or return-path issues. Use this as an early decision point to pivot from generic reachability testing to targeted routing-table inspection on the implicated leaf.", "score": 0.84, "time_created": "2026-01-17 05:53:40", "time_modified": "2026-01-17 05:53:40", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When east-west host connectivity fails in a Clos/leaf-spine fabric and you need to quickly distinguish control-plane vs access/host issues", "experience": "Start with bidirectional host-to-host pings and interpret the failure mode. A first-hop ICMP error like \"Destination Net Unreachable\" sourced from the default gateway strongly localizes the problem to the gateway/router RIB/FIB (missing route/next-hop), whereas a silent timeout suggests drops/blackholes/ACLs or return-path issues. Use this as an early decision point to pivot from generic reachability testing to targeted routing-table inspection on the implicated leaf.", "tags": ["icmp", "failure-signature", "first-hop-unreachable", "fault-localization", "clos", "leaf-spine"], "confidence": 0.82, "step_type": "decision", "tools_used": ["ping"]}}
{"workspace_id": "nika_v1", "memory_id": "4444931fe8294235a50f2839c0ad518f", "memory_type": "task", "when_to_use": "When end-to-end user access fails in a routed enterprise design that depends on DHCP relay and dynamic routing (e.g., OSPF), and you need to quickly localize whether the problem is access/DHCP vs. routing/core.", "content": "Start by separating \"server-farm internal health\" from \"user-subnet reachability\" using lightweight reachability checks. Verify that servers within the same subnet (e.g., 10.200.0.0/24) can ping each other (proves L2/L3 local segment is healthy). In parallel, check user hosts' L3 state (ip addr/ifconfig + ip route). If hosts have no IPv4 and empty routing, treat it as a DHCP provisioning symptom, not the root cause. This two-sided validation prevents misattributing the outage to DNS/web/DHCP server failure when the real issue is upstream routing/relay reachability.", "score": 0.74, "time_created": "2026-01-17 06:48:22", "time_modified": "2026-01-17 06:48:22", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When end-to-end user access fails in a routed enterprise design that depends on DHCP relay and dynamic routing (e.g., OSPF), and you need to quickly localize whether the problem is access/DHCP vs. routing/core.", "experience": "Start by separating \"server-farm internal health\" from \"user-subnet reachability\" using lightweight reachability checks. Verify that servers within the same subnet (e.g., 10.200.0.0/24) can ping each other (proves L2/L3 local segment is healthy). In parallel, check user hosts' L3 state (ip addr/ifconfig + ip route). If hosts have no IPv4 and empty routing, treat it as a DHCP provisioning symptom, not the root cause. This two-sided validation prevents misattributing the outage to DNS/web/DHCP server failure when the real issue is upstream routing/relay reachability.", "tags": ["triage", "dhcp", "dhcp-relay", "scope-isolation", "server-farm", "host-missing-ip"], "confidence": 0.82, "step_type": "decision", "tools_used": ["reachability_matrix", "get_node_state"]}}
{"workspace_id": "nika_v1", "memory_id": "be680a7b9c9447d289ef9bb4c072c7e7", "memory_type": "task", "when_to_use": "When end-to-end service reachability (client->DNS/HTTP) fails in a routed Clos/EBGP fabric and you need to quickly distinguish host misconfig vs routing/control-plane failure.", "content": "Use a layered reachability triage: (1) verify host IP/netmask/default-gateway via ifconfig/ip addr + ip route; (2) run pings from client to service IPs and note ICMP error text; (3) run pings from service hosts to each other and to their default gateway. In this run, explicit ICMP 'Destination Net Unreachable' sourced from the first-hop gateways (192.168.0.1 and 10.0.0.1) was a key decision signal that routes were missing (not ACL drops or random loss). Confirming host configs were correct and local L3 to gateways worked narrowed the fault domain away from endpoints and access links and toward the fabric routing plane.", "score": 0.86, "time_created": "2026-01-17 09:26:11", "time_modified": "2026-01-17 09:26:11", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When end-to-end service reachability (client->DNS/HTTP) fails in a routed Clos/EBGP fabric and you need to quickly distinguish host misconfig vs routing/control-plane failure.", "experience": "Use a layered reachability triage: (1) verify host IP/netmask/default-gateway via ifconfig/ip addr + ip route; (2) run pings from client to service IPs and note ICMP error text; (3) run pings from service hosts to each other and to their default gateway. In this run, explicit ICMP 'Destination Net Unreachable' sourced from the first-hop gateways (192.168.0.1 and 10.0.0.1) was a key decision signal that routes were missing (not ACL drops or random loss). Confirming host configs were correct and local L3 to gateways worked narrowed the fault domain away from endpoints and access links and toward the fabric routing plane.", "tags": ["clos", "bgp", "reachability", "icmp", "fault-localization", "host-validation"], "confidence": 0.83, "step_type": "decision", "tools_used": ["ip_addr", "ip_route", "ping"]}}
{"workspace_id": "nika_v1", "memory_id": "8b2575ef881046b1a0eed9ae72abd096", "memory_type": "task", "when_to_use": "When end-to-end reachability is reported broken in a multi-tier routed network (core/distribution/access) and you need to quickly localize whether the issue is host/L2, local subnet, or inter-subnet routing/control-plane.", "content": "Use a layered reachability triage: (1) verify intra-subnet health in the destination segment (e.g., server farm hosts ping each other) to rule out local L2/L3 issues; (2) verify each client host’s L3 basics (IP/mask/default route) and confirm it can ARP/ping its default gateway; (3) attempt cross-subnet ping from the client to a known destination (DNS/server) and interpret the failure mode. In this run, intra-server-farm pings succeeded while client-to-server pings timed out, and the client gateway returned 'Destination Net Unreachable', which strongly indicates the gateway lacks a route (routing/control-plane problem) rather than DNS/app or simple ACL/ICMP filtering. This sequence works because it isolates failure domains with minimal tests and uses ICMP error semantics to pinpoint where routing knowledge is missing.", "score": 0.86, "time_created": "2026-01-17 09:37:57", "time_modified": "2026-01-17 09:37:57", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When end-to-end reachability is reported broken in a multi-tier routed network (core/distribution/access) and you need to quickly localize whether the issue is host/L2, local subnet, or inter-subnet routing/control-plane.", "experience": "Use a layered reachability triage: (1) verify intra-subnet health in the destination segment (e.g., server farm hosts ping each other) to rule out local L2/L3 issues; (2) verify each client host’s L3 basics (IP/mask/default route) and confirm it can ARP/ping its default gateway; (3) attempt cross-subnet ping from the client to a known destination (DNS/server) and interpret the failure mode. In this run, intra-server-farm pings succeeded while client-to-server pings timed out, and the client gateway returned 'Destination Net Unreachable', which strongly indicates the gateway lacks a route (routing/control-plane problem) rather than DNS/app or simple ACL/ICMP filtering. This sequence works because it isolates failure domains with minimal tests and uses ICMP error semantics to pinpoint where routing knowledge is missing.", "tags": ["troubleshooting", "layered-triage", "reachability", "default-gateway", "icmp", "fault-localization"], "confidence": 0.84, "step_type": "decision", "tools_used": ["ping", "ip_addr", "ip_route", "ip_neigh", "connectivity_matrix"]}}
{"workspace_id": "nika_v1", "memory_id": "89b05de3b70041b6a82121c4dc03102e", "memory_type": "task", "when_to_use": "When some end-to-end pings time out in a routed multi-domain lab and you need to quickly determine whether the failure is local LAN, transit routing, or remote LAN/return-path related.", "content": "Start with a connectivity matrix using targeted ICMP tests from representative endpoints on each side (internal host↔internal host, internal host→external server IPs, external server↔external server, gateway→external server). This isolates whether the problem is (a) internal LAN/routing, (b) external LAN, or (c) inter-domain routing/return path. In the trajectory, internal host-to-host succeeded while internal-to-external timed out, yet external LAN pings succeeded and gateway-to-external succeeded—strongly indicating an asymmetric/return-path routing issue rather than a physical/link failure or server outage.", "score": 0.86, "time_created": "2026-01-17 09:40:54", "time_modified": "2026-01-17 09:40:54", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When some end-to-end pings time out in a routed multi-domain lab and you need to quickly determine whether the failure is local LAN, transit routing, or remote LAN/return-path related.", "experience": "Start with a connectivity matrix using targeted ICMP tests from representative endpoints on each side (internal host↔internal host, internal host→external server IPs, external server↔external server, gateway→external server). This isolates whether the problem is (a) internal LAN/routing, (b) external LAN, or (c) inter-domain routing/return path. In the trajectory, internal host-to-host succeeded while internal-to-external timed out, yet external LAN pings succeeded and gateway-to-external succeeded—strongly indicating an asymmetric/return-path routing issue rather than a physical/link failure or server outage.", "tags": ["triage", "ping-matrix", "fault-isolation", "return-path", "asymmetric-routing"], "confidence": 0.84, "step_type": "action", "tools_used": ["ping", "ping_mesh"]}}
{"workspace_id": "nika_v1", "memory_id": "a442649898be49eabc7dab9e3fdaca3e", "memory_type": "task", "when_to_use": "When the environment expects a VPN overlay (WireGuard) to provide access to services, but overlay traffic appears dead or services remain unreachable.", "content": "Correlate overlay interface state with underlay reachability before attributing failures to VPN configuration. Check WG interfaces for UP state and packet counters (RX/TX), and verify whether the client can reach the server’s underlay endpoint IP. In the trajectory, both ends had wg0 UP with correct tunnel IPs, but host_1 had 0 RX/TX and could not ping the VPN server’s underlay IP—supporting the conclusion that the VPN issue was downstream of (caused by) underlay routing failure, not WireGuard misconfiguration.", "score": 0.87, "time_created": "2026-01-17 09:40:54", "time_modified": "2026-01-17 09:40:54", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When the environment expects a VPN overlay (WireGuard) to provide access to services, but overlay traffic appears dead or services remain unreachable.", "experience": "Correlate overlay interface state with underlay reachability before attributing failures to VPN configuration. Check WG interfaces for UP state and packet counters (RX/TX), and verify whether the client can reach the server’s underlay endpoint IP. In the trajectory, both ends had wg0 UP with correct tunnel IPs, but host_1 had 0 RX/TX and could not ping the VPN server’s underlay IP—supporting the conclusion that the VPN issue was downstream of (caused by) underlay routing failure, not WireGuard misconfiguration.", "tags": ["wireguard", "overlay-underlay", "vpn-triage", "packet-counters", "dependency-check"], "confidence": 0.8, "step_type": "observation", "tools_used": ["ifconfig", "ip_route", "ping"]}}
{"workspace_id": "nika_v1", "memory_id": "91df6515d3d34830b567b5add704c414", "memory_type": "task", "when_to_use": "When end-to-end service access (DNS/HTTP) is failing in a routed fabric and you need to quickly determine whether the fault is at the edge (first hop) or inside the routing/control-plane.", "content": "Start by validating the client’s local L3 baseline (IP/netmask/default route) and immediately test first-hop reachability (client -> default gateway, and gateway -> client). If first-hop fails, treat downstream failures (DNS/web subnets) as expected symptoms and avoid spending time on BGP/route debugging. This sequencing worked because it constrained the fault domain early to the client–gateway segment, preventing misattribution to the Clos fabric.", "score": 0.86, "time_created": "2026-01-17 09:53:16", "time_modified": "2026-01-17 09:53:16", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When end-to-end service access (DNS/HTTP) is failing in a routed fabric and you need to quickly determine whether the fault is at the edge (first hop) or inside the routing/control-plane.", "experience": "Start by validating the client’s local L3 baseline (IP/netmask/default route) and immediately test first-hop reachability (client -> default gateway, and gateway -> client). If first-hop fails, treat downstream failures (DNS/web subnets) as expected symptoms and avoid spending time on BGP/route debugging. This sequencing worked because it constrained the fault domain early to the client–gateway segment, preventing misattribution to the Clos fabric.", "tags": ["first-hop", "fault-domain", "edge-vs-core", "routing-triage", "ping"], "confidence": 0.84, "step_type": "decision", "tools_used": ["host_ifconfig/ip_addr/ip_route collection", "ping"]}}
{"workspace_id": "nika_v1", "memory_id": "9bf65dfde1544ab18c300a3787abc730", "memory_type": "task", "when_to_use": "When a multi-zone routed network has partial reachability and you need to quickly determine whether the issue is core routing vs. a single edge host/segment problem.", "content": "Start with a small reachability matrix (host-to-host, host-to-service, service-to-host). If one host (e.g., host_1) fails to reach everything while another host (host_2) succeeds to the same destinations, treat it as a localization signal: the fault is likely on the failing host or its first-hop segment, not in the routing core. Then validate by testing the failing host to its default gateway first; inability to ping the gateway is a strong indicator of a local L2/L3 adjacency issue (ARP, VLAN/bridge, link, host firewall) rather than RIP/route propagation. This pattern works because it uses comparative control tests to avoid prematurely blaming the core and narrows the fault domain to the smallest shared component unique to the failing source.", "score": 0.87, "time_created": "2026-01-17 10:10:23", "time_modified": "2026-01-17 10:10:23", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When a multi-zone routed network has partial reachability and you need to quickly determine whether the issue is core routing vs. a single edge host/segment problem.", "experience": "Start with a small reachability matrix (host-to-host, host-to-service, service-to-host). If one host (e.g., host_1) fails to reach everything while another host (host_2) succeeds to the same destinations, treat it as a localization signal: the fault is likely on the failing host or its first-hop segment, not in the routing core. Then validate by testing the failing host to its default gateway first; inability to ping the gateway is a strong indicator of a local L2/L3 adjacency issue (ARP, VLAN/bridge, link, host firewall) rather than RIP/route propagation. This pattern works because it uses comparative control tests to avoid prematurely blaming the core and narrows the fault domain to the smallest shared component unique to the failing source.", "tags": ["troubleshooting", "fault-localization", "reachability-matrix", "edge-vs-core", "ping", "first-hop"], "confidence": 0.82, "step_type": "decision", "tools_used": ["ping", "connectivity_matrix"]}}
{"workspace_id": "nika_v1", "memory_id": "5fc24660004c4d1fa802c544644c3621", "memory_type": "task", "when_to_use": "When troubleshooting an SDN/L2 access fabric where some pings succeed but there is intermittent loss and the fault domain is unclear (could be host, edge, core, or controller).", "content": "Start with a broad but lightweight reachability sweep (matrix/subset) to spot which source/destination pairs fail, then immediately follow with directed pings from the suspected endpoint(s) to confirm directionality (e.g., loss only when host_X is the source). This pattern quickly distinguishes fabric-wide issues (symmetric/multi-host loss) from edge/host-local issues (directional loss concentrated on a single sender). Directionality is a key decision point: if loss is source-specific, prioritize inspecting the sender's egress path rather than controller/OVS flows.", "score": 0.86, "time_created": "2026-01-17 12:28:15", "time_modified": "2026-01-17 12:28:15", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When troubleshooting an SDN/L2 access fabric where some pings succeed but there is intermittent loss and the fault domain is unclear (could be host, edge, core, or controller).", "experience": "Start with a broad but lightweight reachability sweep (matrix/subset) to spot which source/destination pairs fail, then immediately follow with directed pings from the suspected endpoint(s) to confirm directionality (e.g., loss only when host_X is the source). This pattern quickly distinguishes fabric-wide issues (symmetric/multi-host loss) from edge/host-local issues (directional loss concentrated on a single sender). Directionality is a key decision point: if loss is source-specific, prioritize inspecting the sender's egress path rather than controller/OVS flows.", "tags": ["sdn", "reachability", "ping", "directionality", "fault-localization", "l2"], "confidence": 0.84, "step_type": "decision", "tools_used": ["reachability_matrix", "ping"]}}
{"workspace_id": "nika_v1", "memory_id": "fcf0248a5c0c4a17a3d31c8f8f25d00b", "memory_type": "task", "when_to_use": "When troubleshooting SDN fabrics where reachability exists but packet loss is reported and may be asymmetric (some src→dst pairs healthy while others are lossy).", "content": "Start with a lightweight reachability sweep (multi host-to-host pings) to detect whether loss is fabric-wide or source-specific. Then immediately validate the suspicious direction with longer, repeated pings from the implicated source(s) to multiple destinations. This pattern quickly distinguishes transient ARP/flow warm-up from persistent impairment and localizes the fault domain (single host vs. switching fabric). In the trajectory, the sweep showed loss concentrated on host_1_1 as sender; follow-up pings confirmed persistent high loss from that source.", "score": 0.84, "time_created": "2026-01-17 12:36:24", "time_modified": "2026-01-17 12:36:24", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When troubleshooting SDN fabrics where reachability exists but packet loss is reported and may be asymmetric (some src→dst pairs healthy while others are lossy).", "experience": "Start with a lightweight reachability sweep (multi host-to-host pings) to detect whether loss is fabric-wide or source-specific. Then immediately validate the suspicious direction with longer, repeated pings from the implicated source(s) to multiple destinations. This pattern quickly distinguishes transient ARP/flow warm-up from persistent impairment and localizes the fault domain (single host vs. switching fabric). In the trajectory, the sweep showed loss concentrated on host_1_1 as sender; follow-up pings confirmed persistent high loss from that source.", "tags": ["sdn", "triage", "asymmetric-loss", "ping-matrix", "fault-localization"], "confidence": 0.82, "step_type": "decision", "tools_used": ["reachability_sweep", "ping"]}}
{"workspace_id": "nika_v1", "memory_id": "ab91abc6514447ea98896d7bea46c7e5", "memory_type": "task", "when_to_use": "When end-to-end connectivity between leaf/host subnets fails in a Clos/EBGP fabric and you need to quickly localize whether the failure is edge, underlay, or control-plane.", "content": "Use a hop-by-hop isolation pattern from an available host: (1) verify host IP/default route, (2) ping the default gateway, then (3) ping a remote subnet/gateway. If step (2) succeeds but step (3) returns an ICMP error like \"Destination Net Unreachable\" sourced from the default gateway, treat it as strong evidence the first-hop leaf lacks a route (routing/control-plane issue) rather than a dataplane link drop deeper in the fabric. This narrows scope immediately to the leaf's routing table/BGP state and avoids chasing physical links prematurely.", "score": 0.82, "time_created": "2026-01-17 14:28:37", "time_modified": "2026-01-17 14:28:37", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When end-to-end connectivity between leaf/host subnets fails in a Clos/EBGP fabric and you need to quickly localize whether the failure is edge, underlay, or control-plane.", "experience": "Use a hop-by-hop isolation pattern from an available host: (1) verify host IP/default route, (2) ping the default gateway, then (3) ping a remote subnet/gateway. If step (2) succeeds but step (3) returns an ICMP error like \"Destination Net Unreachable\" sourced from the default gateway, treat it as strong evidence the first-hop leaf lacks a route (routing/control-plane issue) rather than a dataplane link drop deeper in the fabric. This narrows scope immediately to the leaf's routing table/BGP state and avoids chasing physical links prematurely.", "tags": ["clos", "fat-tree", "ebgp", "first-hop-localization", "icmp-unreachable", "route-missing", "triage"], "confidence": 0.84, "step_type": "reasoning", "tools_used": ["host_info", "exec_command"]}}
{"workspace_id": "nika_v1", "memory_id": "69883c43634a4c58a1881e4c8ef2ecaf", "memory_type": "task", "when_to_use": "When an initial automated reachability/health tool fails or returns parsing errors, and you still need to produce a reliable diagnosis from raw device state.", "content": "Fall back to direct per-node CLI sampling to rebuild ground truth. First, verify whether endpoints are even runnable (container/device running state). Then collect `ip addr` + `ip route` from key tiers (server edge router, servers, core, distribution). This pattern worked because it avoided dependence on a brittle aggregator, quickly revealed that both hosts were down, and provided enough L3 evidence (connected routes only on dist routers) to infer a control-plane/routing-plane gap.", "score": 0.82, "time_created": "2026-01-17 14:35:06", "time_modified": "2026-01-17 14:35:06", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When an initial automated reachability/health tool fails or returns parsing errors, and you still need to produce a reliable diagnosis from raw device state.", "experience": "Fall back to direct per-node CLI sampling to rebuild ground truth. First, verify whether endpoints are even runnable (container/device running state). Then collect `ip addr` + `ip route` from key tiers (server edge router, servers, core, distribution). This pattern worked because it avoided dependence on a brittle aggregator, quickly revealed that both hosts were down, and provided enough L3 evidence (connected routes only on dist routers) to infer a control-plane/routing-plane gap.", "tags": ["fallback", "tooling-failure", "ground-truth", "ip-addr", "ip-route", "endpoint-health"], "confidence": 0.84, "step_type": "decision", "tools_used": ["get_reachability", "exec"]}}
{"workspace_id": "nika_v1", "memory_id": "4e196166ca2d434c967765d5ef2f7600", "memory_type": "task", "when_to_use": "When initial automated reachability/health tools fail (e.g., JSON parsing errors) or return ambiguous results, but you still need to assess network health and localize faults.", "content": "Fallback from brittle/aggregated tooling to direct per-node CLI evidence collection. After get_reachability/ping_pair failed due to `ip -j addr` parsing issues, the agent switched to running targeted `ping` and `ip addr/ip route` via exec_shell on specific nodes. This preserved forward progress, produced verifiable dataplane evidence, and avoided being blocked by a tooling defect. Key technique: treat tool failures as measurement issues and immediately pivot to simpler primitives (ping, ip route) on known-good nodes.", "score": 0.87, "time_created": "2026-01-17 14:42:32", "time_modified": "2026-01-17 14:42:32", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When initial automated reachability/health tools fail (e.g., JSON parsing errors) or return ambiguous results, but you still need to assess network health and localize faults.", "experience": "Fallback from brittle/aggregated tooling to direct per-node CLI evidence collection. After get_reachability/ping_pair failed due to `ip -j addr` parsing issues, the agent switched to running targeted `ping` and `ip addr/ip route` via exec_shell on specific nodes. This preserved forward progress, produced verifiable dataplane evidence, and avoided being blocked by a tooling defect. Key technique: treat tool failures as measurement issues and immediately pivot to simpler primitives (ping, ip route) on known-good nodes.", "tags": ["tool-fallback", "evidence-collection", "exec_shell", "reachability", "diagnostics"], "confidence": 0.84, "step_type": "decision", "tools_used": ["get_reachability", "ping_pair", "exec_shell"]}}
{"workspace_id": "nika_v1", "memory_id": "d869f4da02d945ef9e3a1efdf9d4bd1a", "memory_type": "task", "when_to_use": "When an automated topology/reachability tool fails or returns parsing/collection errors (e.g., cannot parse `ip -j addr`), especially in containerized/mininet-like labs where endpoints may be stopped.", "content": "Treat tooling failures as a diagnostic signal and immediately pivot to per-endpoint liveness checks. Query each host for basic interface/route state; if a host command returns a container/runtime error (e.g., “container is not running”), classify this as an endpoint outage rather than a dataplane issue. This prevents misattributing a data-collection exception to SDN/controller or fabric failure and quickly localizes the fault domain to the host itself.", "score": 0.86, "time_created": "2026-01-17 14:44:45", "time_modified": "2026-01-17 14:44:45", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When an automated topology/reachability tool fails or returns parsing/collection errors (e.g., cannot parse `ip -j addr`), especially in containerized/mininet-like labs where endpoints may be stopped.", "experience": "Treat tooling failures as a diagnostic signal and immediately pivot to per-endpoint liveness checks. Query each host for basic interface/route state; if a host command returns a container/runtime error (e.g., “container is not running”), classify this as an endpoint outage rather than a dataplane issue. This prevents misattributing a data-collection exception to SDN/controller or fabric failure and quickly localizes the fault domain to the host itself.", "tags": ["tooling-failure", "endpoint-liveness", "container-not-running", "fault-localization", "sdn-troubleshooting"], "confidence": 0.86, "step_type": "decision", "tools_used": ["get_host_info"]}}
{"workspace_id": "nika_v1", "memory_id": "85df08b3d89049f683bd26492ab41878", "memory_type": "task", "when_to_use": "When an SDN lab reports connectivity issues and automated topology/reachability tooling fails or returns errors.", "content": "Fallback to a manual, layered triage sequence: (1) verify endpoint/container liveness by running a simple command on each host; treat 'container not running' as an endpoint outage that can invalidate tests involving that host. (2) For remaining live hosts, confirm L2/L3 prerequisites with `ip link/addr/route` (interface UP, correct /24, on-link route). (3) Probe data-plane behavior with targeted pings that include both same-leaf and cross-leaf pairs; if even same-leaf fails, bias diagnosis toward leaf forwarding/controller programming rather than spine/interconnect. This works because it quickly separates endpoint failures from fabric-wide forwarding failures and uses minimal, high-signal checks.", "score": 0.86, "time_created": "2026-01-17 14:47:22", "time_modified": "2026-01-17 14:47:22", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When an SDN lab reports connectivity issues and automated topology/reachability tooling fails or returns errors.", "experience": "Fallback to a manual, layered triage sequence: (1) verify endpoint/container liveness by running a simple command on each host; treat 'container not running' as an endpoint outage that can invalidate tests involving that host. (2) For remaining live hosts, confirm L2/L3 prerequisites with `ip link/addr/route` (interface UP, correct /24, on-link route). (3) Probe data-plane behavior with targeted pings that include both same-leaf and cross-leaf pairs; if even same-leaf fails, bias diagnosis toward leaf forwarding/controller programming rather than spine/interconnect. This works because it quickly separates endpoint failures from fabric-wide forwarding failures and uses minimal, high-signal checks.", "tags": ["sdn", "spine-leaf", "triage", "reachability", "host-liveness", "ip-route", "ping-matrix"], "confidence": 0.82, "step_type": "action", "tools_used": ["exec_on_host"]}}
{"workspace_id": "nika_v1", "memory_id": "f80766f74fe5466e8cfe2b15612e6a0f", "memory_type": "task", "when_to_use": "When troubleshooting Clos/EBGP fabrics where end-to-end reachability is unclear or test results are missing/\"unknown\"; especially when host connectivity is suspected.", "content": "Start at the edge and validate host L3 identity and default routing before diving into BGP. Pull `ip addr`/`ifconfig` and `ip route` from each host, then compare against the expected per-leaf /24 scheme (10.<pod>.<leaf>.0/24) and the connected leaf’s host-facing gateway IP. In this case, inspecting both hosts immediately revealed a duplicate IP (pc_0_0 and pc_0_1 both 10.0.0.2/24) and a subnet mismatch (pc_0_1 should align with leaf_router_0_1’s 10.0.1.1/24). This quickly localizes faults to the edge and prevents misattributing failures to the fabric core.", "score": 0.86, "time_created": "2026-01-17 14:52:47", "time_modified": "2026-01-17 14:52:47", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When troubleshooting Clos/EBGP fabrics where end-to-end reachability is unclear or test results are missing/\"unknown\"; especially when host connectivity is suspected.", "experience": "Start at the edge and validate host L3 identity and default routing before diving into BGP. Pull `ip addr`/`ifconfig` and `ip route` from each host, then compare against the expected per-leaf /24 scheme (10.<pod>.<leaf>.0/24) and the connected leaf’s host-facing gateway IP. In this case, inspecting both hosts immediately revealed a duplicate IP (pc_0_0 and pc_0_1 both 10.0.0.2/24) and a subnet mismatch (pc_0_1 should align with leaf_router_0_1’s 10.0.1.1/24). This quickly localizes faults to the edge and prevents misattributing failures to the fabric core.", "tags": ["clos", "fat-tree", "ebgp", "edge-validation", "host-ip-conflict", "subnet-mismatch", "default-gateway"], "confidence": 0.86, "step_type": "action", "tools_used": ["get_ifconfig", "get_ip_route"]}}
{"workspace_id": "nika_v1", "memory_id": "1e52af65b64644aa8e5a2d6352164e65", "memory_type": "task", "when_to_use": "When SDN fabric connectivity looks mostly healthy but there are intermittent/\"unknown\" reachability results or inconsistent host-to-host behavior in a single L2 access subnet.", "content": "Start with a broad reachability sweep (multi-host ping matrix) to quickly separate fabric-wide forwarding failures from endpoint-specific anomalies. Then pivot to address/identity validation on any suspicious endpoints by pulling host interface config (ip addr/ifconfig) and comparing IP-to-host mappings. In this case, the sweep revealed mostly 0% loss across many pairs (suggesting the SDN dataplane is generally working) while also exposing anomalies involving a specific host. Verifying host_1 and host_4 interface configs confirmed both were configured with 10.0.0.1/24, immediately identifying an IP conflict as the likely root cause.", "score": 0.86, "time_created": "2026-01-17 15:13:29", "time_modified": "2026-01-17 15:13:29", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When SDN fabric connectivity looks mostly healthy but there are intermittent/\"unknown\" reachability results or inconsistent host-to-host behavior in a single L2 access subnet.", "experience": "Start with a broad reachability sweep (multi-host ping matrix) to quickly separate fabric-wide forwarding failures from endpoint-specific anomalies. Then pivot to address/identity validation on any suspicious endpoints by pulling host interface config (ip addr/ifconfig) and comparing IP-to-host mappings. In this case, the sweep revealed mostly 0% loss across many pairs (suggesting the SDN dataplane is generally working) while also exposing anomalies involving a specific host. Verifying host_1 and host_4 interface configs confirmed both were configured with 10.0.0.1/24, immediately identifying an IP conflict as the likely root cause.", "tags": ["sdn", "triage", "ping-matrix", "fault-localization", "ip-conflict", "endpoint-validation"], "confidence": 0.84, "step_type": "action", "tools_used": ["reachability_sweep", "host_ifconfig"]}}
{"workspace_id": "nika_v1", "memory_id": "f97b3d35586d4f4ea2dbfbfb9b155f50", "memory_type": "task", "when_to_use": "After producing a diagnosis report, you must map findings to a standardized root-cause taxonomy for submission/automation.", "content": "Translate narrative diagnosis into a structured classification by selecting the closest matching root-cause label from the available taxonomy, then submit with (a) `is_anomaly`, (b) `faulty_devices`, and (c) `root_cause_name`. Here, the key decision was choosing `host_ip_conflict` and listing both implicated hosts. This pattern works because it enforces consistency between evidence (duplicate IPs) and the final machine-consumable output, reducing ambiguity and improving downstream automation.", "score": 0.86, "time_created": "2026-01-17 15:16:20", "time_modified": "2026-01-17 15:16:20", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "After producing a diagnosis report, you must map findings to a standardized root-cause taxonomy for submission/automation.", "experience": "Translate narrative diagnosis into a structured classification by selecting the closest matching root-cause label from the available taxonomy, then submit with (a) `is_anomaly`, (b) `faulty_devices`, and (c) `root_cause_name`. Here, the key decision was choosing `host_ip_conflict` and listing both implicated hosts. This pattern works because it enforces consistency between evidence (duplicate IPs) and the final machine-consumable output, reducing ambiguity and improving downstream automation.", "tags": ["taxonomy-mapping", "structured-output", "root-cause", "submission", "classification"], "confidence": 0.82, "step_type": "action", "tools_used": ["list_root_causes", "submit_diagnosis"]}}
{"workspace_id": "nika_v1", "memory_id": "f41ff4c576c6474bbd6c89803f3bc858", "memory_type": "task", "when_to_use": "When host-to-host connectivity fails in a routed Clos/leaf-spine fabric and you need to quickly determine whether the fault is at the access edge (host/gateway) or in the routing fabric.", "content": "Use a bottom-up isolation sequence: (1) run a direct ping between affected hosts; (2) if it fails, immediately test each host's reachability to its configured default gateway; (3) pull each host's IP/route table to verify the gateway IP is in-subnet and matches the expected leaf SVI/interface; (4) validate the access link from the leaf side by pinging the host from the connected leaf. This pattern worked because it distinguished 'host can't reach gateway' (pc_0_0) from 'host can reach gateway but gateway has no route' (pc_0_1 receiving Net Unreachable from 10.0.1.1), allowing confident localization to a misconfigured host gateway without needing full fabric visibility.", "score": 0.86, "time_created": "2026-01-17 15:33:07", "time_modified": "2026-01-17 15:33:07", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When host-to-host connectivity fails in a routed Clos/leaf-spine fabric and you need to quickly determine whether the fault is at the access edge (host/gateway) or in the routing fabric.", "experience": "Use a bottom-up isolation sequence: (1) run a direct ping between affected hosts; (2) if it fails, immediately test each host's reachability to its configured default gateway; (3) pull each host's IP/route table to verify the gateway IP is in-subnet and matches the expected leaf SVI/interface; (4) validate the access link from the leaf side by pinging the host from the connected leaf. This pattern worked because it distinguished 'host can't reach gateway' (pc_0_0) from 'host can reach gateway but gateway has no route' (pc_0_1 receiving Net Unreachable from 10.0.1.1), allowing confident localization to a misconfigured host gateway without needing full fabric visibility.", "tags": ["clos", "leaf-spine", "bgp", "host-connectivity", "default-gateway", "edge-isolation", "icmp-unreachable"], "confidence": 0.86, "step_type": "decision", "tools_used": ["ping", "ifconfig", "ip addr", "ip route"]}}
{"workspace_id": "nika_v1", "memory_id": "a26a62f684ff4e7a8f50f11c7dff7fb5", "memory_type": "task", "when_to_use": "Client cannot reach any remote service (DNS/HTTP/ICMP) and errors look generic (timeouts or 'Destination Host Unreachable'), especially in routed fabrics where many layers could be blamed.", "content": "Start troubleshooting from the client edge with a minimal reachability ladder: (1) ping the target service IPs to confirm failure, (2) immediately inspect the client routing table and per-destination route selection (e.g., `ip route`, `ip route get <dst>`), and (3) validate L2 adjacency to the chosen next-hop using `ping` and neighbor/ARP state (`ip neigh`). In this case, `ip route get` showed the client forwarding 10.0.0.2/10.0.1.2 via 192.168.0.254; `ip neigh` showed that gateway as INCOMPLETE and ping to it failed, proving the break is before the Clos fabric. This pattern localizes faults quickly and prevents wasting time on BGP/fabric checks when the first hop is wrong/unreachable.", "score": 0.86, "time_created": "2026-01-17 15:36:26", "time_modified": "2026-01-17 15:36:26", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "Client cannot reach any remote service (DNS/HTTP/ICMP) and errors look generic (timeouts or 'Destination Host Unreachable'), especially in routed fabrics where many layers could be blamed.", "experience": "Start troubleshooting from the client edge with a minimal reachability ladder: (1) ping the target service IPs to confirm failure, (2) immediately inspect the client routing table and per-destination route selection (e.g., `ip route`, `ip route get <dst>`), and (3) validate L2 adjacency to the chosen next-hop using `ping` and neighbor/ARP state (`ip neigh`). In this case, `ip route get` showed the client forwarding 10.0.0.2/10.0.1.2 via 192.168.0.254; `ip neigh` showed that gateway as INCOMPLETE and ping to it failed, proving the break is before the Clos fabric. This pattern localizes faults quickly and prevents wasting time on BGP/fabric checks when the first hop is wrong/unreachable.", "tags": ["troubleshooting", "first-hop", "default-gateway", "ip-route-get", "arp", "fault-localization"], "confidence": 0.88, "step_type": "decision", "tools_used": ["ping", "show_host_state"]}}
{"workspace_id": "nika_v1", "memory_id": "2c3a4a6fd1fc4a60be2943adfb0b24bf", "memory_type": "task", "when_to_use": "When a multi-router network has partial reachability and one endpoint appears uniquely affected (e.g., one host times out while other hosts can reach the same destinations).", "content": "Start with a small reachability matrix (ping) that includes: (1) host-to-host inside the same domain, (2) host-to-known-good external service, and (3) external-to-external sanity checks. Compare results to isolate the fault domain: if multiple other nodes can reach external targets but one host cannot reach anything (including another internal host), treat it as an edge/first-hop problem rather than a routing-core problem. This pattern quickly narrows scope before spending time on routing daemons.", "score": 0.86, "time_created": "2026-01-17 15:52:37", "time_modified": "2026-01-17 15:52:37", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When a multi-router network has partial reachability and one endpoint appears uniquely affected (e.g., one host times out while other hosts can reach the same destinations).", "experience": "Start with a small reachability matrix (ping) that includes: (1) host-to-host inside the same domain, (2) host-to-known-good external service, and (3) external-to-external sanity checks. Compare results to isolate the fault domain: if multiple other nodes can reach external targets but one host cannot reach anything (including another internal host), treat it as an edge/first-hop problem rather than a routing-core problem. This pattern quickly narrows scope before spending time on routing daemons.", "tags": ["triage", "reachability-matrix", "fault-domain-isolation", "ping", "underlay-vs-edge"], "confidence": 0.84, "step_type": "decision", "tools_used": ["ping", "precomputed_connectivity_test"]}}
{"workspace_id": "nika_v1", "memory_id": "1985d6bd421d4f3e9b490355bc712fcb", "memory_type": "task", "when_to_use": "When host-to-host connectivity fails in a routed Clos/leaf-spine fabric and you need to quickly localize whether the break is at the edge (L2/first-hop) or in the routing/control-plane.", "content": "Start with bidirectional pings between representative hosts and interpret the *source* of ICMP unreachable messages to localize the fault domain. If the sender reports \"Destination Host Unreachable\", suspect local subnet/ARP/first-hop reachability. If the default gateway reports \"Destination Net Unreachable\", suspect missing routes/control-plane distribution. This pattern worked because it used error provenance (who generated the ICMP) as a fast decision point to split troubleshooting into edge vs fabric investigations before collecting deeper state.", "score": 0.78, "time_created": "2026-01-17 15:55:21", "time_modified": "2026-01-17 15:55:21", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When host-to-host connectivity fails in a routed Clos/leaf-spine fabric and you need to quickly localize whether the break is at the edge (L2/first-hop) or in the routing/control-plane.", "experience": "Start with bidirectional pings between representative hosts and interpret the *source* of ICMP unreachable messages to localize the fault domain. If the sender reports \"Destination Host Unreachable\", suspect local subnet/ARP/first-hop reachability. If the default gateway reports \"Destination Net Unreachable\", suspect missing routes/control-plane distribution. This pattern worked because it used error provenance (who generated the ICMP) as a fast decision point to split troubleshooting into edge vs fabric investigations before collecting deeper state.", "tags": ["icmp", "unreachable", "fault-localization", "edge-vs-fabric", "clos", "leaf-spine"], "confidence": 0.86, "step_type": "decision", "tools_used": ["ping"]}}
{"workspace_id": "nika_v1", "memory_id": "2e1b662af2704a17b286d6db662a80ad", "memory_type": "task", "when_to_use": "When end-to-end connectivity from a client to multiple destinations fails and ICMP errors appear (especially \"Destination Host Unreachable\").", "content": "Start with symptom triage from the client: ping a representative set of targets (DNS IP, local pod web IP, remote pod web IP). If failures are consistent and the ICMP \"Host Unreachable\" is sourced by the client itself, pivot immediately to validating the client’s L3 edge assumptions (IP/mask/default gateway) using ip_addr/ip_route. This pattern works because a client-sourced unreachable usually indicates it cannot resolve/ARP for its next hop or believes the destination is on-link; confirming default route + on-link subnet quickly distinguishes host-edge misconfig from fabric routing issues.", "score": 0.86, "time_created": "2026-01-17 15:58:15", "time_modified": "2026-01-17 15:58:15", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When end-to-end connectivity from a client to multiple destinations fails and ICMP errors appear (especially \"Destination Host Unreachable\").", "experience": "Start with symptom triage from the client: ping a representative set of targets (DNS IP, local pod web IP, remote pod web IP). If failures are consistent and the ICMP \"Host Unreachable\" is sourced by the client itself, pivot immediately to validating the client’s L3 edge assumptions (IP/mask/default gateway) using ip_addr/ip_route. This pattern works because a client-sourced unreachable usually indicates it cannot resolve/ARP for its next hop or believes the destination is on-link; confirming default route + on-link subnet quickly distinguishes host-edge misconfig from fabric routing issues.", "tags": ["connectivity", "icmp", "host-unreachable", "default-gateway", "edge-triage"], "confidence": 0.86, "step_type": "decision", "tools_used": ["ping", "ip_addr", "ip_route"]}}
{"workspace_id": "nika_v1", "memory_id": "9f9945202497477c9f1918dcb315aa18", "memory_type": "task", "when_to_use": "When diagnosing multi-tier enterprise networks where end-to-end reachability is failing and you must quickly separate local-segment issues from routing/control-plane failures.", "content": "Start with a broad, automated connectivity/state snapshot (inventory IPs + a few representative pings between key roles: host↔gateway, host↔server-farm, server-farm↔core). Use the results to cluster symptoms by domain (access vs routing vs server-farm). In this run, early data showed server-farm hosts could reach each other, while hosts could not reliably reach gateways and server-farm could not reach core. This immediately narrowed the problem to (a) access/DHCP for hosts and (b) missing inter-subnet routing, avoiding wasted deep dives into DNS/HTTP.", "score": 0.84, "time_created": "2026-01-17 16:02:24", "time_modified": "2026-01-17 16:02:24", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When diagnosing multi-tier enterprise networks where end-to-end reachability is failing and you must quickly separate local-segment issues from routing/control-plane failures.", "experience": "Start with a broad, automated connectivity/state snapshot (inventory IPs + a few representative pings between key roles: host↔gateway, host↔server-farm, server-farm↔core). Use the results to cluster symptoms by domain (access vs routing vs server-farm). In this run, early data showed server-farm hosts could reach each other, while hosts could not reliably reach gateways and server-farm could not reach core. This immediately narrowed the problem to (a) access/DHCP for hosts and (b) missing inter-subnet routing, avoiding wasted deep dives into DNS/HTTP.", "tags": ["triage", "domain-isolation", "connectivity-matrix", "end-to-end", "hierarchical-network"], "confidence": 0.78, "step_type": "decision", "tools_used": ["connectivity_check"]}}
{"workspace_id": "nika_v1", "memory_id": "8a082fa816d0453facbf8e75fdc1ef82", "memory_type": "task", "when_to_use": "When a subset of endpoints cannot reach services and you need to quickly determine whether the issue is local (LAN/adjacency) vs. routing-domain wide (RIP/IGP).", "content": "Start with a broad reachability snapshot (multi-source ping matrix) to identify which nodes are healthy and which are isolated. Then immediately validate the suspected isolated host's L3 edge by checking its default route and attempting to ping the default gateway. If the gateway ping fails, inspect neighbor/ARP state to distinguish 'no L2 adjacency' from 'routing beyond gateway'. This pattern works because it prevents premature deep-dives into RIP and focuses first on the dependency chain closest to the failing node (host -> gateway -> routing core).", "score": 0.86, "time_created": "2026-01-17 16:13:31", "time_modified": "2026-01-17 16:13:31", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When a subset of endpoints cannot reach services and you need to quickly determine whether the issue is local (LAN/adjacency) vs. routing-domain wide (RIP/IGP).", "experience": "Start with a broad reachability snapshot (multi-source ping matrix) to identify which nodes are healthy and which are isolated. Then immediately validate the suspected isolated host's L3 edge by checking its default route and attempting to ping the default gateway. If the gateway ping fails, inspect neighbor/ARP state to distinguish 'no L2 adjacency' from 'routing beyond gateway'. This pattern works because it prevents premature deep-dives into RIP and focuses first on the dependency chain closest to the failing node (host -> gateway -> routing core).", "tags": ["triage", "reachability-matrix", "localization", "default-gateway", "arp", "rip"], "confidence": 0.86, "step_type": "decision", "tools_used": ["reachability_matrix", "show_ip_conf", "ping"]}}
{"workspace_id": "nika_v1", "memory_id": "d5c1dbd70b1c4f1ab7b5169a63e24c19", "memory_type": "task", "when_to_use": "When you must produce a coherent diagnosis report and a structured 'submission' classification from observed evidence.", "content": "Write the report in a fixed structure: (1) overall health (partial vs systemic), (2) symptoms with concrete command outputs, (3) fault localization, (4) config/state evidence from both sides of the failing link, (5) most likely root-cause hypothesis, (6) impact summary. Then map the hypothesis to the closest matching fault taxonomy labels (e.g., host_incorrect_gateway/host_incorrect_ip) and submit only if a report exists. This works because it ties each conclusion to specific observations, avoids proposing fixes, and produces an unambiguous classification aligned with the evidence.", "score": 0.86, "time_created": "2026-01-17 16:13:31", "time_modified": "2026-01-17 16:13:31", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When you must produce a coherent diagnosis report and a structured 'submission' classification from observed evidence.", "experience": "Write the report in a fixed structure: (1) overall health (partial vs systemic), (2) symptoms with concrete command outputs, (3) fault localization, (4) config/state evidence from both sides of the failing link, (5) most likely root-cause hypothesis, (6) impact summary. Then map the hypothesis to the closest matching fault taxonomy labels (e.g., host_incorrect_gateway/host_incorrect_ip) and submit only if a report exists. This works because it ties each conclusion to specific observations, avoids proposing fixes, and produces an unambiguous classification aligned with the evidence.", "tags": ["reporting", "evidence-based", "fault-taxonomy", "structured-diagnosis", "submission"], "confidence": 0.82, "step_type": "action", "tools_used": ["submit_fault"]}}
{"workspace_id": "nika_v1", "memory_id": "527145fc6930463a8bcf0a3111b829e3", "memory_type": "task", "when_to_use": "SDN/OpenFlow topology where some endpoints have reachability issues and you must determine whether the fault is control-plane, fabric-wide data-plane, or localized to an edge/host.", "content": "Use a layered validation sequence: (1) verify controller session health on every switch via OVS (controller target + is_connected + fail_mode), (2) verify management-plane IP addressing and reachability to the controller from each switch (ping 20.0.0.100), then (3) test data-plane reachability between multiple non-suspect hosts to establish a working baseline. This works because it quickly rules out systemic SDN/controller failures (which would be amplified by fail_mode=secure) and proves the fabric can forward for at least part of the network, allowing you to confidently narrow scope to a subset of hosts/edges.", "score": 0.86, "time_created": "2026-01-17 16:16:09", "time_modified": "2026-01-17 16:16:09", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "SDN/OpenFlow topology where some endpoints have reachability issues and you must determine whether the fault is control-plane, fabric-wide data-plane, or localized to an edge/host.", "experience": "Use a layered validation sequence: (1) verify controller session health on every switch via OVS (controller target + is_connected + fail_mode), (2) verify management-plane IP addressing and reachability to the controller from each switch (ping 20.0.0.100), then (3) test data-plane reachability between multiple non-suspect hosts to establish a working baseline. This works because it quickly rules out systemic SDN/controller failures (which would be amplified by fail_mode=secure) and proves the fabric can forward for at least part of the network, allowing you to confidently narrow scope to a subset of hosts/edges.", "tags": ["sdn", "openflow", "ovs", "control-plane", "management-plane", "baseline-testing", "fault-isolation"], "confidence": 0.86, "step_type": "action", "tools_used": ["ovs-vsctl show", "ip addr", "ip route", "ping"]}}
{"workspace_id": "nika_v1", "memory_id": "6dec3ab3c68745ae99b19d5d10db95d6", "memory_type": "task", "when_to_use": "When troubleshooting SDN fabrics where symptoms could be either control-plane (controller/OpenFlow) or data-plane (host/leaf/uplink) related, and you need fast fault-domain isolation.", "content": "Use a two-lane triage sequence: (1) verify control-plane health on every switch via `ovs-vsctl show` focusing on `Controller ... is_connected:true` and `fail_mode`, and (2) verify management-plane reachability to the controller with simple pings from each switch. This quickly rules out controller/session outages and prevents misattributing host reachability failures to SDN control issues. In the trace, all switches were connected and could ping 20.0.0.100, so the investigation correctly pivoted to endpoint/data-plane causes.", "score": 0.84, "time_created": "2026-01-17 16:19:04", "time_modified": "2026-01-17 16:19:04", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When troubleshooting SDN fabrics where symptoms could be either control-plane (controller/OpenFlow) or data-plane (host/leaf/uplink) related, and you need fast fault-domain isolation.", "experience": "Use a two-lane triage sequence: (1) verify control-plane health on every switch via `ovs-vsctl show` focusing on `Controller ... is_connected:true` and `fail_mode`, and (2) verify management-plane reachability to the controller with simple pings from each switch. This quickly rules out controller/session outages and prevents misattributing host reachability failures to SDN control issues. In the trace, all switches were connected and could ping 20.0.0.100, so the investigation correctly pivoted to endpoint/data-plane causes.", "tags": ["sdn", "openflow", "ovs", "control-plane", "management-plane", "fault-domain-isolation"], "confidence": 0.84, "step_type": "decision", "tools_used": ["ovs-vsctl", "ping"]}}
{"workspace_id": "nika_v1", "memory_id": "258cea5efbfa46d98266a149b8292475", "memory_type": "task", "when_to_use": "When end-to-end host connectivity in a routed Clos/leaf-spine fabric is failing and you need to quickly localize whether the break is at the host, first-hop gateway, or fabric routing/control-plane.", "content": "Start with symmetric host-to-host pings and interpret the failure mode to localize the fault domain. A fast failure with ICMP 'Destination Net Unreachable' sourced from the default gateway implies the first-hop router lacks a route to the destination prefix (routing/control-plane issue). A timeout suggests blackholing/return-path issues and warrants checking both directions. This pattern worked because it converted a generic 'ping fails' symptom into a concrete routing inference (missing route on leaf_router_0_1 toward 10.0.0.0/24), guiding subsequent checks toward routing tables and BGP/FRR rather than L2/link debugging.", "score": 0.86, "time_created": "2026-01-17 16:24:34", "time_modified": "2026-01-17 16:24:34", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When end-to-end host connectivity in a routed Clos/leaf-spine fabric is failing and you need to quickly localize whether the break is at the host, first-hop gateway, or fabric routing/control-plane.", "experience": "Start with symmetric host-to-host pings and interpret the failure mode to localize the fault domain. A fast failure with ICMP 'Destination Net Unreachable' sourced from the default gateway implies the first-hop router lacks a route to the destination prefix (routing/control-plane issue). A timeout suggests blackholing/return-path issues and warrants checking both directions. This pattern worked because it converted a generic 'ping fails' symptom into a concrete routing inference (missing route on leaf_router_0_1 toward 10.0.0.0/24), guiding subsequent checks toward routing tables and BGP/FRR rather than L2/link debugging.", "tags": ["troubleshooting", "leaf-spine", "clos", "icmp", "fault-localization", "ping"], "confidence": 0.82, "step_type": "decision", "tools_used": ["ping"]}}
{"workspace_id": "nika_v1", "memory_id": "f11ea883119c4d4f92d5636d18916130", "memory_type": "task", "when_to_use": "When end-to-end reachability to a remote subnet/service fails in a multi-tier routed network and you need to quickly localize whether the fault is host-local, gateway/local routing, or upstream routing/control-plane.", "content": "Use a layered ping strategy that interprets ICMP source/error text to localize the fault domain: (1) ping the local default gateway from the host to confirm local L2/L3; (2) ping the remote service IP from multiple vantage points (at least one host per site) and record whether the ICMP error is self-generated (host unreachable) vs generated by the gateway (net unreachable); (3) validate remote-subnet local health by pinging between two nodes within the remote subnet. This worked because 'Destination Host Unreachable' sourced from the host pointed to a host-side L2/on-link assumption (later confirmed by /8 netmask), while 'Destination Net Unreachable' sourced from the gateway indicated missing routes on the distribution layer, and successful intra-server-farm ping proved the server LAN itself was healthy.", "score": 0.86, "time_created": "2026-01-17 16:38:04", "time_modified": "2026-01-17 16:38:04", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When end-to-end reachability to a remote subnet/service fails in a multi-tier routed network and you need to quickly localize whether the fault is host-local, gateway/local routing, or upstream routing/control-plane.", "experience": "Use a layered ping strategy that interprets ICMP source/error text to localize the fault domain: (1) ping the local default gateway from the host to confirm local L2/L3; (2) ping the remote service IP from multiple vantage points (at least one host per site) and record whether the ICMP error is self-generated (host unreachable) vs generated by the gateway (net unreachable); (3) validate remote-subnet local health by pinging between two nodes within the remote subnet. This worked because 'Destination Host Unreachable' sourced from the host pointed to a host-side L2/on-link assumption (later confirmed by /8 netmask), while 'Destination Net Unreachable' sourced from the gateway indicated missing routes on the distribution layer, and successful intra-server-farm ping proved the server LAN itself was healthy.", "tags": ["triage", "icmp", "fault-domain-isolation", "ping", "layered-testing", "routing-vs-host"], "confidence": 0.86, "step_type": "decision", "tools_used": ["ping", "batched_connectivity_test"]}}
{"workspace_id": "nika_v1", "memory_id": "4a5fc3c473fe4662a2abf44cce4db67f", "memory_type": "task", "when_to_use": "When dynamic routing appears inactive but standard routing CLI tools (e.g., vtysh) may be missing/unavailable in the execution environment.", "content": "Fall back to process/config presence checks to validate daemon inactivity: attempt `vtysh` (and treat 'command not found' as an environment constraint), then check for running FRR processes (zebra/ospfd) and confirm intended daemon enablement via `/etc/frr/daemons` and presence of `/etc/frr/frr.conf`. This worked because it separated 'OSPF misconfigured' from 'OSPF not running at all' and produced defensible evidence (no processes + only connected routes) even without vtysh output.", "score": 0.86, "time_created": "2026-01-17 16:38:04", "time_modified": "2026-01-17 16:38:04", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When dynamic routing appears inactive but standard routing CLI tools (e.g., vtysh) may be missing/unavailable in the execution environment.", "experience": "Fall back to process/config presence checks to validate daemon inactivity: attempt `vtysh` (and treat 'command not found' as an environment constraint), then check for running FRR processes (zebra/ospfd) and confirm intended daemon enablement via `/etc/frr/daemons` and presence of `/etc/frr/frr.conf`. This worked because it separated 'OSPF misconfigured' from 'OSPF not running at all' and produced defensible evidence (no processes + only connected routes) even without vtysh output.", "tags": ["frr", "ospfd", "zebra", "process-check", "environment-constraints", "diagnostic-evidence"], "confidence": 0.78, "step_type": "action", "tools_used": ["ps", "cat", "ls", "vtysh"]}}
{"workspace_id": "nika_v1", "memory_id": "7592c7e0f1f549eab5676030bb4ca887", "memory_type": "task", "when_to_use": "When a single endpoint reports reachability issues but the network is multi-hop (dynamic routing, multiple routers/segments) and you need to quickly determine whether the fault is host-local vs routing-core.", "content": "Start with a broad reachability snapshot (matrix/health-check) to see if failures are isolated or systemic. Then run targeted pings from the suspected endpoint to (1) a same-domain internal host and (2) an external/other-zone host. If internal fails but external succeeds, bias investigation toward host-local L3 config (mask/gateway/ARP behavior) rather than routing protocol failure. This pattern worked here: host_1 could ping external 20.0.0.x but not internal 10.0.1.2, indicating the core RIP domain was likely healthy and the issue was specific to host_1.", "score": 0.72, "time_created": "2026-01-17 16:40:54", "time_modified": "2026-01-17 16:40:54", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When a single endpoint reports reachability issues but the network is multi-hop (dynamic routing, multiple routers/segments) and you need to quickly determine whether the fault is host-local vs routing-core.", "experience": "Start with a broad reachability snapshot (matrix/health-check) to see if failures are isolated or systemic. Then run targeted pings from the suspected endpoint to (1) a same-domain internal host and (2) an external/other-zone host. If internal fails but external succeeds, bias investigation toward host-local L3 config (mask/gateway/ARP behavior) rather than routing protocol failure. This pattern worked here: host_1 could ping external 20.0.0.x but not internal 10.0.1.2, indicating the core RIP domain was likely healthy and the issue was specific to host_1.", "tags": ["triage", "reachability", "fault-isolation", "host-vs-network", "icmp", "baseline"], "confidence": 0.82, "step_type": "decision", "tools_used": ["reachability_matrix", "ping"]}}
{"workspace_id": "nika_v1", "memory_id": "d926f64ac21c443bb94d9c3023e68ae7", "memory_type": "task", "when_to_use": "When end-to-end pings fail immediately with local errors (e.g., \"Network is unreachable\") or when the test harness cannot resolve a host IP.", "content": "Start troubleshooting at the edge by validating host L3 basics before inspecting the fabric: run host interface and routing checks (ip addr/ifconfig, ip route) and correlate with the exact ping error. In this case, confirming pc_0_0 had eth0 UP but no IPv4 address and an empty routing table fully explained the failure and prevented wasted effort debugging BGP/underlay.", "score": 0.9, "time_created": "2026-01-17 16:43:27", "time_modified": "2026-01-17 16:43:27", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When end-to-end pings fail immediately with local errors (e.g., \"Network is unreachable\") or when the test harness cannot resolve a host IP.", "experience": "Start troubleshooting at the edge by validating host L3 basics before inspecting the fabric: run host interface and routing checks (ip addr/ifconfig, ip route) and correlate with the exact ping error. In this case, confirming pc_0_0 had eth0 UP but no IPv4 address and an empty routing table fully explained the failure and prevented wasted effort debugging BGP/underlay.", "tags": ["edge-first", "host-triage", "network-unreachable", "ip-addr", "ip-route"], "confidence": 0.86, "step_type": "decision", "tools_used": ["ping", "host_ifconfig", "host_ip_addr", "host_ip_route"]}}
{"workspace_id": "nika_v1", "memory_id": "c350a24177cb4bf2b4e70da67d3e1eb9", "memory_type": "task", "when_to_use": "When an endpoint (client/host) cannot reach any in-fabric IPs and failures return immediately (e.g., \"Network is unreachable\") rather than timing out.", "content": "Start troubleshooting at the failing endpoint and validate basic L3 prerequisites before investigating the fabric: check `ip addr` for an IPv4 address on the active interface and `ip route` for a default route. Immediate \"Network is unreachable\" strongly indicates missing local addressing/routing (kernel has no route), which can fully explain downstream DNS/HTTP failures without needing to inspect BGP or ACLs. This quickly localizes the fault domain and prevents unnecessary deep-dives into the Clos control plane.", "score": 0.9, "time_created": "2026-01-17 16:46:03", "time_modified": "2026-01-17 16:46:03", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When an endpoint (client/host) cannot reach any in-fabric IPs and failures return immediately (e.g., \"Network is unreachable\") rather than timing out.", "experience": "Start troubleshooting at the failing endpoint and validate basic L3 prerequisites before investigating the fabric: check `ip addr` for an IPv4 address on the active interface and `ip route` for a default route. Immediate \"Network is unreachable\" strongly indicates missing local addressing/routing (kernel has no route), which can fully explain downstream DNS/HTTP failures without needing to inspect BGP or ACLs. This quickly localizes the fault domain and prevents unnecessary deep-dives into the Clos control plane.", "tags": ["endpoint-first", "l3-basics", "network-unreachable", "ip-addr", "ip-route", "fault-localization"], "confidence": 0.9, "step_type": "decision", "tools_used": ["host_ifconfig_iproute"]}}
{"workspace_id": "nika_v1", "memory_id": "3fe257444d204e89beef5a4e485c9df4", "memory_type": "task", "when_to_use": "When end users report total loss of connectivity and you suspect basic L3 readiness issues (DHCP/gateway) versus upstream routing problems.", "content": "Start troubleshooting at the edge by validating host L3 state before investigating the routing core. Pull `ip addr` and `ip route` from representative hosts; if IPv4 and default route are missing, immediately test a simple ping to a known on-subnet target and note errors like `Network is unreachable` (indicates no route, not just packet loss). Corroborate DHCP failure by checking the DHCP client lease file size/contents. This sequence quickly distinguishes 'host not configured' from 'configured but cannot reach', preventing wasted time on deeper routing checks.", "score": 0.86, "time_created": "2026-01-17 16:50:15", "time_modified": "2026-01-17 16:50:15", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When end users report total loss of connectivity and you suspect basic L3 readiness issues (DHCP/gateway) versus upstream routing problems.", "experience": "Start troubleshooting at the edge by validating host L3 state before investigating the routing core. Pull `ip addr` and `ip route` from representative hosts; if IPv4 and default route are missing, immediately test a simple ping to a known on-subnet target and note errors like `Network is unreachable` (indicates no route, not just packet loss). Corroborate DHCP failure by checking the DHCP client lease file size/contents. This sequence quickly distinguishes 'host not configured' from 'configured but cannot reach', preventing wasted time on deeper routing checks.", "tags": ["dhcp", "host_missing_ip", "l3_readiness", "edge_first", "triage"], "confidence": 0.86, "step_type": "decision", "tools_used": ["get_ifconfig", "get_ip_addr", "get_ip_route", "exec_cmd", "ping"]}}
{"workspace_id": "nika_v1", "memory_id": "4ed197ca73394cb0b0636ec22aaa125f", "memory_type": "task", "when_to_use": "When end-to-end connectivity is reported broken in a hierarchical routed network and you need to quickly localize whether the issue is in the server segment, access edge, or routing domain.", "content": "Start by establishing a known-good baseline inside a single subnet (e.g., server farm). Ping between servers on the same VLAN/subnet (dns->webs) to validate L2 and local L3. Then test from user hosts toward the same server IPs. Compare outcomes: if intra-subnet pings succeed but user-to-server fails, the fault is upstream routing/edge, not the server farm. This sequencing worked because it isolates failure domains early and prevents chasing application/DNS issues when basic reachability is absent.", "score": 0.86, "time_created": "2026-01-17 16:53:18", "time_modified": "2026-01-17 16:53:18", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When end-to-end connectivity is reported broken in a hierarchical routed network and you need to quickly localize whether the issue is in the server segment, access edge, or routing domain.", "experience": "Start by establishing a known-good baseline inside a single subnet (e.g., server farm). Ping between servers on the same VLAN/subnet (dns->webs) to validate L2 and local L3. Then test from user hosts toward the same server IPs. Compare outcomes: if intra-subnet pings succeed but user-to-server fails, the fault is upstream routing/edge, not the server farm. This sequencing worked because it isolates failure domains early and prevents chasing application/DNS issues when basic reachability is absent.", "tags": ["fault-domain-isolation", "baseline-testing", "icmp", "server-farm", "hierarchical-network"], "confidence": 0.83, "step_type": "action", "tools_used": ["ping"]}}
{"workspace_id": "nika_v1", "memory_id": "afaa1854074e472e9951957758548d72", "memory_type": "task", "when_to_use": "When a multi-router network mostly works but one endpoint shows failed/unknown reachability, and you need to quickly localize whether the issue is host-local vs routing-fabric-wide.", "content": "Start with a broad reachability matrix (multiple src/dst pairs) to identify whether failures are systemic or isolated. If most hosts/servers can reach each other with low loss/RTT but one host’s tests are 'unknown' or failing, treat that host as the primary suspect. Then run a single direct connectivity check from the suspect host to a nearby/internal target (e.g., another host) to confirm whether it can originate any traffic at all. This pattern works because it separates 'network-wide routing/links down' from 'single-node misconfig' using minimal tests and clear comparative evidence.", "score": 0.84, "time_created": "2026-01-17 16:55:32", "time_modified": "2026-01-17 16:55:32", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When a multi-router network mostly works but one endpoint shows failed/unknown reachability, and you need to quickly localize whether the issue is host-local vs routing-fabric-wide.", "experience": "Start with a broad reachability matrix (multiple src/dst pairs) to identify whether failures are systemic or isolated. If most hosts/servers can reach each other with low loss/RTT but one host’s tests are 'unknown' or failing, treat that host as the primary suspect. Then run a single direct connectivity check from the suspect host to a nearby/internal target (e.g., another host) to confirm whether it can originate any traffic at all. This pattern works because it separates 'network-wide routing/links down' from 'single-node misconfig' using minimal tests and clear comparative evidence.", "tags": ["fault-localization", "reachability-matrix", "isolation", "icmp", "triage"], "confidence": 0.82, "step_type": "decision", "tools_used": ["reachability_matrix", "ping"]}}
{"workspace_id": "nika_v1", "memory_id": "855949c0ed9c4c709e27af1d11c2457a", "memory_type": "task", "when_to_use": "SDN/OpenFlow spine–leaf fabric where hosts show widespread ping failures/timeouts and you must quickly separate control-plane health from data-plane forwarding health.", "content": "Use a layered validation sequence: (1) collect broad reachability signals (multi-host ping matrix if available) to confirm scope; (2) verify switch-to-controller connectivity via OVS controller status (e.g., `ovs-vsctl show`) and management-plane ping to the controller IP from each switch; (3) if control-plane is up but data-plane is down, immediately inspect datapath programming by dumping OpenFlow flows/ports (`ovs-ofctl dump-flows`, `dump-ports`). This pattern works because it prevents misattributing fabric-wide forwarding failures to host issues: it proves the controller is reachable while simultaneously showing whether the switches actually have forwarding state (critical when `fail_mode: secure` implies 'no flows = no forwarding').", "score": 0.86, "time_created": "2026-01-17 17:01:15", "time_modified": "2026-01-17 17:01:15", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "SDN/OpenFlow spine–leaf fabric where hosts show widespread ping failures/timeouts and you must quickly separate control-plane health from data-plane forwarding health.", "experience": "Use a layered validation sequence: (1) collect broad reachability signals (multi-host ping matrix if available) to confirm scope; (2) verify switch-to-controller connectivity via OVS controller status (e.g., `ovs-vsctl show`) and management-plane ping to the controller IP from each switch; (3) if control-plane is up but data-plane is down, immediately inspect datapath programming by dumping OpenFlow flows/ports (`ovs-ofctl dump-flows`, `dump-ports`). This pattern works because it prevents misattributing fabric-wide forwarding failures to host issues: it proves the controller is reachable while simultaneously showing whether the switches actually have forwarding state (critical when `fail_mode: secure` implies 'no flows = no forwarding').", "tags": ["sdn", "openflow", "ovs", "spine-leaf", "control-plane-vs-data-plane", "dump-flows", "fail-mode-secure"], "confidence": 0.84, "step_type": "decision", "tools_used": ["ping", "ovs-vsctl", "ovs-ofctl"]}}
{"workspace_id": "nika_v1", "memory_id": "217729669dcd4dce9c98c2d63fd09941", "memory_type": "task", "when_to_use": "When end-host reachability is broadly failing/unknown and you need to quickly decide whether the issue is at the access edge (DHCP/IP config) versus core routing/services.", "content": "Start by validating L3 presence on representative hosts before investigating the core: collect `ip addr`/`ifconfig` and `ip route` from 1-2 user hosts. If hosts show no IPv4 on the access interface and an empty route table, treat this as a DHCP/provisioning failure and deprioritize OSPF/core debugging. This works because missing host IP/default-gateway prevents any meaningful end-to-end testing and is a high-signal indicator that the fault is upstream of routing/services.", "score": 0.86, "time_created": "2026-01-17 17:05:00", "time_modified": "2026-01-17 17:05:00", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When end-host reachability is broadly failing/unknown and you need to quickly decide whether the issue is at the access edge (DHCP/IP config) versus core routing/services.", "experience": "Start by validating L3 presence on representative hosts before investigating the core: collect `ip addr`/`ifconfig` and `ip route` from 1-2 user hosts. If hosts show no IPv4 on the access interface and an empty route table, treat this as a DHCP/provisioning failure and deprioritize OSPF/core debugging. This works because missing host IP/default-gateway prevents any meaningful end-to-end testing and is a high-signal indicator that the fault is upstream of routing/services.", "tags": ["dhcp", "host-missing-ip", "first-principles", "edge-triage", "ip-route"], "confidence": 0.86, "step_type": "decision", "tools_used": ["ifconfig", "ip_addr", "ip_route"]}}
{"workspace_id": "nika_v1", "memory_id": "27cc105f1a334e3dbfc2ab8581dee388", "memory_type": "task", "when_to_use": "When an external client cannot reach in-fabric services (DNS/HTTP) in a Clos/BGP data center and initial connectivity tests time out or hang.", "content": "Start from the client and validate progressively: (1) run quick reachability checks to key service IPs (DNS, web) and a simple application probe (curl) to confirm the symptom; (2) immediately verify the client’s local L3 baseline (ip -br a, ip r) and confirm the default gateway is reachable (ping gw) to localize whether the failure is at the access edge or deeper in the fabric. This sequence worked because it separated 'client misconfig/access link down' from 'fabric routing/service failure' early, preventing wasted time debugging BGP before confirming the client edge was healthy.", "score": 0.78, "time_created": "2026-01-17 17:08:24", "time_modified": "2026-01-17 17:08:24", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When an external client cannot reach in-fabric services (DNS/HTTP) in a Clos/BGP data center and initial connectivity tests time out or hang.", "experience": "Start from the client and validate progressively: (1) run quick reachability checks to key service IPs (DNS, web) and a simple application probe (curl) to confirm the symptom; (2) immediately verify the client’s local L3 baseline (ip -br a, ip r) and confirm the default gateway is reachable (ping gw) to localize whether the failure is at the access edge or deeper in the fabric. This sequence worked because it separated 'client misconfig/access link down' from 'fabric routing/service failure' early, preventing wasted time debugging BGP before confirming the client edge was healthy.", "tags": ["client-first", "progressive-localization", "ping", "curl", "default-gateway-check", "clos", "l3-baseline"], "confidence": 0.84, "step_type": "decision", "tools_used": ["ping", "ip", "curl"]}}
{"workspace_id": "nika_v1", "memory_id": "1d332b5ba0e2428fbfe400d0a8611436", "memory_type": "task", "when_to_use": "When end-to-end reachability tests return null/unknown or fail broadly across many src/dst pairs in a routed fabric (e.g., Clos/EBGP), suggesting a systemic issue rather than a single service failure.", "content": "Start by validating the edge/endpoint attachment before diving into routing or services. Pull the client host's interface list, IP addressing, and routing table (e.g., ifconfig/ip addr/ip route). If the client lacks the expected NIC (per topology) or has no IP/default route, treat this as a hard blocker: downstream DNS/HTTP checks are not meaningful. This works because widespread 'everything fails' symptoms are often caused by a single missing L2/L3 prerequisite at the traffic source.", "score": 0.86, "time_created": "2026-01-17 17:31:57", "time_modified": "2026-01-17 17:31:57", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When end-to-end reachability tests return null/unknown or fail broadly across many src/dst pairs in a routed fabric (e.g., Clos/EBGP), suggesting a systemic issue rather than a single service failure.", "experience": "Start by validating the edge/endpoint attachment before diving into routing or services. Pull the client host's interface list, IP addressing, and routing table (e.g., ifconfig/ip addr/ip route). If the client lacks the expected NIC (per topology) or has no IP/default route, treat this as a hard blocker: downstream DNS/HTTP checks are not meaningful. This works because widespread 'everything fails' symptoms are often caused by a single missing L2/L3 prerequisite at the traffic source.", "tags": ["triage", "endpoint-first", "clos", "systemic-outage", "ip-addressing", "default-route", "link-detach"], "confidence": 0.86, "step_type": "decision", "tools_used": ["lab_reachability_matrix", "host_ifconfig_iproute"]}}
{"workspace_id": "nika_v1", "memory_id": "39da31ee3a3544d69e18b375fbedcc7b", "memory_type": "task", "when_to_use": "When many reachability tests are \"unknown\" or end-host-to-anything tests fail early, especially in networks that rely on DHCP for host addressing.", "content": "Start by validating host L3 readiness before deeper routing analysis: pull `ip addr`/`ifconfig` and `ip route` from representative hosts. If hosts lack an IPv4 address and default route, treat upstream ping/HTTP/DNS failures as non-actionable until DHCP/L2 attachment is confirmed. Also check whether the expected NIC (e.g., eth0) exists and is UP; a missing interface strongly indicates link detach/host-side attachment issues rather than pure DHCP misbehavior. This quickly separates 'host not configured' from 'network routing broken' and prevents wasted effort on core routing when hosts cannot originate traffic.", "score": 0.86, "time_created": "2026-01-17 17:35:46", "time_modified": "2026-01-17 17:35:46", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When many reachability tests are \"unknown\" or end-host-to-anything tests fail early, especially in networks that rely on DHCP for host addressing.", "experience": "Start by validating host L3 readiness before deeper routing analysis: pull `ip addr`/`ifconfig` and `ip route` from representative hosts. If hosts lack an IPv4 address and default route, treat upstream ping/HTTP/DNS failures as non-actionable until DHCP/L2 attachment is confirmed. Also check whether the expected NIC (e.g., eth0) exists and is UP; a missing interface strongly indicates link detach/host-side attachment issues rather than pure DHCP misbehavior. This quickly separates 'host not configured' from 'network routing broken' and prevents wasted effort on core routing when hosts cannot originate traffic.", "tags": ["dhcp", "host-missing-ip", "l2-attachment", "ip-route", "triage"], "confidence": 0.86, "step_type": "decision", "tools_used": ["get_host_config", "get_reachability"]}}
{"workspace_id": "nika_v1", "memory_id": "11b0fedbe7c041d1b85d51f8b1ceb826", "memory_type": "task", "when_to_use": "When a VPN overlay is involved and there is ambiguity whether the failure is due to the tunnel itself or the underlay required to establish/maintain it.", "content": "Validate the VPN’s operational state separately from the interface presence: check whether the tunnel interface exists/has an IP and whether the management service is running (e.g., wg-quick@wg0). Then reason about dependency: even if wg0 is UP, lack of underlay interface/default route means the peer endpoint is unreachable and the VPN cannot function. This prevents misattributing the outage to WireGuard config when the real issue is underlay detachment/missing NIC.", "score": 0.87, "time_created": "2026-01-17 17:44:50", "time_modified": "2026-01-17 17:44:50", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When a VPN overlay is involved and there is ambiguity whether the failure is due to the tunnel itself or the underlay required to establish/maintain it.", "experience": "Validate the VPN’s operational state separately from the interface presence: check whether the tunnel interface exists/has an IP and whether the management service is running (e.g., wg-quick@wg0). Then reason about dependency: even if wg0 is UP, lack of underlay interface/default route means the peer endpoint is unreachable and the VPN cannot function. This prevents misattributing the outage to WireGuard config when the real issue is underlay detachment/missing NIC.", "tags": ["vpn-dependency", "wireguard", "overlay-underlay", "service-state", "root-cause"], "confidence": 0.78, "step_type": "reasoning", "tools_used": ["get_host_network_state", "check_service_status"]}}
{"workspace_id": "nika_v1", "memory_id": "544c4d10afb047d9aa6fe14b2ec14653", "memory_type": "task", "when_to_use": "SDN fabric troubleshooting when a subset of hosts show reachability issues and you need to quickly determine whether the fault is endpoint-local, access-link, or fabric/control-plane wide.", "content": "Use a layered triage sequence: (1) start with a broad connectivity snapshot (e.g., automated ping matrix) to spot which pairs fail vs succeed; (2) immediately validate endpoint liveness on the suspected host(s) with `ip -br a` and routing table presence; (3) corroborate symptom directionality by pinging the suspected host IP from a known-good peer and interpreting errors (e.g., 'Destination Host Unreachable' suggests missing ARP/neighbor due to host/interface absence); (4) only then check SDN control-plane health on all switches via `ovs-vsctl show`/OpenFlow feature output to confirm controller connectivity and avoid misattributing an endpoint issue to the fabric. This works because it narrows the fault domain early (endpoint vs fabric) using low-cost, high-signal checks, and uses control-plane confirmation as a gating decision before deeper fabric analysis.", "score": 0.78, "time_created": "2026-01-17 17:51:38", "time_modified": "2026-01-17 17:51:38", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "SDN fabric troubleshooting when a subset of hosts show reachability issues and you need to quickly determine whether the fault is endpoint-local, access-link, or fabric/control-plane wide.", "experience": "Use a layered triage sequence: (1) start with a broad connectivity snapshot (e.g., automated ping matrix) to spot which pairs fail vs succeed; (2) immediately validate endpoint liveness on the suspected host(s) with `ip -br a` and routing table presence; (3) corroborate symptom directionality by pinging the suspected host IP from a known-good peer and interpreting errors (e.g., 'Destination Host Unreachable' suggests missing ARP/neighbor due to host/interface absence); (4) only then check SDN control-plane health on all switches via `ovs-vsctl show`/OpenFlow feature output to confirm controller connectivity and avoid misattributing an endpoint issue to the fabric. This works because it narrows the fault domain early (endpoint vs fabric) using low-cost, high-signal checks, and uses control-plane confirmation as a gating decision before deeper fabric analysis.", "tags": ["sdn", "spine-leaf", "fault-domain", "endpoint-liveness", "ping", "arp-neighbor", "ovs", "controller-connectivity"], "confidence": 0.86, "step_type": "decision", "tools_used": ["ping_matrix_or_reachability_sweep", "host_ip_br_a", "host_ping", "ovs_vsctl_show", "ovs_ofctl_show"]}}
{"workspace_id": "nika_v1", "memory_id": "60d99317795245518ea4362865134f0a", "memory_type": "task", "when_to_use": "When end-host connectivity tests fail in a routed Clos/leaf-spine fabric and you need to quickly localize whether the fault is at the host edge vs. the routing/control-plane.", "content": "Start with simple reachability signals from hosts (e.g., ping). Interpret ICMP errors to localize the failure domain: 'Destination Net Unreachable' sourced from the default gateway implies the host-to-gateway path is working but the gateway lacks a route onward (control-plane/route distribution issue). In contrast, 'Network is unreachable' on the source host suggests local interface/route problems. Then validate this hypothesis by checking host interface state and routing table (ip link/ip route) and comparing against the leaf’s host-facing interface state/IP. This pattern quickly separates access-edge failures (host NIC down/missing default route) from fabric routing failures.", "score": 0.86, "time_created": "2026-01-17 17:54:45", "time_modified": "2026-01-17 17:54:45", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When end-host connectivity tests fail in a routed Clos/leaf-spine fabric and you need to quickly localize whether the fault is at the host edge vs. the routing/control-plane.", "experience": "Start with simple reachability signals from hosts (e.g., ping). Interpret ICMP errors to localize the failure domain: 'Destination Net Unreachable' sourced from the default gateway implies the host-to-gateway path is working but the gateway lacks a route onward (control-plane/route distribution issue). In contrast, 'Network is unreachable' on the source host suggests local interface/route problems. Then validate this hypothesis by checking host interface state and routing table (ip link/ip route) and comparing against the leaf’s host-facing interface state/IP. This pattern quickly separates access-edge failures (host NIC down/missing default route) from fabric routing failures.", "tags": ["clos", "leaf-spine", "icmp-unreachable", "fault-localization", "host-routing", "default-gateway"], "confidence": 0.84, "step_type": "decision", "tools_used": ["ping", "show_ip_link", "show_interfaces"]}}
{"workspace_id": "nika_v1", "memory_id": "850bc12eb57c487fa3e40a939a0c0a8f", "memory_type": "task", "when_to_use": "When end-to-end service access is failing in a routed Clos fabric and you need to quickly determine whether the issue is at the edge host/link, the first-hop gateway, or the routing/control-plane.", "content": "Start troubleshooting from the client edge: pull `ip addr/ifconfig` and `ip route` to validate L1/L2 interface state and presence of a default route. If the client shows `state DOWN` and an empty routing table, immediately classify this as an access/link-level outage and corroborate with a simple reachability test (e.g., `ping` returning \"Network is unreachable\"). This pattern works because it distinguishes local host/link failures from upstream routing problems before spending time on fabric-wide diagnostics.", "score": 0.84, "time_created": "2026-01-17 17:57:43", "time_modified": "2026-01-17 17:57:43", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When end-to-end service access is failing in a routed Clos fabric and you need to quickly determine whether the issue is at the edge host/link, the first-hop gateway, or the routing/control-plane.", "experience": "Start troubleshooting from the client edge: pull `ip addr/ifconfig` and `ip route` to validate L1/L2 interface state and presence of a default route. If the client shows `state DOWN` and an empty routing table, immediately classify this as an access/link-level outage and corroborate with a simple reachability test (e.g., `ping` returning \"Network is unreachable\"). This pattern works because it distinguishes local host/link failures from upstream routing problems before spending time on fabric-wide diagnostics.", "tags": ["clos", "edge-validation", "link-down", "default-route", "network-unreachable"], "confidence": 0.86, "step_type": "action", "tools_used": ["host_ifconfig_iproute", "ping"]}}
{"workspace_id": "nika_v1", "memory_id": "48945c1a91924dc988c235498ce2ecc7", "memory_type": "task", "when_to_use": "When end-to-end reachability tests are inconclusive because client hosts may not have DHCP leases or may be physically disconnected (e.g., many destinations show null/unknown IPs).", "content": "Start troubleshooting from the edge by validating each client host’s L1/L2/L3 readiness before blaming routing/services. Concretely: pull `ifconfig`/`ip addr`/`ip route` from representative hosts to check (1) link state (UP/DOWN), (2) presence of an IPv4 address, and (3) presence of a default route. This quickly splits failures into distinct fault domains (e.g., link-down vs DHCP failure) and prevents wasted effort on OSPF/DNS/HTTP when the host cannot even send traffic. In this run, it identified host_1_1_1_1 as eth0 DOWN (pure access-edge issue) and host_2_1_1_1 as eth0 UP but missing IP/route (DHCP provisioning path issue).", "score": 0.9, "time_created": "2026-01-17 18:01:46", "time_modified": "2026-01-17 18:01:46", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When end-to-end reachability tests are inconclusive because client hosts may not have DHCP leases or may be physically disconnected (e.g., many destinations show null/unknown IPs).", "experience": "Start troubleshooting from the edge by validating each client host’s L1/L2/L3 readiness before blaming routing/services. Concretely: pull `ifconfig`/`ip addr`/`ip route` from representative hosts to check (1) link state (UP/DOWN), (2) presence of an IPv4 address, and (3) presence of a default route. This quickly splits failures into distinct fault domains (e.g., link-down vs DHCP failure) and prevents wasted effort on OSPF/DNS/HTTP when the host cannot even send traffic. In this run, it identified host_1_1_1_1 as eth0 DOWN (pure access-edge issue) and host_2_1_1_1 as eth0 UP but missing IP/route (DHCP provisioning path issue).", "tags": ["edge-first", "dhcp", "host-config", "fault-domain-isolation", "l1-l3-validation"], "confidence": 0.86, "step_type": "decision", "tools_used": ["ifconfig", "ip_addr", "ip_route"]}}
{"workspace_id": "nika_v1", "memory_id": "b85e1872fe9043c1975aeeb5f5a04037", "memory_type": "task", "when_to_use": "When reachability tests show mixed failures (e.g., some pings succeed, others return 'Network is unreachable' or time out) and you need to quickly localize whether the fault is host-local vs upstream routing.", "content": "Start with a small set of cross-domain dataplane probes from multiple vantage points (user host -> server, server -> user, user -> user). Treat 'ping: connect: Network is unreachable' as a strong indicator of a LOCAL routing/iface issue on the source host (missing default route, interface down, or empty routing table), while timeouts suggest upstream forwarding/routing or return-path problems. This decision point guides the next step: immediately inspect the local host's interface state and routing table before spending time on core routing.", "score": 0.84, "time_created": "2026-01-17 18:09:33", "time_modified": "2026-01-17 18:09:33", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When reachability tests show mixed failures (e.g., some pings succeed, others return 'Network is unreachable' or time out) and you need to quickly localize whether the fault is host-local vs upstream routing.", "experience": "Start with a small set of cross-domain dataplane probes from multiple vantage points (user host -> server, server -> user, user -> user). Treat 'ping: connect: Network is unreachable' as a strong indicator of a LOCAL routing/iface issue on the source host (missing default route, interface down, or empty routing table), while timeouts suggest upstream forwarding/routing or return-path problems. This decision point guides the next step: immediately inspect the local host's interface state and routing table before spending time on core routing.", "tags": ["triage", "ping", "network-unreachable", "timeout", "fault-localization", "multi-vantage"], "confidence": 0.84, "step_type": "decision", "tools_used": ["ping"]}}
{"workspace_id": "nika_v1", "memory_id": "1b5a36ef8bfb475aaba82ba3678f88ee", "memory_type": "task", "when_to_use": "When a connectivity complaint might be localized to one endpoint (e.g., one host cannot reach anything) and you need to quickly determine whether the issue is core-routing-wide or edge/source-specific.", "content": "Start with a broad, comparative reachability matrix (multi-source pings) to separate systemic failures from single-node failures. In this run, seeing many successful paths among other hosts/servers while host_1 paths were unknown/failed immediately narrowed the fault domain to host_1 or its access link, avoiding unnecessary deep dives into RIP/FRR core. This pattern works because it uses control-group style evidence (other sources) to localize the blast radius before inspecting configs.", "score": 0.86, "time_created": "2026-01-17 18:13:03", "time_modified": "2026-01-17 18:13:03", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When a connectivity complaint might be localized to one endpoint (e.g., one host cannot reach anything) and you need to quickly determine whether the issue is core-routing-wide or edge/source-specific.", "experience": "Start with a broad, comparative reachability matrix (multi-source pings) to separate systemic failures from single-node failures. In this run, seeing many successful paths among other hosts/servers while host_1 paths were unknown/failed immediately narrowed the fault domain to host_1 or its access link, avoiding unnecessary deep dives into RIP/FRR core. This pattern works because it uses control-group style evidence (other sources) to localize the blast radius before inspecting configs.", "tags": ["triage", "fault-localization", "reachability-matrix", "compare-sources", "network-diagnosis"], "confidence": 0.83, "step_type": "decision", "tools_used": ["get_reachability"]}}
{"workspace_id": "nika_v1", "memory_id": "b721f0e741ca4d2bb693e4dfcaded50a", "memory_type": "task", "when_to_use": "When submitting a final diagnosis requires mapping raw observations to a predefined taxonomy of root-cause labels.", "content": "Translate concrete evidence into standardized root-cause categories and submit only after the written diagnosis is coherent. In this case, eth0 state DOWN mapped cleanly to 'link_down', and the absence of a default route/gateway on host_1 mapped to a gateway misconfiguration category. This works because it preserves fidelity (evidence-driven labeling) while producing machine-consumable outputs for downstream evaluation/automation.", "score": 0.84, "time_created": "2026-01-17 18:13:03", "time_modified": "2026-01-17 18:13:03", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When submitting a final diagnosis requires mapping raw observations to a predefined taxonomy of root-cause labels.", "experience": "Translate concrete evidence into standardized root-cause categories and submit only after the written diagnosis is coherent. In this case, eth0 state DOWN mapped cleanly to 'link_down', and the absence of a default route/gateway on host_1 mapped to a gateway misconfiguration category. This works because it preserves fidelity (evidence-driven labeling) while producing machine-consumable outputs for downstream evaluation/automation.", "tags": ["taxonomy-mapping", "root-cause-labeling", "evidence-based", "structured-submission"], "confidence": 0.78, "step_type": "decision", "tools_used": ["list_submission_categories", "submit"]}}
{"workspace_id": "nika_v1", "memory_id": "e09494d1fde3448ca61517732d6a09f5", "memory_type": "task", "when_to_use": "When diagnosing partial connectivity in an SDN/L2 access subnet and you need to quickly determine whether the issue is host-local, edge-access, or fabric-wide.", "content": "Start with a broad reachability sweep (multi-host ping matrix) to identify whether failures are isolated to a single endpoint or affect multiple paths. Then validate directionality by testing both src→dst and dst→src for the suspected endpoint. This pattern quickly localized the problem to host_1 because (a) most other host pairs were OK, and (b) any path involving host_1 failed. The directionality check distinguished 'host can’t transmit at all' (network unreachable) from 'others can’t reach host' (destination host unreachable), strengthening the localization before deeper inspection.", "score": 0.86, "time_created": "2026-01-17 18:15:40", "time_modified": "2026-01-17 18:15:40", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When diagnosing partial connectivity in an SDN/L2 access subnet and you need to quickly determine whether the issue is host-local, edge-access, or fabric-wide.", "experience": "Start with a broad reachability sweep (multi-host ping matrix) to identify whether failures are isolated to a single endpoint or affect multiple paths. Then validate directionality by testing both src→dst and dst→src for the suspected endpoint. This pattern quickly localized the problem to host_1 because (a) most other host pairs were OK, and (b) any path involving host_1 failed. The directionality check distinguished 'host can’t transmit at all' (network unreachable) from 'others can’t reach host' (destination host unreachable), strengthening the localization before deeper inspection.", "tags": ["sdn", "reachability", "ping-matrix", "fault-localization", "directionality", "l2-access"], "confidence": 0.84, "step_type": "decision", "tools_used": ["ping_all", "ping"]}}
{"workspace_id": "nika_v1", "memory_id": "29197acbb155424698df0401961d6ef5", "memory_type": "task", "when_to_use": "When an internal host cannot reach any off-subnet destinations (including its default gateway) and errors look local (e.g., 'Destination Host Unreachable'), especially in a routed lab where the core may still be healthy.", "content": "Use a bottom-up isolation sequence: (1) run targeted pings from the failing host to (a) default gateway, (b) a same-site internal host, and (c) an external target; (2) compare with a known-good host’s reachability to the same external targets to determine whether the issue is access-edge vs. core routing; (3) immediately inspect the failing host’s interface state and routing table (ip addr/ifconfig + ip route) to confirm correct IP/mask/default route; (4) check the host neighbor table for gateway resolution (ip neigh). This pattern quickly distinguishes 'no route/core down' from 'can’t even ARP the gateway' and prevents wasting time on RIP/VPN debugging when the host can’t reach L2 next-hop.", "score": 0.86, "time_created": "2026-01-17 20:30:48", "time_modified": "2026-01-17 20:30:48", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When an internal host cannot reach any off-subnet destinations (including its default gateway) and errors look local (e.g., 'Destination Host Unreachable'), especially in a routed lab where the core may still be healthy.", "experience": "Use a bottom-up isolation sequence: (1) run targeted pings from the failing host to (a) default gateway, (b) a same-site internal host, and (c) an external target; (2) compare with a known-good host’s reachability to the same external targets to determine whether the issue is access-edge vs. core routing; (3) immediately inspect the failing host’s interface state and routing table (ip addr/ifconfig + ip route) to confirm correct IP/mask/default route; (4) check the host neighbor table for gateway resolution (ip neigh). This pattern quickly distinguishes 'no route/core down' from 'can’t even ARP the gateway' and prevents wasting time on RIP/VPN debugging when the host can’t reach L2 next-hop.", "tags": ["triage", "bottom-up", "default-gateway", "ip-neigh", "reachability", "core-vs-edge", "RIP"], "confidence": 0.82, "step_type": "decision", "tools_used": ["ping", "ifconfig", "ip addr", "ip route", "ip neigh"]}}
{"workspace_id": "nika_v1", "memory_id": "6ce9f85048194d298e0d7ba1b3340e20", "memory_type": "task", "when_to_use": "When end-to-end pings from a client to routed service subnets fail and you need to quickly determine whether the issue is in the fabric (routing/BGP/links) or at the host edge.", "content": "Start with simple reachability probes (ping service IPs and the client’s default gateway). If pings fail, immediately pull the client’s local interface and routing state (ip addr/ifconfig + ip route) to validate L3 basics (correct IP/mask, default route present). Then test the first-hop adjacency specifically: if ARP works (arping succeeds and neighbor table shows MACs) but ICMP fails even to/from the gateway, treat it as a strong indicator of host-level filtering rather than a fabric routing failure. This sequence localizes the fault quickly by separating L2 adjacency from L3/ICMP behavior.", "score": 0.82, "time_created": "2026-01-17 22:19:46", "time_modified": "2026-01-17 22:19:46", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When end-to-end pings from a client to routed service subnets fail and you need to quickly determine whether the issue is in the fabric (routing/BGP/links) or at the host edge.", "experience": "Start with simple reachability probes (ping service IPs and the client’s default gateway). If pings fail, immediately pull the client’s local interface and routing state (ip addr/ifconfig + ip route) to validate L3 basics (correct IP/mask, default route present). Then test the first-hop adjacency specifically: if ARP works (arping succeeds and neighbor table shows MACs) but ICMP fails even to/from the gateway, treat it as a strong indicator of host-level filtering rather than a fabric routing failure. This sequence localizes the fault quickly by separating L2 adjacency from L3/ICMP behavior.", "tags": ["troubleshooting", "localization", "arping", "icmp", "default-gateway", "clos", "edge-vs-fabric"], "confidence": 0.86, "step_type": "decision", "tools_used": ["ping", "show_host_state", "arping"]}}
{"workspace_id": "nika_v1", "memory_id": "508c7aa8c7d64e4a884c7d9b1470498a", "memory_type": "task", "when_to_use": "When end-to-end reachability tests show a single host failing broadly, but the rest of the network appears healthy or tool-based reachability summaries contain 'unknown' results.", "content": "Start with a coarse reachability snapshot to spot whether the issue is isolated vs systemic, then immediately validate with targeted pings from the suspected host to (1) its default gateway, then (2) a representative internal host and external service. This quickly distinguishes 'network-wide routing failure' from 'host/local-segment isolation' and avoids over-indexing on potentially flaky aggregated probe outputs.", "score": 0.86, "time_created": "2026-01-17 22:38:04", "time_modified": "2026-01-17 22:38:04", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When end-to-end reachability tests show a single host failing broadly, but the rest of the network appears healthy or tool-based reachability summaries contain 'unknown' results.", "experience": "Start with a coarse reachability snapshot to spot whether the issue is isolated vs systemic, then immediately validate with targeted pings from the suspected host to (1) its default gateway, then (2) a representative internal host and external service. This quickly distinguishes 'network-wide routing failure' from 'host/local-segment isolation' and avoids over-indexing on potentially flaky aggregated probe outputs.", "tags": ["triage", "reachability", "isolation", "ping", "baseline-validation"], "confidence": 0.78, "step_type": "decision", "tools_used": ["reachability_matrix", "ping"]}}
{"workspace_id": "nika_v1", "memory_id": "a7365a00f98e4d9a949f67b5f7a25f05", "memory_type": "task", "when_to_use": "When a topology-wide issue is suspected but initial evidence suggests only some endpoints are impacted (mixed reachability results).", "content": "Start with a broad reachability sweep (multi-source ping matrix) to quickly separate fabric-wide failures from endpoint-localized anomalies. Then pivot to targeted pairwise tests involving the suspected endpoint to confirm asymmetry/symmetry (e.g., host_1->host_2 and host_2->host_1). This pattern worked because it bounded the fault domain early (only host_1 involved) and avoided wasting time inspecting the entire SDN fabric when other hosts were healthy.", "score": 0.86, "time_created": "2026-01-17 22:41:07", "time_modified": "2026-01-17 22:41:07", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When a topology-wide issue is suspected but initial evidence suggests only some endpoints are impacted (mixed reachability results).", "experience": "Start with a broad reachability sweep (multi-source ping matrix) to quickly separate fabric-wide failures from endpoint-localized anomalies. Then pivot to targeted pairwise tests involving the suspected endpoint to confirm asymmetry/symmetry (e.g., host_1->host_2 and host_2->host_1). This pattern worked because it bounded the fault domain early (only host_1 involved) and avoided wasting time inspecting the entire SDN fabric when other hosts were healthy.", "tags": ["sdn", "fault-localization", "reachability-matrix", "scoping", "icmp"], "confidence": 0.82, "step_type": "decision", "tools_used": ["ping_sweep", "ping"]}}
{"workspace_id": "nika_v1", "memory_id": "e17305ae11ba47889a1ded3486349c2d", "memory_type": "task", "when_to_use": "When end users report they cannot reach services in a routed Clos fabric (DNS/HTTP timeouts) and you need to quickly determine whether the issue is app/host-side vs routing/control-plane.", "content": "Start at the edge and work inward: (1) verify host IP/netmask/default-gateway on client, DNS, and a representative web server; (2) validate the applications are actually listening/running (e.g., named active, web server listening on :80) to rule out service crashes; (3) run a small set of targeted pings that isolate routing domains: client→service IPs and service-host→other-service-subnet IPs. A key discriminator is ICMP 'Destination Net Unreachable' sourced from the default gateway—this strongly indicates the first-hop router lacks a route, shifting focus from endpoints to routing distribution. This pattern worked because it produced a clear localization signal (gateway-generated unreachable) while simultaneously eliminating common false leads (DNS daemon down, web server down, wrong gateway).", "score": 0.86, "time_created": "2026-01-17 23:10:54", "time_modified": "2026-01-17 23:10:54", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When end users report they cannot reach services in a routed Clos fabric (DNS/HTTP timeouts) and you need to quickly determine whether the issue is app/host-side vs routing/control-plane.", "experience": "Start at the edge and work inward: (1) verify host IP/netmask/default-gateway on client, DNS, and a representative web server; (2) validate the applications are actually listening/running (e.g., named active, web server listening on :80) to rule out service crashes; (3) run a small set of targeted pings that isolate routing domains: client→service IPs and service-host→other-service-subnet IPs. A key discriminator is ICMP 'Destination Net Unreachable' sourced from the default gateway—this strongly indicates the first-hop router lacks a route, shifting focus from endpoints to routing distribution. This pattern worked because it produced a clear localization signal (gateway-generated unreachable) while simultaneously eliminating common false leads (DNS daemon down, web server down, wrong gateway).", "tags": ["clos", "bgp", "edge-validation", "fault-localization", "icmp-unreachable", "dns", "http"], "confidence": 0.84, "step_type": "decision", "tools_used": ["ip_addr", "ip_route", "ifconfig", "ping", "systemctl", "ss", "curl"]}}
{"workspace_id": "nika_v1", "memory_id": "cd704e7f1ebf44d2bef0c4383d640c84", "memory_type": "task", "when_to_use": "When end-host connectivity in a Clos/EBGP fabric is reported down and you need to quickly localize whether the failure is at the host edge (L2/L3 gateway) vs. in the fabric control-plane.", "content": "Start from the hosts and validate the L3 edge first: (1) confirm each host’s IP/netmask/default route, then (2) ping the default gateway from each host. Use asymmetry as a decision point: if one host can reach its gateway and the other cannot, prioritize the failing host’s leaf/gateway rather than chasing spine/super-spine BGP. This pattern worked here because pc_0_0 could not ping 10.0.0.1 while pc_0_1 could ping 10.0.1.1, immediately narrowing the fault domain to leaf_router_0_0’s host-facing segment and preventing unnecessary fabric-wide investigation.", "score": 0.86, "time_created": "2026-01-17 23:13:40", "time_modified": "2026-01-17 23:13:40", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When end-host connectivity in a Clos/EBGP fabric is reported down and you need to quickly localize whether the failure is at the host edge (L2/L3 gateway) vs. in the fabric control-plane.", "experience": "Start from the hosts and validate the L3 edge first: (1) confirm each host’s IP/netmask/default route, then (2) ping the default gateway from each host. Use asymmetry as a decision point: if one host can reach its gateway and the other cannot, prioritize the failing host’s leaf/gateway rather than chasing spine/super-spine BGP. This pattern worked here because pc_0_0 could not ping 10.0.0.1 while pc_0_1 could ping 10.0.1.1, immediately narrowing the fault domain to leaf_router_0_0’s host-facing segment and preventing unnecessary fabric-wide investigation.", "tags": ["clos", "ebgp", "host-edge", "gateway-ping", "fault-localization", "asymmetric-connectivity"], "confidence": 0.86, "step_type": "decision", "tools_used": ["ping", "show_host_info"]}}
{"workspace_id": "nika_v1", "memory_id": "1707c6114007423fb3fbd77a13277448", "memory_type": "task", "when_to_use": "SDN/OpenFlow network where hosts in the same L2 access subnet cannot reach each other (e.g., ARP failures, host-to-host ping unreachable) and switches are expected to be controller-programmed.", "content": "Start by validating the data-plane symptom from an endpoint (ping peers) and immediately corroborate with L2 evidence (ARP/neighbor table showing FAILED/incomplete). This quickly distinguishes 'routing/gateway' issues from 'no L2 forwarding/flooding' issues. In SDN fabrics, ARP failure across multiple peers is a strong indicator that the switching fabric is not forwarding broadcast/unknown traffic, often because OpenFlow rules are missing or blocked by secure fail-mode.", "score": 0.86, "time_created": "2026-01-18 00:48:26", "time_modified": "2026-01-18 00:48:26", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "SDN/OpenFlow network where hosts in the same L2 access subnet cannot reach each other (e.g., ARP failures, host-to-host ping unreachable) and switches are expected to be controller-programmed.", "experience": "Start by validating the data-plane symptom from an endpoint (ping peers) and immediately corroborate with L2 evidence (ARP/neighbor table showing FAILED/incomplete). This quickly distinguishes 'routing/gateway' issues from 'no L2 forwarding/flooding' issues. In SDN fabrics, ARP failure across multiple peers is a strong indicator that the switching fabric is not forwarding broadcast/unknown traffic, often because OpenFlow rules are missing or blocked by secure fail-mode.", "tags": ["sdn", "openflow", "arp", "l2", "data-plane", "symptom-triage"], "confidence": 0.82, "step_type": "decision", "tools_used": ["ping", "ip neigh", "arp", "ip route", "ip addr"]}}
{"workspace_id": "nika_v1", "memory_id": "825a90e888d745929238e9cc982efd76", "memory_type": "task", "when_to_use": "When end-host connectivity fails in a routed Clos/leaf-spine fabric and you need to quickly distinguish edge/link issues from routing/control-plane issues.", "content": "Start with a direct host-to-host ping and interpret the ICMP source. If the error is \"Destination Net Unreachable\" sourced from the host’s default gateway (leaf SVI/IP), treat it as strong evidence the first-hop router lacks a route (control-plane/route distribution), not a random data-plane drop. Immediately validate the edge by pinging each host’s default gateway and checking host IP/default route. This narrows the fault domain to the fabric control plane before spending time on link-level debugging.", "score": 0.86, "time_created": "2026-01-18 01:06:00", "time_modified": "2026-01-18 01:06:00", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When end-host connectivity fails in a routed Clos/leaf-spine fabric and you need to quickly distinguish edge/link issues from routing/control-plane issues.", "experience": "Start with a direct host-to-host ping and interpret the ICMP source. If the error is \"Destination Net Unreachable\" sourced from the host’s default gateway (leaf SVI/IP), treat it as strong evidence the first-hop router lacks a route (control-plane/route distribution), not a random data-plane drop. Immediately validate the edge by pinging each host’s default gateway and checking host IP/default route. This narrows the fault domain to the fabric control plane before spending time on link-level debugging.", "tags": ["icmp", "net-unreachable", "fault-localization", "clos", "leaf-spine", "default-gateway", "host-validation"], "confidence": 0.86, "step_type": "decision", "tools_used": ["ping", "host_ifconfig_ip_route"]}}
{"workspace_id": "nika_v1", "memory_id": "459979d80a8344a0a3967c95b42f6d5d", "memory_type": "task", "when_to_use": "When an end-to-end service (DNS/HTTP) is unreachable across a routed Clos fabric and you need to quickly localize whether the failure is host-side, gateway-side, or control-plane (BGP) related.", "content": "Use a bidirectional reachability triage: (1) test client -> service IP reachability (e.g., ping DNS/web IPs) to confirm user impact; (2) immediately test service host -> client IP to distinguish forward-path vs return-path issues. If the reverse test returns \"Destination Net Unreachable\" sourced from the service subnet gateway IP (e.g., 10.x.x.1), treat that as strong evidence the default gateway router lacks a route to the client subnet (not an ACL/MTU issue). This narrows the fault domain from hosts/apps to routing distribution on the fabric.", "score": 0.86, "time_created": "2026-01-18 01:08:58", "time_modified": "2026-01-18 01:08:58", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When an end-to-end service (DNS/HTTP) is unreachable across a routed Clos fabric and you need to quickly localize whether the failure is host-side, gateway-side, or control-plane (BGP) related.", "experience": "Use a bidirectional reachability triage: (1) test client -> service IP reachability (e.g., ping DNS/web IPs) to confirm user impact; (2) immediately test service host -> client IP to distinguish forward-path vs return-path issues. If the reverse test returns \"Destination Net Unreachable\" sourced from the service subnet gateway IP (e.g., 10.x.x.1), treat that as strong evidence the default gateway router lacks a route to the client subnet (not an ACL/MTU issue). This narrows the fault domain from hosts/apps to routing distribution on the fabric.", "tags": ["clos", "triage", "reachability", "forward-path", "return-path", "icmp-unreachable", "fault-localization"], "confidence": 0.86, "step_type": "decision", "tools_used": ["ping", "host_info"]}}
{"workspace_id": "nika_v1", "memory_id": "fbb7a559bae44e04b529243c92173b26", "memory_type": "task", "when_to_use": "When diagnosing end-to-end service reachability in a multi-tier routed enterprise network (hosts via DHCP + server farm) and initial symptoms suggest users cannot reach internal services.", "content": "Start by separating the problem into (1) server-farm health and (2) user-edge health. Use a broad reachability sweep/ping matrix across server-farm nodes first to confirm the shared service segment (DNS/DHCP/LB/web) is stable; this quickly rules out the server farm as the primary fault domain. Then pivot to the user hosts and inspect L1/L2/L3 basics (interface state, IPv4 address presence, routing table). In this case, server-farm pings were clean (0% loss), while both hosts had no IPv4 and empty routes (and one NIC down), strongly localizing the issue to DHCP acquisition and/or access edge connectivity rather than OSPF or server outages.", "score": 0.578219210104423, "time_created": "2026-01-17 01:11:11", "time_modified": "2026-01-17 01:11:11", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When diagnosing end-to-end service reachability in a multi-tier routed enterprise network (hosts via DHCP + server farm) and initial symptoms suggest users cannot reach internal services.", "experience": "Start by separating the problem into (1) server-farm health and (2) user-edge health. Use a broad reachability sweep/ping matrix across server-farm nodes first to confirm the shared service segment (DNS/DHCP/LB/web) is stable; this quickly rules out the server farm as the primary fault domain. Then pivot to the user hosts and inspect L1/L2/L3 basics (interface state, IPv4 address presence, routing table). In this case, server-farm pings were clean (0% loss), while both hosts had no IPv4 and empty routes (and one NIC down), strongly localizing the issue to DHCP acquisition and/or access edge connectivity rather than OSPF or server outages.", "tags": ["fault-isolation", "server-farm-first", "dhcp", "host-missing-ip", "layer1-layer3-triage", "reachability-matrix"], "confidence": 0.86, "step_type": "decision", "tools_used": ["reachability_test", "get_host_network_state"], "freq": 9, "utility": 2}}
{"workspace_id": "nika_v1", "memory_id": "8b2c1d8447ce45609db58de1605f47f6", "memory_type": "task", "when_to_use": "When multiple subnets/areas are supposed to have end-to-end reachability (e.g., OSPF enterprise network) but users report they can only reach local devices and not remote services (DNS/web/server farm).", "content": "Start by running a small set of targeted pings that differentiate (1) local-subnet health vs (2) routed-path health. Specifically: ping from an access host to a remote subnet service (e.g., 10.200.0.2) and note whether the error is returned by the default gateway and whether it is 'Destination Net Unreachable'. Then ping within the same subnet on the remote side (e.g., dns_server -> web_server_0) to confirm L2/local IP stack is fine. Finally, ping from the remote subnet back toward core infrastructure (e.g., dns_server -> 172.16.x) to see if the remote gateway also lacks routes. This triangulates the fault domain to routing/control-plane (missing routes on gateways) rather than link, ACL, or application issues, because 'net unreachable' sourced by gateways strongly implies absent routes beyond connected networks.", "score": 0.84, "time_created": "2026-01-18 01:15:21", "time_modified": "2026-01-18 01:15:21", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When multiple subnets/areas are supposed to have end-to-end reachability (e.g., OSPF enterprise network) but users report they can only reach local devices and not remote services (DNS/web/server farm).", "experience": "Start by running a small set of targeted pings that differentiate (1) local-subnet health vs (2) routed-path health. Specifically: ping from an access host to a remote subnet service (e.g., 10.200.0.2) and note whether the error is returned by the default gateway and whether it is 'Destination Net Unreachable'. Then ping within the same subnet on the remote side (e.g., dns_server -> web_server_0) to confirm L2/local IP stack is fine. Finally, ping from the remote subnet back toward core infrastructure (e.g., dns_server -> 172.16.x) to see if the remote gateway also lacks routes. This triangulates the fault domain to routing/control-plane (missing routes on gateways) rather than link, ACL, or application issues, because 'net unreachable' sourced by gateways strongly implies absent routes beyond connected networks.", "tags": ["triage", "icmp", "net-unreachable", "fault-domain-isolation", "routing-vs-l2", "ospf"], "confidence": 0.84, "step_type": "decision", "tools_used": ["ping"]}}
{"workspace_id": "nika_v1", "memory_id": "b8eae365b54e4c64b6edaa12bf950d2b", "memory_type": "task", "when_to_use": "When end hosts can reach each other internally but cannot reach an external subnet/service (e.g., 20.0.x.x), and initial pings time out or show 100% loss.", "content": "Start with quick reachability probes from multiple vantage points (internal hosts, VPN server, and any adjacent routers) to separate 'core routing OK' from 'edge/zone failure'. Use simple ICMP tests to confirm internal LAN health (host_1↔host_2) and then test the specific external endpoints (vpn_server_1, web servers). If internal works but external fails consistently, pivot immediately to control-plane and routing-table inspection on the border routers rather than continuing more pings. This pattern worked because it rapidly narrowed the fault domain to the internal/external boundary and avoided spending time on unrelated host misconfigs.", "score": 0.82, "time_created": "2026-01-18 01:18:36", "time_modified": "2026-01-18 01:18:36", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When end hosts can reach each other internally but cannot reach an external subnet/service (e.g., 20.0.x.x), and initial pings time out or show 100% loss.", "experience": "Start with quick reachability probes from multiple vantage points (internal hosts, VPN server, and any adjacent routers) to separate 'core routing OK' from 'edge/zone failure'. Use simple ICMP tests to confirm internal LAN health (host_1↔host_2) and then test the specific external endpoints (vpn_server_1, web servers). If internal works but external fails consistently, pivot immediately to control-plane and routing-table inspection on the border routers rather than continuing more pings. This pattern worked because it rapidly narrowed the fault domain to the internal/external boundary and avoided spending time on unrelated host misconfigs.", "tags": ["fault-localization", "reachability", "vantage-points", "icmp", "edge-isolation"], "confidence": 0.78, "step_type": "action", "tools_used": ["connectivity_check", "run_cmd"]}}
{"workspace_id": "nika_v1", "memory_id": "0a8631b42f7c4b4a8c951ea40c3964d9", "memory_type": "task", "when_to_use": "When host-to-host traffic fails in a Clos/leaf-spine fabric and you need to quickly determine whether the issue is access-layer (host↔leaf) vs. underlay/control-plane (leaf↔spine↔super-spine/BGP).", "content": "Use a layered triage sequence: (1) attempt an end-to-end ping between hosts to confirm the symptom; (2) immediately validate each host’s local L3 baseline (IP/mask/default route) and test reachability to its default gateway; (3) if a gateway ping fails, pivot to L2/neighbor evidence (ARP/nd table) and compare the host’s gateway MAC entry to the leaf interface MAC. This isolates purely local access failures (e.g., pinned/incorrect ARP) from fabric-wide routing issues. In the trace, pc_0_0 had correct IP and default route but could not ping 10.0.0.1; ARP showed a PERMANENT gateway MAC that did not match leaf_router_0_0’s eth MAC, strongly localizing the fault to host neighbor state rather than underlay routing.", "score": 0.86, "time_created": "2026-01-18 01:24:38", "time_modified": "2026-01-18 01:24:38", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When host-to-host traffic fails in a Clos/leaf-spine fabric and you need to quickly determine whether the issue is access-layer (host↔leaf) vs. underlay/control-plane (leaf↔spine↔super-spine/BGP).", "experience": "Use a layered triage sequence: (1) attempt an end-to-end ping between hosts to confirm the symptom; (2) immediately validate each host’s local L3 baseline (IP/mask/default route) and test reachability to its default gateway; (3) if a gateway ping fails, pivot to L2/neighbor evidence (ARP/nd table) and compare the host’s gateway MAC entry to the leaf interface MAC. This isolates purely local access failures (e.g., pinned/incorrect ARP) from fabric-wide routing issues. In the trace, pc_0_0 had correct IP and default route but could not ping 10.0.0.1; ARP showed a PERMANENT gateway MAC that did not match leaf_router_0_0’s eth MAC, strongly localizing the fault to host neighbor state rather than underlay routing.", "tags": ["triage", "layered-debug", "access-layer", "arp", "default-gateway", "clos"], "confidence": 0.86, "step_type": "decision", "tools_used": ["ping", "ifconfig", "ip addr", "ip route", "arp", "ip neigh", "ip link"]}}
{"workspace_id": "nika_v1", "memory_id": "bcbe37eddf2b473aaf5a6ad91497d83d", "memory_type": "task", "when_to_use": "When a single host cannot reach external destinations in a routed lab network and you need to quickly determine whether the issue is local (host edge) or systemic (routing core).", "content": "Start with a broad reachability snapshot (any available matrix/telemetry) to see if other hosts/servers can reach the same destinations. Then run targeted ICMP tests from the failing host to (1) its default gateway and (2) at least one known-good remote IP. In parallel, run the same gateway test from a known-good host on a different LAN. This isolates the fault domain: if other hosts succeed and the failing host cannot even reach its first hop, the problem is almost certainly at the access edge (L2/L3 on the host or its directly connected router interface), not RIP/core routing. This pattern prevents wasted time debugging routing protocols when the failure is before the first hop.", "score": 0.86, "time_created": "2026-01-18 01:45:17", "time_modified": "2026-01-18 01:45:17", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When a single host cannot reach external destinations in a routed lab network and you need to quickly determine whether the issue is local (host edge) or systemic (routing core).", "experience": "Start with a broad reachability snapshot (any available matrix/telemetry) to see if other hosts/servers can reach the same destinations. Then run targeted ICMP tests from the failing host to (1) its default gateway and (2) at least one known-good remote IP. In parallel, run the same gateway test from a known-good host on a different LAN. This isolates the fault domain: if other hosts succeed and the failing host cannot even reach its first hop, the problem is almost certainly at the access edge (L2/L3 on the host or its directly connected router interface), not RIP/core routing. This pattern prevents wasted time debugging routing protocols when the failure is before the first hop.", "tags": ["fault-domain-isolation", "first-hop-test", "comparative-testing", "icmp", "edge-vs-core"], "confidence": 0.86, "step_type": "decision", "tools_used": ["reachability_matrix", "ping"]}}
{"workspace_id": "nika_v1", "memory_id": "a61ac9ef4ad74fe7a03095fe08329983", "memory_type": "task", "when_to_use": "When end-to-end host connectivity fails in a routed Clos/leaf-spine fabric and you need to quickly localize whether the fault is at the host edge, underlay links, or routing control-plane.", "content": "Start with bidirectional host-to-host pings and interpret the failure mode (e.g., gateway returning 'Destination Net Unreachable' implies the first-hop router lacks a route, not just ICMP filtering). Then immediately validate host-edge adjacency by pinging the directly connected leaf interface(s). If adjacency is healthy, pull host IP/interface/route state (ip addr/ifconfig + ip route) to catch common L3 edge errors (wrong netmask, wrong gateway). This sequence worked because it narrowed the fault domain from 'fabric-wide' to 'beyond the host↔leaf link' while simultaneously uncovering a concrete host misconfiguration (pc_0_0 /8 instead of /24) that can independently break expected routed behavior.", "score": 0.596252722262748, "time_created": "2026-01-16 23:19:42", "time_modified": "2026-01-16 23:19:42", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When end-to-end host connectivity fails in a routed Clos/leaf-spine fabric and you need to quickly localize whether the fault is at the host edge, underlay links, or routing control-plane.", "experience": "Start with bidirectional host-to-host pings and interpret the failure mode (e.g., gateway returning 'Destination Net Unreachable' implies the first-hop router lacks a route, not just ICMP filtering). Then immediately validate host-edge adjacency by pinging the directly connected leaf interface(s). If adjacency is healthy, pull host IP/interface/route state (ip addr/ifconfig + ip route) to catch common L3 edge errors (wrong netmask, wrong gateway). This sequence worked because it narrowed the fault domain from 'fabric-wide' to 'beyond the host↔leaf link' while simultaneously uncovering a concrete host misconfiguration (pc_0_0 /8 instead of /24) that can independently break expected routed behavior.", "tags": ["troubleshooting", "fault-localization", "clos", "leaf-spine", "icmp", "host-edge", "ip-route", "netmask"], "confidence": 0.84, "step_type": "action", "tools_used": ["ping", "ifconfig", "ip addr", "ip route"], "freq": 21, "utility": 6}}
{"workspace_id": "nika_v1", "memory_id": "400fa9534a5e48cd9706b029dc79bc88", "memory_type": "task", "when_to_use": "When diagnosing a routed Clos/leaf-spine fabric where end-to-end service access fails and you need to quickly separate endpoint/service health from routing/control-plane issues.", "content": "Start with a liveness sweep of every role (client, DNS, web, routers) using lightweight host commands: verify interfaces/IPs/routes and confirm critical daemons are listening (e.g., named on UDP/53, http.server on TCP/80). In parallel, attempt the canonical routing-plane entry point (e.g., vtysh). If vtysh is missing or unusable, immediately fall back to process inspection (ps/pgrep for frr/bgpd/zebra) and kernel routes (ip route) to determine whether the control plane is absent vs merely misconfigured. This works because it establishes a clean baseline: endpoints can be healthy while the fabric is still broken due to missing route distribution.", "score": 0.5974480702599639, "time_created": "2026-01-17 14:31:31", "time_modified": "2026-01-17 14:31:31", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When diagnosing a routed Clos/leaf-spine fabric where end-to-end service access fails and you need to quickly separate endpoint/service health from routing/control-plane issues.", "experience": "Start with a liveness sweep of every role (client, DNS, web, routers) using lightweight host commands: verify interfaces/IPs/routes and confirm critical daemons are listening (e.g., named on UDP/53, http.server on TCP/80). In parallel, attempt the canonical routing-plane entry point (e.g., vtysh). If vtysh is missing or unusable, immediately fall back to process inspection (ps/pgrep for frr/bgpd/zebra) and kernel routes (ip route) to determine whether the control plane is absent vs merely misconfigured. This works because it establishes a clean baseline: endpoints can be healthy while the fabric is still broken due to missing route distribution.", "tags": ["clos", "leaf-spine", "baseline", "service-health", "control-plane", "frr", "vtysh"], "confidence": 0.86, "step_type": "action", "tools_used": ["exec (ip a, ip route, ss/netstat, ps/pgrep, vtysh)"], "freq": 9, "utility": 2}}
{"workspace_id": "nika_v1", "memory_id": "a57be42d48f24abba8da292b98576bc0", "memory_type": "task", "when_to_use": "When a single host reports inability to reach remote subnets in a routed network (e.g., RIP/OSPF/BGP) and you need to quickly determine whether the issue is host-local vs. routing-domain-wide.", "content": "Start with broad reachability sampling across multiple sources (at least one other internal host and, if possible, a server-side source) to establish whether the routing domain is generally healthy. In this case, comparing host_1 vs host_2 pings to the same external prefixes showed host_2 had 0% loss while host_1 was unknown/failing, strongly indicating the problem was localized. This decision prevents wasting time debugging RIP/FRR when the evidence suggests an edge/host misconfiguration.", "score": 0.5447280944853357, "time_created": "2026-01-17 17:44:50", "time_modified": "2026-01-17 17:44:50", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When a single host reports inability to reach remote subnets in a routed network (e.g., RIP/OSPF/BGP) and you need to quickly determine whether the issue is host-local vs. routing-domain-wide.", "experience": "Start with broad reachability sampling across multiple sources (at least one other internal host and, if possible, a server-side source) to establish whether the routing domain is generally healthy. In this case, comparing host_1 vs host_2 pings to the same external prefixes showed host_2 had 0% loss while host_1 was unknown/failing, strongly indicating the problem was localized. This decision prevents wasting time debugging RIP/FRR when the evidence suggests an edge/host misconfiguration.", "tags": ["fault-localization", "compare-sources", "reachability-matrix", "rip", "scoping"], "confidence": 0.82, "step_type": "decision", "tools_used": ["reachability_matrix"], "freq": 9, "utility": 3}}
{"workspace_id": "nika_v1", "memory_id": "4368883916b646498aa815d1841ccb40", "memory_type": "task", "when_to_use": "When diagnosing SDN/L2 fabric health and you need to quickly determine whether an issue is fabric-wide vs isolated to a host/edge segment.", "content": "Start with a lightweight reachability sweep across multiple host pairs (not just one failing pair) to establish a baseline. Compare results to identify whether failures cluster around a specific endpoint or link. In this trajectory, most pairs showed 0% loss while host_1→host_4 was unknown/failed, immediately indicating a localized or path-specific issue rather than a controller/hub outage. This narrows the fault domain early and prevents unnecessary deep-dives into unrelated components.", "score": 0.84, "time_created": "2026-01-18 03:48:23", "time_modified": "2026-01-18 03:48:23", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When diagnosing SDN/L2 fabric health and you need to quickly determine whether an issue is fabric-wide vs isolated to a host/edge segment.", "experience": "Start with a lightweight reachability sweep across multiple host pairs (not just one failing pair) to establish a baseline. Compare results to identify whether failures cluster around a specific endpoint or link. In this trajectory, most pairs showed 0% loss while host_1→host_4 was unknown/failed, immediately indicating a localized or path-specific issue rather than a controller/hub outage. This narrows the fault domain early and prevents unnecessary deep-dives into unrelated components.", "tags": ["sdn", "baseline", "reachability-sweep", "fault-domain", "localization", "icmp", "l2-fabric"], "confidence": 0.84, "step_type": "action", "tools_used": ["ping_mesh"]}}
{"workspace_id": "nika_v1", "memory_id": "3980c93030424b89ad98fc24d724f899", "memory_type": "task", "when_to_use": "SDN/OpenFlow topology where some hosts have reachability issues and you need to quickly determine whether the problem is fabric-wide (controller/core) or localized (single host/edge).", "content": "Start with a broad data-plane reachability sweep across multiple host pairs to identify whether failures cluster around a single endpoint or are widespread. Then compare 'good' paths (other hosts communicating successfully) vs 'bad' paths (those involving the suspect host) to bound the fault domain. This works because a star SDN fabric will show widespread pair failures if the controller/core is broken, while a single host/link issue will appear as an isolated cluster around that host.", "score": 0.6150018457067866, "time_created": "2026-01-17 16:57:41", "time_modified": "2026-01-17 16:57:41", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "SDN/OpenFlow topology where some hosts have reachability issues and you need to quickly determine whether the problem is fabric-wide (controller/core) or localized (single host/edge).", "experience": "Start with a broad data-plane reachability sweep across multiple host pairs to identify whether failures cluster around a single endpoint or are widespread. Then compare 'good' paths (other hosts communicating successfully) vs 'bad' paths (those involving the suspect host) to bound the fault domain. This works because a star SDN fabric will show widespread pair failures if the controller/core is broken, while a single host/link issue will appear as an isolated cluster around that host.", "tags": ["sdn", "openflow", "fault-domain", "reachability", "ping-sweep", "localization"], "confidence": 0.84, "step_type": "action", "tools_used": ["reachability_sweep"], "freq": 15, "utility": 3}}
{"workspace_id": "nika_v1", "memory_id": "b1472aeca0f844ac9eb79fccec3eb8d3", "memory_type": "task", "when_to_use": "When diagnosing an SDN/L2 access network where some host-to-host tests fail but others succeed, and you need to quickly determine whether the issue is systemic (fabric/controller) or localized (endpoint/link).", "content": "Start with a lightweight reachability sweep (e.g., selective pings across multiple host pairs) and classify outcomes into: (a) consistent multi-pair failure (suggests fabric/controller/central switch) vs (b) failures concentrated on a single host (suggests endpoint or its access link). In this case, successful host_2→host_3 and host_4→host_3 pings ruled out a fabric-wide outage, while every test involving host_1 returned 'unknown', strongly localizing the fault domain to host_1 or its immediate attachment. This pattern works because it uses comparative dataplane evidence to narrow scope before spending effort on deeper device inspection.", "score": 0.6125530206470482, "time_created": "2026-01-16 23:55:06", "time_modified": "2026-01-16 23:55:06", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When diagnosing an SDN/L2 access network where some host-to-host tests fail but others succeed, and you need to quickly determine whether the issue is systemic (fabric/controller) or localized (endpoint/link).", "experience": "Start with a lightweight reachability sweep (e.g., selective pings across multiple host pairs) and classify outcomes into: (a) consistent multi-pair failure (suggests fabric/controller/central switch) vs (b) failures concentrated on a single host (suggests endpoint or its access link). In this case, successful host_2→host_3 and host_4→host_3 pings ruled out a fabric-wide outage, while every test involving host_1 returned 'unknown', strongly localizing the fault domain to host_1 or its immediate attachment. This pattern works because it uses comparative dataplane evidence to narrow scope before spending effort on deeper device inspection.", "tags": ["sdn", "scope-reduction", "reachability-matrix", "fault-localization", "dataplane"], "confidence": 0.84, "step_type": "decision", "tools_used": ["ping_sweep"], "freq": 31, "utility": 11}}
{"workspace_id": "nika_v1", "memory_id": "1a6cd968bf68405f9633886b37d13a93", "memory_type": "task", "when_to_use": "When troubleshooting an SDN/L2 access network where basic connectivity seems mostly OK but there are oddities (e.g., unusually low RTTs, asymmetric/unknown test results, or inconsistent host identity).", "content": "Start with a broad end-to-end liveness sweep (multi-pair ping matrix) and simultaneously capture the tool’s host->IP inventory mapping. Then sanity-check that mapping for uniqueness within the shared subnet. In this case, the ping sweep showed 0% loss (fabric forwarding works), but the inventory revealed host_1 and host_4 both mapped to 10.0.0.1. Correlating that with an extremely low RTT on host_1->host_4 (10.0.0.1) supported the interpretation that the traffic was effectively looping back/pinging itself due to duplicate addressing. This pattern quickly separates 'fabric down' from 'endpoint identity/addressing' faults and avoids unnecessary deep SDN controller/flow debugging.", "score": 0.6124176158860979, "time_created": "2026-01-16 22:06:43", "time_modified": "2026-01-16 22:06:43", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When troubleshooting an SDN/L2 access network where basic connectivity seems mostly OK but there are oddities (e.g., unusually low RTTs, asymmetric/unknown test results, or inconsistent host identity).", "experience": "Start with a broad end-to-end liveness sweep (multi-pair ping matrix) and simultaneously capture the tool’s host->IP inventory mapping. Then sanity-check that mapping for uniqueness within the shared subnet. In this case, the ping sweep showed 0% loss (fabric forwarding works), but the inventory revealed host_1 and host_4 both mapped to 10.0.0.1. Correlating that with an extremely low RTT on host_1->host_4 (10.0.0.1) supported the interpretation that the traffic was effectively looping back/pinging itself due to duplicate addressing. This pattern quickly separates 'fabric down' from 'endpoint identity/addressing' faults and avoids unnecessary deep SDN controller/flow debugging.", "tags": ["sdn", "star-topology", "reachability", "ping-sweep", "ip-conflict", "arp-flux", "fault-isolation"], "confidence": 0.86, "step_type": "decision", "tools_used": ["reachability_matrix"], "freq": 37, "utility": 13}}
{"workspace_id": "nika_v1", "memory_id": "2519139a77f04ab3b64f0bc12834ec4b", "memory_type": "task", "when_to_use": "Diagnosing SDN spine–leaf issues where it’s unclear whether the outage is control-plane (controller/switch) or data-plane (host/link/flows).", "content": "Start with a two-plane split: (1) verify management/control-plane health on every switch (management IP present, controller target correct, `is_connected: true`, and ping to controller succeeds), then (2) test data-plane reachability between hosts. This quickly prevents misattributing host connectivity failures to SDN/controller outages. In the sequence, confirming controller connectivity and successful pings to 20.0.0.100 established that OpenFlow sessions and management network were healthy, so subsequent failures were treated as data-plane/local access problems.", "score": 0.6369417181177804, "time_created": "2026-01-17 18:18:25", "time_modified": "2026-01-17 18:18:25", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "Diagnosing SDN spine–leaf issues where it’s unclear whether the outage is control-plane (controller/switch) or data-plane (host/link/flows).", "experience": "Start with a two-plane split: (1) verify management/control-plane health on every switch (management IP present, controller target correct, `is_connected: true`, and ping to controller succeeds), then (2) test data-plane reachability between hosts. This quickly prevents misattributing host connectivity failures to SDN/controller outages. In the sequence, confirming controller connectivity and successful pings to 20.0.0.100 established that OpenFlow sessions and management network were healthy, so subsequent failures were treated as data-plane/local access problems.", "tags": ["sdn", "spine-leaf", "control-plane", "management-network", "openflow", "fault-isolation"], "confidence": 0.86, "step_type": "decision", "tools_used": ["ip addr", "ovs-vsctl show", "ping"], "freq": 11}}
{"workspace_id": "nika_v1", "memory_id": "10bcb02401364b05824f5e595cc3bb73", "memory_type": "task", "when_to_use": "When diagnosing SDN spine–leaf issues where some host-to-host tests fail/are unknown and you need to quickly separate host misconfiguration from fabric/control-plane faults.", "content": "Use a layered triage sequence: (1) run a broad host reachability sweep to identify which endpoints/pairs are failing vs healthy; (2) if a host shows 'unknown' or cannot be targeted, immediately inspect that host's L3 state with `ip -br addr` / routes to confirm whether it has a valid subnet IP; (3) validate that the rest of the fabric is healthy by checking successful inter-leaf pings among other hosts. This works because it localizes faults early (endpoint vs network) and avoids over-investigating the fabric when the failure signature is consistent with a single host missing an IP.", "score": 0.6343033470531058, "time_created": "2026-01-16 23:57:37", "time_modified": "2026-01-16 23:57:37", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When diagnosing SDN spine–leaf issues where some host-to-host tests fail/are unknown and you need to quickly separate host misconfiguration from fabric/control-plane faults.", "experience": "Use a layered triage sequence: (1) run a broad host reachability sweep to identify which endpoints/pairs are failing vs healthy; (2) if a host shows 'unknown' or cannot be targeted, immediately inspect that host's L3 state with `ip -br addr` / routes to confirm whether it has a valid subnet IP; (3) validate that the rest of the fabric is healthy by checking successful inter-leaf pings among other hosts. This works because it localizes faults early (endpoint vs network) and avoids over-investigating the fabric when the failure signature is consistent with a single host missing an IP.", "tags": ["sdn", "spine-leaf", "triage", "reachability-sweep", "host-misconfig", "missing-ip", "fault-localization"], "confidence": 0.84, "step_type": "decision", "tools_used": ["reachability_test", "host_exec"], "freq": 14, "utility": 3}}
{"workspace_id": "nika_v1", "memory_id": "231e8d905c214e4a97aa6261a5c79e70", "memory_type": "task", "when_to_use": "When diagnosing SDN spine–leaf labs where a subset of hosts are unreachable and you must quickly determine whether the fault is endpoint-level vs fabric/control-plane.", "content": "Start with endpoint interface/address sanity checks on every host (e.g., `ip -br link` and `ip -br addr`). Treat 'missing NIC' or 'no IP on expected interface' as a first-class fault signal before spending time on switch/controller debugging. In this run, host_1_1 showed no eth0 at all and only loopback addressing, which immediately localized the problem to the endpoint/link attachment rather than L2/L3 forwarding. This pattern works because it validates the minimum prerequisites for any dataplane test and prevents misattributing failures to the SDN fabric.", "score": 0.6355652483654405, "time_created": "2026-01-17 00:59:47", "time_modified": "2026-01-17 00:59:47", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When diagnosing SDN spine–leaf labs where a subset of hosts are unreachable and you must quickly determine whether the fault is endpoint-level vs fabric/control-plane.", "experience": "Start with endpoint interface/address sanity checks on every host (e.g., `ip -br link` and `ip -br addr`). Treat 'missing NIC' or 'no IP on expected interface' as a first-class fault signal before spending time on switch/controller debugging. In this run, host_1_1 showed no eth0 at all and only loopback addressing, which immediately localized the problem to the endpoint/link attachment rather than L2/L3 forwarding. This pattern works because it validates the minimum prerequisites for any dataplane test and prevents misattributing failures to the SDN fabric.", "tags": ["sdn", "spine-leaf", "host-triage", "ip-link", "missing-interface", "fault-localization"], "confidence": 0.86, "step_type": "decision", "tools_used": ["ip"], "freq": 27, "utility": 4}}
{"workspace_id": "nika_v1", "memory_id": "8627e25116c340bba1457c9123d30287", "memory_type": "task", "when_to_use": "When end-host connectivity tests fail in a routed Clos/leaf-spine fabric and you need to quickly determine whether the failure is edge/L2, underlay, or control-plane (BGP) related.", "content": "Start with symmetric host-to-host pings and interpret the exact ICMP failure type. If the source host receives \"Destination Net Unreachable\" from its default gateway, treat this as strong evidence the first-hop router lacks a route (control-plane/route-distribution issue) rather than a dataplane drop. Immediately validate host IP/netmask/default-gateway and confirm the leaf host-facing interface is up and addressed correctly. This narrows the fault domain away from host misconfig and access link issues before spending time on deeper fabric checks.", "score": 0.6106545722205091, "time_created": "2026-01-17 09:23:09", "time_modified": "2026-01-17 09:23:09", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When end-host connectivity tests fail in a routed Clos/leaf-spine fabric and you need to quickly determine whether the failure is edge/L2, underlay, or control-plane (BGP) related.", "experience": "Start with symmetric host-to-host pings and interpret the exact ICMP failure type. If the source host receives \"Destination Net Unreachable\" from its default gateway, treat this as strong evidence the first-hop router lacks a route (control-plane/route-distribution issue) rather than a dataplane drop. Immediately validate host IP/netmask/default-gateway and confirm the leaf host-facing interface is up and addressed correctly. This narrows the fault domain away from host misconfig and access link issues before spending time on deeper fabric checks.", "tags": ["clos", "leaf-spine", "icmp", "net-unreachable", "fault-domain-isolation", "host-validation"], "confidence": 0.84, "step_type": "decision", "tools_used": ["ping", "ifconfig", "ip addr", "ip route"], "freq": 63, "utility": 16}}
{"workspace_id": "nika_v1", "memory_id": "beb4faef81f1491796571591fa256208", "memory_type": "task", "when_to_use": "When troubleshooting a multi-tier routed enterprise lab where end-to-end service tests fail and you suspect basic underlay issues (DHCP, routing daemons, node liveness).", "content": "Start with a bottom-up viability sweep: (1) verify each critical node/container is running by attempting a simple command; (2) capture interface addressing and routes on routers/servers/hosts via `ip addr` + `ip route`; (3) check control-plane processes with `pgrep` for expected routing daemons. This sequence quickly separated failures into: a crashed host (container not running), a live host missing DHCP address (no IPv4 on eth0), and a systemic routing-plane outage (no FRR daemons / no vtysh). It works because it establishes whether failures are caused by missing endpoints, missing L3 configuration, or missing dynamic routing before spending time on higher-layer service debugging.", "score": 0.5933261399174988, "time_created": "2026-01-16 21:23:51", "time_modified": "2026-01-16 21:23:51", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When troubleshooting a multi-tier routed enterprise lab where end-to-end service tests fail and you suspect basic underlay issues (DHCP, routing daemons, node liveness).", "experience": "Start with a bottom-up viability sweep: (1) verify each critical node/container is running by attempting a simple command; (2) capture interface addressing and routes on routers/servers/hosts via `ip addr` + `ip route`; (3) check control-plane processes with `pgrep` for expected routing daemons. This sequence quickly separated failures into: a crashed host (container not running), a live host missing DHCP address (no IPv4 on eth0), and a systemic routing-plane outage (no FRR daemons / no vtysh). It works because it establishes whether failures are caused by missing endpoints, missing L3 configuration, or missing dynamic routing before spending time on higher-layer service debugging.", "tags": ["triage", "bottom-up", "liveness", "ip-addr", "ip-route", "control-plane", "frr", "ospf"], "confidence": 0.86, "step_type": "action", "tools_used": ["exec (remote command)", "pgrep", "ip"], "freq": 167, "utility": 44}}
{"workspace_id": "nika_v1", "memory_id": "f4cfa3b1dca44ac1b9f13d7cc004f813", "memory_type": "task", "when_to_use": "Diagnosing end-to-end reachability in a hierarchical routed enterprise network where user hosts report inability to reach server-farm services (DNS/HTTP) and you need to quickly localize whether the issue is access-edge, routing, or server-farm.", "content": "Start by testing intra-server-farm connectivity (ping matrix among 10.200.0.0/24 nodes) to establish a known-good baseline and avoid chasing application issues. Then immediately pivot to endpoint L3 readiness checks on each user host (ip addr/ifconfig + ip route). If pings from hosts fail with 'Network is unreachable' or routes are empty, treat it as an addressing/default-gateway/DHCP acquisition problem rather than a routing/ACL issue. This pattern worked because it separated 'server farm healthy' from 'user access broken' early, narrowing the fault domain before deeper control-plane inspection.", "score": 0.6291602524915606, "time_created": "2026-01-17 00:33:37", "time_modified": "2026-01-17 00:33:37", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "Diagnosing end-to-end reachability in a hierarchical routed enterprise network where user hosts report inability to reach server-farm services (DNS/HTTP) and you need to quickly localize whether the issue is access-edge, routing, or server-farm.", "experience": "Start by testing intra-server-farm connectivity (ping matrix among 10.200.0.0/24 nodes) to establish a known-good baseline and avoid chasing application issues. Then immediately pivot to endpoint L3 readiness checks on each user host (ip addr/ifconfig + ip route). If pings from hosts fail with 'Network is unreachable' or routes are empty, treat it as an addressing/default-gateway/DHCP acquisition problem rather than a routing/ACL issue. This pattern worked because it separated 'server farm healthy' from 'user access broken' early, narrowing the fault domain before deeper control-plane inspection.", "tags": ["baseline-first", "fault-domain-isolation", "dhcp", "host-l3-readiness", "ping-matrix"], "confidence": 0.86, "step_type": "reasoning", "tools_used": ["ping_mesh", "get_host_network_state"], "freq": 139, "utility": 20}}
{"workspace_id": "nika_v1", "memory_id": "d78179a15e0c4bedaf575d55dadbbcb8", "memory_type": "task", "when_to_use": "When diagnosing multi-tier enterprise networks where end hosts report loss of connectivity and there are multiple dependent services (DHCP, routing/OSPF, DNS, load balancer).", "content": "Start with a broad reachability snapshot to quickly separate 'local segment healthy' from 'end-to-end broken'. Use a reachability sweep (e.g., ping matrix) between key roles (DHCP/DNS/LB/web) to confirm whether the server farm is internally consistent. This establishes a working baseline and prevents misattributing failures to servers when the issue is upstream. In this case, successful intra-10.200.0.0/24 pings narrowed the fault domain away from server endpoints and toward user-edge/routing.", "score": 0.6019860733229095, "time_created": "2026-01-18 01:12:37", "time_modified": "2026-01-18 01:12:37", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When diagnosing multi-tier enterprise networks where end hosts report loss of connectivity and there are multiple dependent services (DHCP, routing/OSPF, DNS, load balancer).", "experience": "Start with a broad reachability snapshot to quickly separate 'local segment healthy' from 'end-to-end broken'. Use a reachability sweep (e.g., ping matrix) between key roles (DHCP/DNS/LB/web) to confirm whether the server farm is internally consistent. This establishes a working baseline and prevents misattributing failures to servers when the issue is upstream. In this case, successful intra-10.200.0.0/24 pings narrowed the fault domain away from server endpoints and toward user-edge/routing.", "tags": ["triage", "baseline", "reachability-matrix", "fault-domain-isolation", "server-farm"], "confidence": 0.78, "step_type": "action", "tools_used": ["reachability_sweep"], "freq": 19, "utility": 1}}
{"workspace_id": "nika_v1", "memory_id": "c834b0ae8fc7474394bb5687043c97cf", "memory_type": "task", "when_to_use": "When diagnosing multi-zone networks (internal routing + external services + VPN overlay) and you need to quickly determine whether the issue is in the core routing domain or at the edge/host.", "content": "Use a two-phase triage: (1) run an end-to-end reachability matrix (host↔host, external servers↔hosts) to spot asymmetric or localized failures; (2) immediately validate L3 basics on each endpoint (ip addr/link state + ip route). In this case, the matrix showed only paths involving host_1 were failing/unknown while host_2↔external succeeded, prompting inspection of host_1 where eth0 was DOWN and routes were missing. This pattern works because it narrows scope early and prevents over-investigating routing daemons when the fault is a local interface/route state.", "score": 0.5783973845532842, "time_created": "2026-01-17 01:17:14", "time_modified": "2026-01-17 01:17:14", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When diagnosing multi-zone networks (internal routing + external services + VPN overlay) and you need to quickly determine whether the issue is in the core routing domain or at the edge/host.", "experience": "Use a two-phase triage: (1) run an end-to-end reachability matrix (host↔host, external servers↔hosts) to spot asymmetric or localized failures; (2) immediately validate L3 basics on each endpoint (ip addr/link state + ip route). In this case, the matrix showed only paths involving host_1 were failing/unknown while host_2↔external succeeded, prompting inspection of host_1 where eth0 was DOWN and routes were missing. This pattern works because it narrows scope early and prevents over-investigating routing daemons when the fault is a local interface/route state.", "tags": ["triage", "reachability-matrix", "scope-reduction", "host-routing", "link-state"], "confidence": 0.84, "step_type": "decision", "tools_used": ["reachability_matrix", "show_interfaces", "show_routes"], "freq": 40, "utility": 11}}
{"workspace_id": "nika_v1", "memory_id": "ff61c10ca7af4d43ab2cb0803ae4e8e1", "memory_type": "task", "when_to_use": "When a multi-zone network (internal routing + external services + VPN overlay) shows partial reachability and you need to quickly isolate whether the issue is core routing vs an edge host/link.", "content": "Start with a broad reachability matrix (multi-source pings) to identify whether failures are localized to one source/destination or systemic. Compare a suspected host's results against a known-good host (e.g., host_1 vs host_2) for the same targets (internal host, external server). If other nodes can reach across domains with low loss but one host shows intermittent loss to many destinations, treat it as an edge fault-domain and stop chasing routing/VPN causes. This works because it uses differential testing to separate control-plane/routing issues from host/link impairments.", "score": 0.5791381804779023, "time_created": "2026-01-18 03:46:01", "time_modified": "2026-01-18 03:46:01", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When a multi-zone network (internal routing + external services + VPN overlay) shows partial reachability and you need to quickly isolate whether the issue is core routing vs an edge host/link.", "experience": "Start with a broad reachability matrix (multi-source pings) to identify whether failures are localized to one source/destination or systemic. Compare a suspected host's results against a known-good host (e.g., host_1 vs host_2) for the same targets (internal host, external server). If other nodes can reach across domains with low loss but one host shows intermittent loss to many destinations, treat it as an edge fault-domain and stop chasing routing/VPN causes. This works because it uses differential testing to separate control-plane/routing issues from host/link impairments.", "tags": ["fault-domain-isolation", "differential-diagnosis", "reachability-matrix", "multi-zone-network", "rip", "vpn"], "confidence": 0.83, "step_type": "decision", "tools_used": ["reachability_matrix", "ping"], "freq": 3}}
{"workspace_id": "nika_v1", "memory_id": "ad053cf5c9c44ba49f4cb7dd976a5f22", "memory_type": "task", "when_to_use": "When troubleshooting a lab/network where automated reachability tooling fails or returns parsing errors (e.g., cannot parse `ip -j addr`) and you still need to produce a reliable health diagnosis.", "content": "Fall back from high-level reachability tools to per-node, low-level state collection: query each device for `ifconfig`/`ip addr`/`ip route` to establish (1) whether the node is running, (2) interface up/up and IP assignments, and (3) presence/absence of default and connected routes. This works because it decouples diagnosis from brittle JSON/parsing dependencies and immediately distinguishes 'tool failure' from 'node is down' (e.g., Docker conflict: container not running). It also provides concrete evidence for the report (IPs, routes, interface states) even when end-to-end tests are impossible.", "score": 0.86, "time_created": "2026-01-18 05:33:46", "time_modified": "2026-01-18 05:33:46", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When troubleshooting a lab/network where automated reachability tooling fails or returns parsing errors (e.g., cannot parse `ip -j addr`) and you still need to produce a reliable health diagnosis.", "experience": "Fall back from high-level reachability tools to per-node, low-level state collection: query each device for `ifconfig`/`ip addr`/`ip route` to establish (1) whether the node is running, (2) interface up/up and IP assignments, and (3) presence/absence of default and connected routes. This works because it decouples diagnosis from brittle JSON/parsing dependencies and immediately distinguishes 'tool failure' from 'node is down' (e.g., Docker conflict: container not running). It also provides concrete evidence for the report (IPs, routes, interface states) even when end-to-end tests are impossible.", "tags": ["fallback", "baseline-inventory", "device-health", "ip-addr", "ip-route", "tooling-failure", "container-down"], "confidence": 0.86, "step_type": "action", "tools_used": ["get_ifconfig_inet", "get_ip_addr", "get_ip_route"]}}
{"workspace_id": "nika_v1", "memory_id": "823addae5e0641ae91ad82142fa7ffdb", "memory_type": "task", "when_to_use": "When initial reachability tests (e.g., ping) look healthy but the topology implies routing/BGP should be involved and results seem suspiciously \"too good\".", "content": "Start with a minimal end-to-end connectivity check (ping both directions), then immediately validate whether the test is actually exercising the intended path by cross-checking host/interface addressing and L3 adjacency. In this run, ping success was misleading because both hosts resolved to the same destination IP. Verifying `ifconfig/ip addr` and `ip route` on both hosts exposed a duplicate IP and a gateway/subnet mismatch, explaining why ICMP did not prove routed/BGP connectivity. Adding a quick traceroute to the tested destination helped confirm the ping was effectively local/self-referential (first hop == destination), strengthening the diagnosis.", "score": 0.87, "time_created": "2026-01-18 05:37:21", "time_modified": "2026-01-18 05:37:21", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When initial reachability tests (e.g., ping) look healthy but the topology implies routing/BGP should be involved and results seem suspiciously \"too good\".", "experience": "Start with a minimal end-to-end connectivity check (ping both directions), then immediately validate whether the test is actually exercising the intended path by cross-checking host/interface addressing and L3 adjacency. In this run, ping success was misleading because both hosts resolved to the same destination IP. Verifying `ifconfig/ip addr` and `ip route` on both hosts exposed a duplicate IP and a gateway/subnet mismatch, explaining why ICMP did not prove routed/BGP connectivity. Adding a quick traceroute to the tested destination helped confirm the ping was effectively local/self-referential (first hop == destination), strengthening the diagnosis.", "tags": ["reachability-validation", "false-positive-ping", "ip-conflict", "subnet-mismatch", "traceroute-corroboration"], "confidence": 0.84, "step_type": "decision", "tools_used": ["reachability_test", "show_ifconfig", "show_ip_addr", "show_ip_route", "traceroute"]}}
{"workspace_id": "nika_v1", "memory_id": "53c6627d91dc45389219446e0820132a", "memory_type": "task", "when_to_use": "When users report end-to-end service failure in a routed Clos/EVPN/EBGP fabric and you need to quickly localize whether the issue is host-side, first-hop, or fabric routing/control-plane.", "content": "Use a layered reachability triage: (1) verify host IP/default route on each endpoint (client, DNS, web) via ip addr/ip route; (2) ping the local default gateway from each endpoint to separate L2/first-hop issues from fabric issues; (3) attempt cross-subnet pings and note the exact ICMP behavior (silent drop vs 'Destination Net Unreachable' and who generated it). This sequence worked because it immediately showed: client can reach its gateway (192.168.0.1) but receives 'net unreachable' from that gateway to 10.0.1.0/24 (missing route upstream), while dns_pod0 cannot reach its own gateway (10.0.0.1), indicating a local leaf/subnet problem distinct from the fabric-wide routing failure.", "score": 0.5478689404778511, "time_created": "2026-01-17 05:59:50", "time_modified": "2026-01-17 05:59:50", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When users report end-to-end service failure in a routed Clos/EVPN/EBGP fabric and you need to quickly localize whether the issue is host-side, first-hop, or fabric routing/control-plane.", "experience": "Use a layered reachability triage: (1) verify host IP/default route on each endpoint (client, DNS, web) via ip addr/ip route; (2) ping the local default gateway from each endpoint to separate L2/first-hop issues from fabric issues; (3) attempt cross-subnet pings and note the exact ICMP behavior (silent drop vs 'Destination Net Unreachable' and who generated it). This sequence worked because it immediately showed: client can reach its gateway (192.168.0.1) but receives 'net unreachable' from that gateway to 10.0.1.0/24 (missing route upstream), while dns_pod0 cannot reach its own gateway (10.0.0.1), indicating a local leaf/subnet problem distinct from the fabric-wide routing failure.", "tags": ["clos", "bgp", "reachability", "fault-localization", "icmp", "default-gateway", "layered-triage"], "confidence": 0.84, "step_type": "decision", "tools_used": ["ifconfig/ip addr", "ip route", "ping"], "freq": 3, "utility": 2}}
{"workspace_id": "nika_v1", "memory_id": "b720a689050245aeb0d475c040449b85", "memory_type": "task", "when_to_use": "When diagnosing end-to-end reachability failures in a small routed lab (hosts behind routers) and you need to quickly localize whether the fault is at the edge, the transit link, or the routing/control-plane.", "content": "Use a bottom-up, hop-by-hop validation sequence: (1) collect IP addressing + default routes on each host, (2) collect interface/IP and kernel routes on each router, (3) run targeted pings that separately test host->gateway, router->router underlay, and host->remote host. Then interpret ICMP error sources/messages to localize the fault domain: 'Host Unreachable' sourced by the host often indicates first-hop/ARP/gateway issues; 'Net Unreachable' sourced by a router indicates missing route in that router's RIB/FIB. This pattern worked because it decomposed the problem into independent segments and used the error origin (who generated the ICMP) as a strong decision signal.", "score": 0.5502755278133603, "time_created": "2026-01-18 05:39:19", "time_modified": "2026-01-18 05:39:19", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When diagnosing end-to-end reachability failures in a small routed lab (hosts behind routers) and you need to quickly localize whether the fault is at the edge, the transit link, or the routing/control-plane.", "experience": "Use a bottom-up, hop-by-hop validation sequence: (1) collect IP addressing + default routes on each host, (2) collect interface/IP and kernel routes on each router, (3) run targeted pings that separately test host->gateway, router->router underlay, and host->remote host. Then interpret ICMP error sources/messages to localize the fault domain: 'Host Unreachable' sourced by the host often indicates first-hop/ARP/gateway issues; 'Net Unreachable' sourced by a router indicates missing route in that router's RIB/FIB. This pattern worked because it decomposed the problem into independent segments and used the error origin (who generated the ICMP) as a strong decision signal.", "tags": ["troubleshooting", "reachability", "fault-localization", "icmp", "default-gateway", "routing-table"], "confidence": 0.86, "step_type": "reasoning", "tools_used": ["ifconfig", "ip addr", "ip route", "ping"], "freq": 1, "utility": 1}}
{"workspace_id": "nika_v1", "memory_id": "96ee360144a44473b6e6eb29564ae897", "memory_type": "task", "when_to_use": "When end-to-end connectivity tests are missing/\"unknown\" or fail, and you need to quickly localize whether the issue is host-edge L3, underlay link, or routing/control-plane.", "content": "Use a bottom-up dataplane validation sequence: (1) collect interface/IP/route state on all endpoints (hosts + routers), then (2) run targeted pings only across directly connected adjacencies (router1↔router2 transit, router2↔pc2 access, router1↔pc1 access). This isolates fault domains without assuming routing is working. In this case, adjacency pings proved the transit and pc2-side access were healthy, while pc1 showed no IPv4 address and an empty routing table—pinpointing the primary failure as host-edge L3 on pc1.", "score": 0.86, "time_created": "2026-01-18 05:47:19", "time_modified": "2026-01-18 05:47:19", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When end-to-end connectivity tests are missing/\"unknown\" or fail, and you need to quickly localize whether the issue is host-edge L3, underlay link, or routing/control-plane.", "experience": "Use a bottom-up dataplane validation sequence: (1) collect interface/IP/route state on all endpoints (hosts + routers), then (2) run targeted pings only across directly connected adjacencies (router1↔router2 transit, router2↔pc2 access, router1↔pc1 access). This isolates fault domains without assuming routing is working. In this case, adjacency pings proved the transit and pc2-side access were healthy, while pc1 showed no IPv4 address and an empty routing table—pinpointing the primary failure as host-edge L3 on pc1.", "tags": ["troubleshooting", "fault-isolation", "dataplane", "adjacency-ping", "host-missing-ip", "ip-route", "ifconfig"], "confidence": 0.86, "step_type": "action", "tools_used": ["ifconfig", "ip addr", "ip route", "ping"]}}
{"workspace_id": "nika_v1", "memory_id": "eb64783c2928494c8b8c4b61c4487fd0", "memory_type": "task", "when_to_use": "When end-to-end connectivity tests are missing/unknown or inconclusive in a small routed lab (hosts + routers) and you need to quickly localize whether the failure is host-side, link-side, or routing/control-plane.", "content": "Use a bottom-up triage sequence: (1) collect `ip addr/ifconfig` and `ip route` from every node to confirm required interfaces exist, are UP, and have expected addressing; (2) run targeted pings that map directly to each physical segment (host->gateway, router<->router, router->host) to validate underlay reachability per hop. This worked because it isolated a definitive L2/L3 provisioning fault on pc1 (missing eth0, empty routes) while simultaneously proving the inter-router and pc2 access segments were healthy, preventing wasted effort debugging BGP before basic attachment was verified.", "score": 0.86, "time_created": "2026-01-18 05:51:40", "time_modified": "2026-01-18 05:51:40", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When end-to-end connectivity tests are missing/unknown or inconclusive in a small routed lab (hosts + routers) and you need to quickly localize whether the failure is host-side, link-side, or routing/control-plane.", "experience": "Use a bottom-up triage sequence: (1) collect `ip addr/ifconfig` and `ip route` from every node to confirm required interfaces exist, are UP, and have expected addressing; (2) run targeted pings that map directly to each physical segment (host->gateway, router<->router, router->host) to validate underlay reachability per hop. This worked because it isolated a definitive L2/L3 provisioning fault on pc1 (missing eth0, empty routes) while simultaneously proving the inter-router and pc2 access segments were healthy, preventing wasted effort debugging BGP before basic attachment was verified.", "tags": ["bottom-up-troubleshooting", "interface-audit", "routing-table-audit", "segment-ping", "fault-localization"], "confidence": 0.86, "step_type": "action", "tools_used": ["ip_addr", "ifconfig", "ip_route", "ping"]}}
{"workspace_id": "nika_v1", "memory_id": "5ed9284f767b464d9bd494d2eedf874e", "memory_type": "task", "when_to_use": "When end-to-end host connectivity is reported broken in a small routed/BGP lab and you need to quickly localize whether the failure is at the edge, transit, or routing/control-plane.", "content": "Use a layered, hop-by-hop validation sequence: (1) collect IP addressing and routes on all nodes (hosts + routers) to spot obvious L3 mismatches; (2) test edge reachability from each host to its default gateway and interpret ICMP source strings (errors sourced by the host often indicate local ARP/gateway/subnet issues); (3) independently test router-to-router transit to confirm underlay health; (4) test remote-subnet reachability from the far-side host and interpret 'Destination Net Unreachable' sourced by the gateway as evidence of missing route on that router. This pattern works because it separates local attachment faults from transit faults and from missing routing information, preventing premature focus on BGP before basic L3 adjacency is validated.", "score": 0.5572850792856093, "time_created": "2026-01-18 05:41:24", "time_modified": "2026-01-18 05:41:24", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When end-to-end host connectivity is reported broken in a small routed/BGP lab and you need to quickly localize whether the failure is at the edge, transit, or routing/control-plane.", "experience": "Use a layered, hop-by-hop validation sequence: (1) collect IP addressing and routes on all nodes (hosts + routers) to spot obvious L3 mismatches; (2) test edge reachability from each host to its default gateway and interpret ICMP source strings (errors sourced by the host often indicate local ARP/gateway/subnet issues); (3) independently test router-to-router transit to confirm underlay health; (4) test remote-subnet reachability from the far-side host and interpret 'Destination Net Unreachable' sourced by the gateway as evidence of missing route on that router. This pattern works because it separates local attachment faults from transit faults and from missing routing information, preventing premature focus on BGP before basic L3 adjacency is validated.", "tags": ["troubleshooting", "layered-isolation", "icmp-interpretation", "default-gateway", "routing-table", "fault-localization"], "confidence": 0.86, "step_type": "decision", "tools_used": ["ifconfig", "ip_addr", "ip_route", "ping"], "freq": 4, "utility": 2}}
{"workspace_id": "nika_v1", "memory_id": "68d88831c30442c88c6ab98863c9f161", "memory_type": "task", "when_to_use": "When end-to-end host connectivity fails in a small routed lab (e.g., BGP between two routers) and you need to quickly localize whether the issue is access, transit, or routing/control-plane.", "content": "Use a layered reachability localization sequence: (1) collect IP addressing + default gateways on each host (ip addr/ifconfig + ip route) to confirm edge correctness; (2) verify router interface addressing and kernel routes to confirm only connected routes exist; (3) run pings in a progression: host->gateway, router<->router transit, then host->remote host. Interpret ICMP errors: 'Destination Net Unreachable' sourced from the gateway strongly indicates the gateway lacks a route to the remote prefix (routing/control-plane), not a link failure. This pattern worked because it isolates the fault domain with minimal tests and ties symptoms (ICMP unreachable) to routing-table evidence (no remote LAN routes).", "score": 0.86, "time_created": "2026-01-18 06:18:11", "time_modified": "2026-01-18 06:18:11", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When end-to-end host connectivity fails in a small routed lab (e.g., BGP between two routers) and you need to quickly localize whether the issue is access, transit, or routing/control-plane.", "experience": "Use a layered reachability localization sequence: (1) collect IP addressing + default gateways on each host (ip addr/ifconfig + ip route) to confirm edge correctness; (2) verify router interface addressing and kernel routes to confirm only connected routes exist; (3) run pings in a progression: host->gateway, router<->router transit, then host->remote host. Interpret ICMP errors: 'Destination Net Unreachable' sourced from the gateway strongly indicates the gateway lacks a route to the remote prefix (routing/control-plane), not a link failure. This pattern worked because it isolates the fault domain with minimal tests and ties symptoms (ICMP unreachable) to routing-table evidence (no remote LAN routes).", "tags": ["troubleshooting", "localization", "icmp-unreachable", "routing-table", "bgp", "layered-testing"], "confidence": 0.86, "step_type": "reasoning", "tools_used": ["ip_addr", "ifconfig", "ip_route", "ping"]}}
{"workspace_id": "nika_v1", "memory_id": "95311237b93a47358d21052b75db8f26", "memory_type": "task", "when_to_use": "When diagnosing suspected end-to-end reachability issues in a multi-area routing network (e.g., OSPF) and you need to quickly separate host misconfiguration from upstream routing/control-plane failure.", "content": "Start by validating L3 correctness at the edges and server farm using host-level commands (IP/mask, default gateway, routes). In this run, checking `ip_addr/ip_route` on `dns_server`, `web_server_0`, and representative user hosts established that endpoints had plausible addressing and default routes (e.g., server farm default via 10.200.0.1; host default via 10.1.1.1). This narrows the fault domain away from applications/host IP config and toward the network core/distribution control plane. This approach works because it rules out the most common local causes before spending time on routing daemons and topology-wide hypotheses.", "score": 0.5594183979299263, "time_created": "2026-01-17 15:04:41", "time_modified": "2026-01-17 15:04:41", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When diagnosing suspected end-to-end reachability issues in a multi-area routing network (e.g., OSPF) and you need to quickly separate host misconfiguration from upstream routing/control-plane failure.", "experience": "Start by validating L3 correctness at the edges and server farm using host-level commands (IP/mask, default gateway, routes). In this run, checking `ip_addr/ip_route` on `dns_server`, `web_server_0`, and representative user hosts established that endpoints had plausible addressing and default routes (e.g., server farm default via 10.200.0.1; host default via 10.1.1.1). This narrows the fault domain away from applications/host IP config and toward the network core/distribution control plane. This approach works because it rules out the most common local causes before spending time on routing daemons and topology-wide hypotheses.", "tags": ["triage", "layer3", "edge-validation", "default-gateway", "fault-domain-isolation"], "confidence": 0.82, "step_type": "action", "tools_used": ["ip_addr", "ip_route", "ifconfig"], "freq": 37, "utility": 9}}
{"workspace_id": "nika_v1", "memory_id": "e9feff03afc948a39e5aded3451ca8bd", "memory_type": "task", "when_to_use": "When end-to-end connectivity between hosts across routers is failing in a small routed/BGP lab and you need to quickly localize whether the issue is access, underlay, or routing/control-plane.", "content": "Use a layered fault-localization sequence: (1) validate host IP/gateway/routes (ip addr/ip route) to confirm correct default-gateway and on-link subnet; (2) validate first-hop reachability (host -> gateway ping) to rule out access-link issues; (3) validate underlay transit (router1 <-> router2 ping on interconnect subnet) to confirm the transport for BGP is up; (4) test end-to-end (pc1 -> pc2) and interpret ICMP errors. In this case, 'Destination Net Unreachable' sourced by router1 pinpointed the failure to router1's routing table (no route to remote LAN), not L2/L3 link failure. This structured progression prevents chasing BGP before proving the physical/L3 foundations and yields a precise fault domain statement.", "score": 0.86, "time_created": "2026-01-18 06:24:37", "time_modified": "2026-01-18 06:24:37", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When end-to-end connectivity between hosts across routers is failing in a small routed/BGP lab and you need to quickly localize whether the issue is access, underlay, or routing/control-plane.", "experience": "Use a layered fault-localization sequence: (1) validate host IP/gateway/routes (ip addr/ip route) to confirm correct default-gateway and on-link subnet; (2) validate first-hop reachability (host -> gateway ping) to rule out access-link issues; (3) validate underlay transit (router1 <-> router2 ping on interconnect subnet) to confirm the transport for BGP is up; (4) test end-to-end (pc1 -> pc2) and interpret ICMP errors. In this case, 'Destination Net Unreachable' sourced by router1 pinpointed the failure to router1's routing table (no route to remote LAN), not L2/L3 link failure. This structured progression prevents chasing BGP before proving the physical/L3 foundations and yields a precise fault domain statement.", "tags": ["troubleshooting", "fault-localization", "layered-testing", "icmp-interpretation", "routing", "bgp-lab"], "confidence": 0.86, "step_type": "reasoning", "tools_used": ["ip_addr", "ip_route", "ping"]}}
{"workspace_id": "nika_v1", "memory_id": "e1b483de4907416eb653b2fa9b9ea009", "memory_type": "task", "when_to_use": "When diagnosing multi-node connectivity failures in a small routed/BGP lab and you need to quickly localize whether the issue is host-edge, transit, or routing/control-plane.", "content": "Use a layered, edge-to-core validation sequence: (1) collect host interface + IP + route tables (ip addr/ifconfig, ip route) to confirm link state and presence of connected/default routes; (2) run host-to-gateway pings and interpret errors (e.g., 'Network is unreachable' indicates local host routing/interface issue, not upstream); (3) validate router-to-router underlay with direct pings on the transit subnet to confirm L2/L3 between routers; (4) test end-to-end and read ICMP source of failures (e.g., 'Destination Net Unreachable' sourced by the gateway indicates the router lacks a route to the destination prefix). This sequence works because it isolates faults by progressively expanding the blast radius and uses error semantics (local vs gateway-generated) to pinpoint the failing layer/device.", "score": 0.5675567970994231, "time_created": "2026-01-18 05:53:37", "time_modified": "2026-01-18 05:53:37", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When diagnosing multi-node connectivity failures in a small routed/BGP lab and you need to quickly localize whether the issue is host-edge, transit, or routing/control-plane.", "experience": "Use a layered, edge-to-core validation sequence: (1) collect host interface + IP + route tables (ip addr/ifconfig, ip route) to confirm link state and presence of connected/default routes; (2) run host-to-gateway pings and interpret errors (e.g., 'Network is unreachable' indicates local host routing/interface issue, not upstream); (3) validate router-to-router underlay with direct pings on the transit subnet to confirm L2/L3 between routers; (4) test end-to-end and read ICMP source of failures (e.g., 'Destination Net Unreachable' sourced by the gateway indicates the router lacks a route to the destination prefix). This sequence works because it isolates faults by progressively expanding the blast radius and uses error semantics (local vs gateway-generated) to pinpoint the failing layer/device.", "tags": ["troubleshooting", "fault-localization", "layered-diagnosis", "icmp-interpretation", "host-edge", "underlay-validation"], "confidence": 0.86, "step_type": "reasoning", "tools_used": ["ifconfig", "ip_addr", "ip_route", "ping"], "freq": 11, "utility": 3}}
{"workspace_id": "nika_v1", "memory_id": "5820c9e804194dcca5f17a82ff8e70df", "memory_type": "task", "when_to_use": "When end-to-end host connectivity is failing in a small routed/BGP lab and you must quickly determine whether the problem is edge L2/L3, underlay, or control-plane (BGP).", "content": "Use a layered validation sequence: (1) collect IP addressing + default routes on all nodes (ip addr/ifconfig + ip route) to confirm intended topology; (2) test each host's first-hop reachability (host->default-gateway ping) before testing remote hosts; (3) validate the transit/underlay separately (router1<->router2 ping on the inter-router subnet). This isolates failures early: if a host cannot reach its gateway, the issue is local/edge and BGP is irrelevant for that direction; if transit is healthy but remote subnets are unreachable, shift focus to routing/control-plane. This pattern worked here by proving pc1's access segment was broken (gateway unreachable) while the router-to-router link was healthy, preventing misattribution to BGP.", "score": 0.5642037410989833, "time_created": "2026-01-18 06:05:56", "time_modified": "2026-01-18 06:05:56", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When end-to-end host connectivity is failing in a small routed/BGP lab and you must quickly determine whether the problem is edge L2/L3, underlay, or control-plane (BGP).", "experience": "Use a layered validation sequence: (1) collect IP addressing + default routes on all nodes (ip addr/ifconfig + ip route) to confirm intended topology; (2) test each host's first-hop reachability (host->default-gateway ping) before testing remote hosts; (3) validate the transit/underlay separately (router1<->router2 ping on the inter-router subnet). This isolates failures early: if a host cannot reach its gateway, the issue is local/edge and BGP is irrelevant for that direction; if transit is healthy but remote subnets are unreachable, shift focus to routing/control-plane. This pattern worked here by proving pc1's access segment was broken (gateway unreachable) while the router-to-router link was healthy, preventing misattribution to BGP.", "tags": ["layered-troubleshooting", "edge-first", "default-gateway", "underlay-validation", "fault-isolation"], "confidence": 0.86, "step_type": "action", "tools_used": ["ifconfig", "ip addr", "ip route", "ping"], "freq": 8, "utility": 2}}
{"workspace_id": "nika_v1", "memory_id": "3f33f01a9f994654a0fb6de549003984", "memory_type": "task", "when_to_use": "When diagnosing a multi-zone routed network (e.g., RIP + VPN overlay) and you need to quickly determine whether symptoms are systemic (routing) or source-specific (edge host/link).", "content": "Start with a broad reachability matrix across representative endpoints (internal host↔internal host, internal host↔external servers, external server↔internal host). Then immediately validate any 'unknown' or suspicious paths with targeted pings from the implicated source. Compare loss/latency patterns across sources: if only one source shows loss to multiple destinations while others are clean, localize the fault domain to that source host or its first-hop access link rather than core routing. This pattern worked here because host_2 and servers had 0% loss to the same destinations where host_1 showed 50–75% loss, strongly indicating an edge impairment instead of RIP/route propagation failure.", "score": 0.5588911244034139, "time_created": "2026-01-17 12:22:59", "time_modified": "2026-01-17 12:22:59", "author": "azure/gpt-5.2", "metadata": {"when_to_use": "When diagnosing a multi-zone routed network (e.g., RIP + VPN overlay) and you need to quickly determine whether symptoms are systemic (routing) or source-specific (edge host/link).", "experience": "Start with a broad reachability matrix across representative endpoints (internal host↔internal host, internal host↔external servers, external server↔internal host). Then immediately validate any 'unknown' or suspicious paths with targeted pings from the implicated source. Compare loss/latency patterns across sources: if only one source shows loss to multiple destinations while others are clean, localize the fault domain to that source host or its first-hop access link rather than core routing. This pattern worked here because host_2 and servers had 0% loss to the same destinations where host_1 showed 50–75% loss, strongly indicating an edge impairment instead of RIP/route propagation failure.", "tags": ["reachability-matrix", "differential-testing", "fault-localization", "rip", "vpn", "icmp-loss"], "confidence": 0.86, "step_type": "decision", "tools_used": ["reachability_matrix", "ping"], "freq": 44, "utility": 15}}
